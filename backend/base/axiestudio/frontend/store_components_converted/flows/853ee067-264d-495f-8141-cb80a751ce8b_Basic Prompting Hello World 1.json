{
  "id": "853ee067-264d-495f-8141-cb80a751ce8b",
  "name": "Basic Prompting (Hello, World) (1)",
  "description": "This flow will get you experimenting with the basics of the UI, the Chat and the Prompt component. \n\nTry changing the Template in it to see how the model behaves. \nYou can change it to this and a Text Input into the `type_of_person` variable : \"Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer: \"  (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "jell0720",
    "first_name": "中銘",
    "last_name": "吳",
    "id": "415118a3-c76f-43b0-835c-017af95327ae",
    "full_name": "中銘 吳"
  },
  "store_url": "https://www.langflow.store/store/component/853ee067-264d-495f-8141-cb80a751ce8b",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-07-08T04:24:06.750Z",
    "updated": "2024-07-08T05:28:10.439Z",
    "downloaded": "2025-08-19T17:50:06.258Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.7",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "data": {
          "id": "ChatInput-71jt5",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "ChatInput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "hidden": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "告訴我鼎漢有哪些部門，做什麼業務？"
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "Session ID for the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            }
          },
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 309,
        "id": "ChatInput-71jt5",
        "position": {
          "x": -1219.4770924594939,
          "y": 711.1925758914338
        },
        "positionAbsolute": {
          "x": -1219.4770924594939,
          "y": 711.1925758914338
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-yGQsE",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "你現在是鼎漢交通顧問公司專任的高級交通運輸工程師，來協助同事。\n---------------\n{document}\n---------------\nUser: {user_input}\n\nAnswer: "
              },
              "user_input": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "user_input",
                "display_name": "user_input",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "document": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "鼎漢是台灣最大的交通顧問公司之一，成立於1989年，專注於運輸系統和城鄉土地的規劃、設計、開發和管理。公司創立的初衷是滿足台灣發展中的規劃需求，並計劃將台灣的專業經驗擴展到海外市場。通過數十年的服務經驗累積，鼎漢在2011年獲得了國家磐石獎的殊榮。未來，鼎漢將以積極創新的思維，高效的專業能力和服務熱忱，與業主攜手合作，共同追求更好的交通、環境和生活質量。公司的核心文化包括以人為本、團隊合作、追求品質、鼓勵創新和重視使命。公司的願景是成為專業創新的先驅，永續樂活的典範，以及全球交通環境的頂尖顧問。\n管理團隊:\n---------\n{\n  \"management_team\": {\n    \"executives\": [\n      {\"title\": \"董事長\", \"name\": \"李俊賢\"},\n      {\"title\": \"副董事長\", \"name\": \"林幸加\"},\n      {\"title\": \"總經理\", \"name\": \"李宗益\"}\n    ],\n    \"departments\": [\n      {\n        \"name\": \"智慧運輸部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林宜達\"}\n      },\n      {\n        \"name\": \"運輸與規劃部門\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"陳柏君\"}\n      },\n      {\n        \"name\": \"交通工程部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林俊甫\"}\n      },\n      {\n        \"name\": \"開發與BOT部門\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"卓明瑩\"}\n      },\n      {\n        \"name\": \"運輸土木部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"黃志賢\"}\n      },\n      {\n        \"name\": \"大數據部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"周諺鴻\"}\n      },\n      {\n        \"name\": \"智慧港灣物流部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林建文\"}\n      },\n      {\n        \"name\": \"高雄分公司\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"程佩鳳\"}\n      },\n      {\n        \"name\": \"台中分公司\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"陳建德\"}\n      },\n      {\n        \"name\": \"城鄉規劃中心\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"吳清如\"}\n      }\n    ]\n  }\n}\n\n服務類別與說明：\n----------------\n{ \"services\": [\n    {\"code\": \"BD\", \"name\": \"資料庫\", \"description\": \"負責交通相關數據的整合、管理及維護。\"},\n    {\"code\": \"BOT\", \"name\": \"促進民間參與\", \"description\": \"推動公私合作模式，增進基礎建設及營運效能。\"},\n    {\"code\": \"CE\", \"name\": \"土木工程\", \"description\": \"包含軌道、橋樑、隧道等基礎建設的工程設計與施工。\"},\n    {\"code\": \"EE\", \"name\": \"環境與能源\", \"description\": \"執行交通計畫的環境評估及能源效率提升。\"},\n    {\"code\": \"ITS\", \"name\": \"智慧型運輸系統\", \"description\": \"運用資訊與通訊技術提升交通系統的效率及服務品質。\"},\n    {\"code\": \"MT\", \"name\": \"大眾運輸規劃\", \"description\": \"設計與規劃符合公共需求的大眾運輸網絡。\"},\n    {\"code\": \"OM\", \"name\": \"營運管理\", \"description\": \"負責日常運輸系統的營運及維護管理。\"},\n    {\"code\": \"OP\", \"name\": \"組織及政策研究\", \"description\": \"進行交通政策的研究及策略發展，以促進交通系統的可持續發展。\"},\n    {\"code\": \"OTH\", \"name\": \"其他\", \"description\": \"涵蓋除上述分類外的其他各類交通相關業務。\"},\n    {\"code\": \"PK\", \"name\": \"停車場\", \"description\": \"規劃與管理停車設施，優化停車空間利用。\"},\n    {\"code\": \"RE\", \"name\": \"道路工程規劃設計\", \"description\": \"進行道路網絡的規劃、設計及建造。\"},\n    {\"code\": \"TA\", \"name\": \"交通經濟\", \"description\": \"進行交通項目的經濟效益分析與評估。\"},\n    {\"code\": \"TC\", \"name\": \"運輸土木\", \"description\": \"專門處理交通基礎建設相關的土木工程規劃與施工。\"},\n    {\"code\": \"TE\", \"name\": \"交通工程\", \"description\": \"專注於交通流量、路網設計及安全系統的工程設計與分析。\"},\n    {\"code\": \"TIA\", \"name\": \"交通衝擊評估\", \"description\": \"評估新開發項目對現有交通系統的影響及應對措施。\"},\n    {\"code\": \"TIP\", \"name\": \"運輸資訊平台\", \"description\": \"建立提供交通資訊和服務的數位平台，增進公眾交通使用體驗。\"},\n    {\"code\": \"TP\", \"name\": \"運輸規劃\", \"description\": \"制定運輸系統的綜合規劃，以提升運輸效率與服務質量。\"},\n    {\"code\": \"TR\", \"name\": \"觀光遊憩\", \"description\": \"規劃與推動與交通相關的觀光及休閒活動。\"},\n    {\"code\": \"UAV\", \"name\": \"無人機運用\", \"description\": \"利用無人機技術於交通監控與數據蒐集，支持交通管理決策。\"},\n    {\"code\": \"UPLD\", \"name\": \"都市規劃與開發\", \"description\": \"規劃與開發城市區域，整合交通網絡與都市基礎設施。\"}\n  ]\n}",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "document",
                "display_name": "document",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "document",
                "user_input"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": false,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 517,
        "id": "Prompt-yGQsE",
        "position": {
          "x": -603.4362562800936,
          "y": 281.3469622510418
        },
        "positionAbsolute": {
          "x": -603.4362562800936,
          "y": 281.3469622510418
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "id": "ChatOutput-KKodY",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "ChatOutput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "data_template": {
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "Session ID for the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            }
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 309,
        "id": "ChatOutput-KKodY",
        "position": {
          "x": 869.7397477089992,
          "y": 499.9519520844052
        },
        "positionAbsolute": {
          "x": 869.7397477089992,
          "y": 499.9519520844052
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "id": "OllamaModel-wEWf0",
        "type": "genericNode",
        "position": {
          "x": 69.84878758888783,
          "y": -115.23340050894652
        },
        "data": {
          "type": "OllamaModel",
          "node": {
            "template": {
              "_type": "Component",
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "http://localhost:11434",
                "name": "base_url",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            value=\"llama2\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=STREAM_INFO_TEXT,\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "format": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "format",
                "display_name": "Format",
                "advanced": true,
                "dynamic": false,
                "info": "Specify the format of the output (e.g., json).",
                "title_case": false,
                "type": "str"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "metadata": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": {},
                "name": "metadata",
                "display_name": "Metadata",
                "advanced": true,
                "dynamic": false,
                "info": "Metadata to add to the run trace.",
                "title_case": false,
                "type": "dict"
              },
              "mirostat": {
                "trace_as_metadata": true,
                "options": [
                  "Disabled",
                  "Mirostat",
                  "Mirostat 2.0"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Disabled",
                "name": "mirostat",
                "display_name": "Mirostat",
                "advanced": true,
                "dynamic": false,
                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                "title_case": false,
                "type": "str"
              },
              "mirostat_eta": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "mirostat_eta",
                "display_name": "Mirostat Eta",
                "advanced": true,
                "dynamic": false,
                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                "title_case": false,
                "type": "float"
              },
              "mirostat_tau": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "mirostat_tau",
                "display_name": "Mirostat Tau",
                "advanced": true,
                "dynamic": false,
                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                "title_case": false,
                "type": "float"
              },
              "model": {
                "trace_as_metadata": true,
                "options": [
                  "shaw/dmeta-embedding-zh:latest",
                  "nomic-embed-text:latest",
                  "wangshenzhi/gemma2-9b-chinese-chat:latest",
                  "wangrongsheng/mistral-7b-v0.3-chinese:latest",
                  "adsfaaron/taide-lx-7b-chat:q5",
                  "yabi/breeze-7b-32k-instruct-v1_0_q3_k:latest",
                  "yabi/breeze-7b-32k-instruct-v1_0_q2_k:latest",
                  "cwchang/llama3-taide-lx-8b-chat-alpha1:latest",
                  "jcai/llama3-taide-lx-8b-chat-alpha1:Q4_K_M",
                  "ryan4559/llama3-taide-lx-8b-chat-alpha1-4bit:latest",
                  "ycchen/breeze-7b-instruct-v1_0:latest",
                  "yi:latest"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "adsfaaron/taide-lx-7b-chat:q5",
                "name": "model",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.ai/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str"
              },
              "num_ctx": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "num_ctx",
                "display_name": "Context Window Size",
                "advanced": true,
                "dynamic": false,
                "info": "Size of the context window for generating tokens. (Default: 2048)",
                "title_case": false,
                "type": "int"
              },
              "num_gpu": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "num_gpu",
                "display_name": "Number of GPUs",
                "advanced": true,
                "dynamic": false,
                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                "title_case": false,
                "type": "int"
              },
              "num_thread": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "num_thread",
                "display_name": "Number of Threads",
                "advanced": true,
                "dynamic": false,
                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                "title_case": false,
                "type": "int"
              },
              "repeat_last_n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "repeat_last_n",
                "display_name": "Repeat Last N",
                "advanced": true,
                "dynamic": false,
                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                "title_case": false,
                "type": "int"
              },
              "repeat_penalty": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "repeat_penalty",
                "display_name": "Repeat Penalty",
                "advanced": true,
                "dynamic": false,
                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                "title_case": false,
                "type": "float"
              },
              "stop_tokens": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "stop_tokens",
                "display_name": "Stop Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                "title_case": false,
                "type": "str"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": false,
                "name": "stream",
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool"
              },
              "system": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system",
                "display_name": "System",
                "advanced": true,
                "dynamic": false,
                "info": "System to use for generating text.",
                "title_case": false,
                "type": "str"
              },
              "system_message": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system_message",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str"
              },
              "tags": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "tags",
                "display_name": "Tags",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tags to add to the run trace.",
                "title_case": false,
                "type": "str"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 0.2,
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls the creativity of model responses.",
                "title_case": false,
                "type": "float"
              },
              "template": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "template",
                "display_name": "Template",
                "advanced": true,
                "dynamic": false,
                "info": "Template to use for generating text.",
                "title_case": false,
                "type": "str"
              },
              "tfs_z": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "tfs_z",
                "display_name": "TFS Z",
                "advanced": true,
                "dynamic": false,
                "info": "Tail free sampling value. (Default: 1)",
                "title_case": false,
                "type": "float"
              },
              "timeout": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "timeout",
                "display_name": "Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "Timeout for the request stream.",
                "title_case": false,
                "type": "int"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "top_k",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Limits token selection to top K. (Default: 40)",
                "title_case": false,
                "type": "int"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "top_p",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "Works together with top-k. (Default: 0.9)",
                "title_case": false,
                "type": "float"
              },
              "verbose": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "verbose",
                "display_name": "Verbose",
                "advanced": false,
                "dynamic": false,
                "info": "Whether to print out response text.",
                "title_case": false,
                "type": "bool"
              }
            },
            "description": "Generate text using Ollama Local LLMs.",
            "icon": "Ollama",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Ollama",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true,
                "hidden": false
              }
            ],
            "field_order": [
              "base_url",
              "model",
              "temperature",
              "format",
              "metadata",
              "mirostat",
              "mirostat_eta",
              "mirostat_tau",
              "num_ctx",
              "num_gpu",
              "num_thread",
              "repeat_last_n",
              "repeat_penalty",
              "tfs_z",
              "timeout",
              "top_k",
              "top_p",
              "verbose",
              "tags",
              "stop_tokens",
              "system",
              "template",
              "input_value",
              "stream",
              "system_message"
            ],
            "beta": false,
            "edited": false
          },
          "id": "OllamaModel-wEWf0"
        },
        "selected": false,
        "width": 384,
        "height": 777,
        "positionAbsolute": {
          "x": 69.84878758888783,
          "y": -115.23340050894652
        },
        "dragging": false
      },
      {
        "id": "PromptTemplate-I5s0f",
        "type": "genericNode",
        "position": {
          "x": -922,
          "y": 136.8828125
        },
        "data": {
          "type": "PromptTemplate",
          "node": {
            "template": {
              "output_parser": {
                "required": false,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "password": false,
                "name": "output_parser",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "BaseOutputParser",
                "list": false
              },
              "input_types": {
                "required": false,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "password": false,
                "name": "input_types",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "dict",
                "list": false
              },
              "input_variables": {
                "required": true,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "password": false,
                "name": "input_variables",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "str",
                "list": true,
                "value": [
                  "document",
                  "question"
                ]
              },
              "partial_variables": {
                "required": false,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "password": false,
                "name": "partial_variables",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "dict",
                "list": false
              },
              "template": {
                "required": true,
                "placeholder": "",
                "show": true,
                "multiline": true,
                "password": false,
                "name": "template",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "prompt",
                "list": false,
                "value": "Answer user's questions based on the document below:\n---\n{document}\n---\nQuestion:\n{question}\n\nAnswer:\n"
              },
              "template_format": {
                "required": false,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "value": "f-string",
                "password": false,
                "name": "template_format",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "str",
                "list": false
              },
              "validate_template": {
                "required": false,
                "placeholder": "",
                "show": false,
                "multiline": false,
                "value": false,
                "password": false,
                "name": "validate_template",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "type": "bool",
                "list": false
              },
              "_type": "PromptTemplate",
              "document": {
                "type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "document",
                "display_name": "document",
                "advanced": false,
                "input_types": [
                  "Document",
                  "BaseOutputParser"
                ],
                "dynamic": false,
                "info": ""
              },
              "question": {
                "type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "秦登达的手机号码是多少",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "question",
                "display_name": "question",
                "advanced": false,
                "input_types": [
                  "Document",
                  "BaseOutputParser"
                ],
                "dynamic": false,
                "info": ""
              }
            },
            "description": "A prompt template for a language model.",
            "icon": null,
            "base_classes": [
              "StringPromptTemplate",
              "BasePromptTemplate",
              "PromptTemplate"
            ],
            "name": "",
            "display_name": "PromptTemplate",
            "documentation": "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/",
            "custom_fields": {
              "": [
                "document",
                "question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "field_formatters": {},
            "beta": false,
            "error": null,
            "official": false
          },
          "id": "PromptTemplate-I5s0f"
        },
        "selected": false
      }
    ],
    "edges": [
      {
        "source": "ChatInput-71jt5",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-71jt5œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-yGQsE",
        "targetHandle": "{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-yGQsEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "user_input",
            "id": "Prompt-yGQsE",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-71jt5",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-71jt5{œdataTypeœ:œChatInputœ,œidœ:œChatInput-71jt5œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-yGQsE{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-yGQsEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Prompt-yGQsE",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-yGQsEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OllamaModel-wEWf0",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-wEWf0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OllamaModel-wEWf0",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-yGQsE",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-yGQsE{œdataTypeœ:œPromptœ,œidœ:œPrompt-yGQsEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-wEWf0{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-wEWf0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "OllamaModel-wEWf0",
        "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-wEWf0œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-KKodY",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KKodYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-KKodY",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OllamaModel",
            "id": "OllamaModel-wEWf0",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OllamaModel-wEWf0{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-wEWf0œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-KKodY{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KKodYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      }
    ],
    "viewport": {
      "x": 1150,
      "y": -79,
      "zoom": 1
    }
  },
  "metadata": {
    "ChatInput": {
      "count": 1
    },
    "Prompt": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "OllamaModel": {
      "count": 1
    },
    "PromptTemplate": {
      "count": 1
    },
    "total": 5
  },
  "original": {
    "id": "853ee067-264d-495f-8141-cb80a751ce8b",
    "name": "Basic Prompting (Hello, World) (1)",
    "description": "This flow will get you experimenting with the basics of the UI, the Chat and the Prompt component. \n\nTry changing the Template in it to see how the model behaves. \nYou can change it to this and a Text Input into the `type_of_person` variable : \"Answer the user as if you were a pirate.\n\nUser: {user_input}\n\nAnswer: \" ",
    "is_component": false,
    "liked_by_count": "1",
    "downloads_count": "4",
    "metadata": {
      "ChatInput": {
        "count": 1
      },
      "Prompt": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "OllamaModel": {
        "count": 1
      },
      "PromptTemplate": {
        "count": 1
      },
      "total": 5
    },
    "last_tested_version": "1.0.7",
    "private": false,
    "data": {
      "nodes": [
        {
          "data": {
            "id": "ChatInput-71jt5",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Get chat inputs from the Playground.",
              "display_name": "Chat Input",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "frozen": false,
              "icon": "ChatInput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "hidden": false,
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "files": {
                  "advanced": true,
                  "display_name": "Files",
                  "dynamic": false,
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "file_path": "",
                  "info": "Files to be sent with the message.",
                  "list": true,
                  "name": "files",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "file",
                  "value": ""
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "告訴我鼎漢有哪些部門，做什麼業務？"
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "session_id": {
                  "advanced": true,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                }
              }
            },
            "type": "ChatInput"
          },
          "dragging": false,
          "height": 309,
          "id": "ChatInput-71jt5",
          "position": {
            "x": -1219.4770924594939,
            "y": 711.1925758914338
          },
          "positionAbsolute": {
            "x": -1219.4770924594939,
            "y": 711.1925758914338
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-yGQsE",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(frontend_template=frontend_node, raw_template=current_build_config[\"template\"])\n        return frontend_node\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "你現在是鼎漢交通顧問公司專任的高級交通運輸工程師，來協助同事。\n---------------\n{document}\n---------------\nUser: {user_input}\n\nAnswer: "
                },
                "user_input": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "user_input",
                  "display_name": "user_input",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "document": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "鼎漢是台灣最大的交通顧問公司之一，成立於1989年，專注於運輸系統和城鄉土地的規劃、設計、開發和管理。公司創立的初衷是滿足台灣發展中的規劃需求，並計劃將台灣的專業經驗擴展到海外市場。通過數十年的服務經驗累積，鼎漢在2011年獲得了國家磐石獎的殊榮。未來，鼎漢將以積極創新的思維，高效的專業能力和服務熱忱，與業主攜手合作，共同追求更好的交通、環境和生活質量。公司的核心文化包括以人為本、團隊合作、追求品質、鼓勵創新和重視使命。公司的願景是成為專業創新的先驅，永續樂活的典範，以及全球交通環境的頂尖顧問。\n管理團隊:\n---------\n{\n  \"management_team\": {\n    \"executives\": [\n      {\"title\": \"董事長\", \"name\": \"李俊賢\"},\n      {\"title\": \"副董事長\", \"name\": \"林幸加\"},\n      {\"title\": \"總經理\", \"name\": \"李宗益\"}\n    ],\n    \"departments\": [\n      {\n        \"name\": \"智慧運輸部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林宜達\"}\n      },\n      {\n        \"name\": \"運輸與規劃部門\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"陳柏君\"}\n      },\n      {\n        \"name\": \"交通工程部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林俊甫\"}\n      },\n      {\n        \"name\": \"開發與BOT部門\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"卓明瑩\"}\n      },\n      {\n        \"name\": \"運輸土木部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"黃志賢\"}\n      },\n      {\n        \"name\": \"大數據部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"周諺鴻\"}\n      },\n      {\n        \"name\": \"智慧港灣物流部門\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"林建文\"}\n      },\n      {\n        \"name\": \"高雄分公司\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"程佩鳳\"}\n      },\n      {\n        \"name\": \"台中分公司\",\n        \"leader\": {\"title\": \"協理\", \"name\": \"陳建德\"}\n      },\n      {\n        \"name\": \"城鄉規劃中心\",\n        \"leader\": {\"title\": \"副總經理\", \"name\": \"吳清如\"}\n      }\n    ]\n  }\n}\n\n服務類別與說明：\n----------------\n{ \"services\": [\n    {\"code\": \"BD\", \"name\": \"資料庫\", \"description\": \"負責交通相關數據的整合、管理及維護。\"},\n    {\"code\": \"BOT\", \"name\": \"促進民間參與\", \"description\": \"推動公私合作模式，增進基礎建設及營運效能。\"},\n    {\"code\": \"CE\", \"name\": \"土木工程\", \"description\": \"包含軌道、橋樑、隧道等基礎建設的工程設計與施工。\"},\n    {\"code\": \"EE\", \"name\": \"環境與能源\", \"description\": \"執行交通計畫的環境評估及能源效率提升。\"},\n    {\"code\": \"ITS\", \"name\": \"智慧型運輸系統\", \"description\": \"運用資訊與通訊技術提升交通系統的效率及服務品質。\"},\n    {\"code\": \"MT\", \"name\": \"大眾運輸規劃\", \"description\": \"設計與規劃符合公共需求的大眾運輸網絡。\"},\n    {\"code\": \"OM\", \"name\": \"營運管理\", \"description\": \"負責日常運輸系統的營運及維護管理。\"},\n    {\"code\": \"OP\", \"name\": \"組織及政策研究\", \"description\": \"進行交通政策的研究及策略發展，以促進交通系統的可持續發展。\"},\n    {\"code\": \"OTH\", \"name\": \"其他\", \"description\": \"涵蓋除上述分類外的其他各類交通相關業務。\"},\n    {\"code\": \"PK\", \"name\": \"停車場\", \"description\": \"規劃與管理停車設施，優化停車空間利用。\"},\n    {\"code\": \"RE\", \"name\": \"道路工程規劃設計\", \"description\": \"進行道路網絡的規劃、設計及建造。\"},\n    {\"code\": \"TA\", \"name\": \"交通經濟\", \"description\": \"進行交通項目的經濟效益分析與評估。\"},\n    {\"code\": \"TC\", \"name\": \"運輸土木\", \"description\": \"專門處理交通基礎建設相關的土木工程規劃與施工。\"},\n    {\"code\": \"TE\", \"name\": \"交通工程\", \"description\": \"專注於交通流量、路網設計及安全系統的工程設計與分析。\"},\n    {\"code\": \"TIA\", \"name\": \"交通衝擊評估\", \"description\": \"評估新開發項目對現有交通系統的影響及應對措施。\"},\n    {\"code\": \"TIP\", \"name\": \"運輸資訊平台\", \"description\": \"建立提供交通資訊和服務的數位平台，增進公眾交通使用體驗。\"},\n    {\"code\": \"TP\", \"name\": \"運輸規劃\", \"description\": \"制定運輸系統的綜合規劃，以提升運輸效率與服務質量。\"},\n    {\"code\": \"TR\", \"name\": \"觀光遊憩\", \"description\": \"規劃與推動與交通相關的觀光及休閒活動。\"},\n    {\"code\": \"UAV\", \"name\": \"無人機運用\", \"description\": \"利用無人機技術於交通監控與數據蒐集，支持交通管理決策。\"},\n    {\"code\": \"UPLD\", \"name\": \"都市規劃與開發\", \"description\": \"規劃與開發城市區域，整合交通網絡與都市基礎設施。\"}\n  ]\n}",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "document",
                  "display_name": "document",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "document",
                  "user_input"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": false,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false
            },
            "type": "Prompt"
          },
          "dragging": false,
          "height": 517,
          "id": "Prompt-yGQsE",
          "position": {
            "x": -603.4362562800936,
            "y": 281.3469622510418
          },
          "positionAbsolute": {
            "x": -603.4362562800936,
            "y": 281.3469622510418
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "id": "ChatOutput-KKodY",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Display a chat message in the Playground.",
              "display_name": "Chat Output",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "frozen": false,
              "icon": "ChatOutput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            self.store_message(message)\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "data_template": {
                  "advanced": true,
                  "display_name": "Data Template",
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "data_template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Machine"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "AI"
                },
                "session_id": {
                  "advanced": true,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                }
              }
            },
            "type": "ChatOutput"
          },
          "dragging": false,
          "height": 309,
          "id": "ChatOutput-KKodY",
          "position": {
            "x": 869.7397477089992,
            "y": 499.9519520844052
          },
          "positionAbsolute": {
            "x": 869.7397477089992,
            "y": 499.9519520844052
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "id": "OllamaModel-wEWf0",
          "type": "genericNode",
          "position": {
            "x": 69.84878758888783,
            "y": -115.23340050894652
          },
          "data": {
            "type": "OllamaModel",
            "node": {
              "template": {
                "_type": "Component",
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "http://localhost:11434",
                  "name": "base_url",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, MessageInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model\",\n            display_name=\"Model Name\",\n            value=\"llama2\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=STREAM_INFO_TEXT,\n        ),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "format": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "format",
                  "display_name": "Format",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Specify the format of the output (e.g., json).",
                  "title_case": false,
                  "type": "str"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "metadata": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": {},
                  "name": "metadata",
                  "display_name": "Metadata",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Metadata to add to the run trace.",
                  "title_case": false,
                  "type": "dict"
                },
                "mirostat": {
                  "trace_as_metadata": true,
                  "options": [
                    "Disabled",
                    "Mirostat",
                    "Mirostat 2.0"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Disabled",
                  "name": "mirostat",
                  "display_name": "Mirostat",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                  "title_case": false,
                  "type": "str"
                },
                "mirostat_eta": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "mirostat_eta",
                  "display_name": "Mirostat Eta",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                  "title_case": false,
                  "type": "float"
                },
                "mirostat_tau": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "mirostat_tau",
                  "display_name": "Mirostat Tau",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                  "title_case": false,
                  "type": "float"
                },
                "model": {
                  "trace_as_metadata": true,
                  "options": [
                    "shaw/dmeta-embedding-zh:latest",
                    "nomic-embed-text:latest",
                    "wangshenzhi/gemma2-9b-chinese-chat:latest",
                    "wangrongsheng/mistral-7b-v0.3-chinese:latest",
                    "adsfaaron/taide-lx-7b-chat:q5",
                    "yabi/breeze-7b-32k-instruct-v1_0_q3_k:latest",
                    "yabi/breeze-7b-32k-instruct-v1_0_q2_k:latest",
                    "cwchang/llama3-taide-lx-8b-chat-alpha1:latest",
                    "jcai/llama3-taide-lx-8b-chat-alpha1:Q4_K_M",
                    "ryan4559/llama3-taide-lx-8b-chat-alpha1-4bit:latest",
                    "ycchen/breeze-7b-instruct-v1_0:latest",
                    "yi:latest"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "adsfaaron/taide-lx-7b-chat:q5",
                  "name": "model",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.ai/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str"
                },
                "num_ctx": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "num_ctx",
                  "display_name": "Context Window Size",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Size of the context window for generating tokens. (Default: 2048)",
                  "title_case": false,
                  "type": "int"
                },
                "num_gpu": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "num_gpu",
                  "display_name": "Number of GPUs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                  "title_case": false,
                  "type": "int"
                },
                "num_thread": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "num_thread",
                  "display_name": "Number of Threads",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                  "title_case": false,
                  "type": "int"
                },
                "repeat_last_n": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "repeat_last_n",
                  "display_name": "Repeat Last N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                  "title_case": false,
                  "type": "int"
                },
                "repeat_penalty": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "repeat_penalty",
                  "display_name": "Repeat Penalty",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                  "title_case": false,
                  "type": "float"
                },
                "stop_tokens": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "stop_tokens",
                  "display_name": "Stop Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                  "title_case": false,
                  "type": "str"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": false,
                  "name": "stream",
                  "display_name": "Stream",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool"
                },
                "system": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system",
                  "display_name": "System",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System to use for generating text.",
                  "title_case": false,
                  "type": "str"
                },
                "system_message": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system_message",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str"
                },
                "tags": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "tags",
                  "display_name": "Tags",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tags to add to the run trace.",
                  "title_case": false,
                  "type": "str"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 0.2,
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Controls the creativity of model responses.",
                  "title_case": false,
                  "type": "float"
                },
                "template": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Template to use for generating text.",
                  "title_case": false,
                  "type": "str"
                },
                "tfs_z": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "tfs_z",
                  "display_name": "TFS Z",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Tail free sampling value. (Default: 1)",
                  "title_case": false,
                  "type": "float"
                },
                "timeout": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "timeout",
                  "display_name": "Timeout",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Timeout for the request stream.",
                  "title_case": false,
                  "type": "int"
                },
                "top_k": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "top_k",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Limits token selection to top K. (Default: 40)",
                  "title_case": false,
                  "type": "int"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "top_p",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Works together with top-k. (Default: 0.9)",
                  "title_case": false,
                  "type": "float"
                },
                "verbose": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "verbose",
                  "display_name": "Verbose",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Whether to print out response text.",
                  "title_case": false,
                  "type": "bool"
                }
              },
              "description": "Generate text using Ollama Local LLMs.",
              "icon": "Ollama",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "Ollama",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true,
                  "hidden": false
                }
              ],
              "field_order": [
                "base_url",
                "model",
                "temperature",
                "format",
                "metadata",
                "mirostat",
                "mirostat_eta",
                "mirostat_tau",
                "num_ctx",
                "num_gpu",
                "num_thread",
                "repeat_last_n",
                "repeat_penalty",
                "tfs_z",
                "timeout",
                "top_k",
                "top_p",
                "verbose",
                "tags",
                "stop_tokens",
                "system",
                "template",
                "input_value",
                "stream",
                "system_message"
              ],
              "beta": false,
              "edited": false
            },
            "id": "OllamaModel-wEWf0"
          },
          "selected": false,
          "width": 384,
          "height": 777,
          "positionAbsolute": {
            "x": 69.84878758888783,
            "y": -115.23340050894652
          },
          "dragging": false
        },
        {
          "id": "PromptTemplate-I5s0f",
          "type": "genericNode",
          "position": {
            "x": -922,
            "y": 136.8828125
          },
          "data": {
            "type": "PromptTemplate",
            "node": {
              "template": {
                "output_parser": {
                  "required": false,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "password": false,
                  "name": "output_parser",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "BaseOutputParser",
                  "list": false
                },
                "input_types": {
                  "required": false,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "password": false,
                  "name": "input_types",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "dict",
                  "list": false
                },
                "input_variables": {
                  "required": true,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "password": false,
                  "name": "input_variables",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "str",
                  "list": true,
                  "value": [
                    "document",
                    "question"
                  ]
                },
                "partial_variables": {
                  "required": false,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "password": false,
                  "name": "partial_variables",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "dict",
                  "list": false
                },
                "template": {
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "multiline": true,
                  "password": false,
                  "name": "template",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "prompt",
                  "list": false,
                  "value": "Answer user's questions based on the document below:\n---\n{document}\n---\nQuestion:\n{question}\n\nAnswer:\n"
                },
                "template_format": {
                  "required": false,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "value": "f-string",
                  "password": false,
                  "name": "template_format",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "str",
                  "list": false
                },
                "validate_template": {
                  "required": false,
                  "placeholder": "",
                  "show": false,
                  "multiline": false,
                  "value": false,
                  "password": false,
                  "name": "validate_template",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "type": "bool",
                  "list": false
                },
                "_type": "PromptTemplate",
                "document": {
                  "type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "document",
                  "display_name": "document",
                  "advanced": false,
                  "input_types": [
                    "Document",
                    "BaseOutputParser"
                  ],
                  "dynamic": false,
                  "info": ""
                },
                "question": {
                  "type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "秦登达的手机号码是多少",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "question",
                  "display_name": "question",
                  "advanced": false,
                  "input_types": [
                    "Document",
                    "BaseOutputParser"
                  ],
                  "dynamic": false,
                  "info": ""
                }
              },
              "description": "A prompt template for a language model.",
              "icon": null,
              "base_classes": [
                "StringPromptTemplate",
                "BasePromptTemplate",
                "PromptTemplate"
              ],
              "name": "",
              "display_name": "PromptTemplate",
              "documentation": "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/",
              "custom_fields": {
                "": [
                  "document",
                  "question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "field_formatters": {},
              "beta": false,
              "error": null,
              "official": false
            },
            "id": "PromptTemplate-I5s0f"
          },
          "selected": false
        }
      ],
      "edges": [
        {
          "source": "ChatInput-71jt5",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-71jt5œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-yGQsE",
          "targetHandle": "{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-yGQsEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "user_input",
              "id": "Prompt-yGQsE",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-71jt5",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-71jt5{œdataTypeœ:œChatInputœ,œidœ:œChatInput-71jt5œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-yGQsE{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-yGQsEœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Prompt-yGQsE",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-yGQsEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "OllamaModel-wEWf0",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-wEWf0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "OllamaModel-wEWf0",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-yGQsE",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-yGQsE{œdataTypeœ:œPromptœ,œidœ:œPrompt-yGQsEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OllamaModel-wEWf0{œfieldNameœ:œinput_valueœ,œidœ:œOllamaModel-wEWf0œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "OllamaModel-wEWf0",
          "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-wEWf0œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatOutput-KKodY",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KKodYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-KKodY",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "OllamaModel",
              "id": "OllamaModel-wEWf0",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-OllamaModel-wEWf0{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-wEWf0œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-KKodY{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KKodYœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        }
      ],
      "viewport": {
        "x": 1150,
        "y": -79,
        "zoom": 1
      }
    },
    "date_created": "2024-07-08T04:24:06.750Z",
    "date_updated": "2024-07-08T05:28:10.439Z",
    "status": "Public",
    "sort": null,
    "user_updated": "415118a3-c76f-43b0-835c-017af95327ae",
    "user_created": {
      "username": "jell0720",
      "first_name": "中銘",
      "last_name": "吳",
      "id": "415118a3-c76f-43b0-835c-017af95327ae"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:01.070Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 32,
    "converter_version": "1.0.0"
  }
}