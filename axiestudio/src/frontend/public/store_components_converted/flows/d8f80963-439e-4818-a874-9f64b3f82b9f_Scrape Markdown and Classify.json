{
  "id": "d8f80963-439e-4818-a874-9f64b3f82b9f",
  "name": "Scrape, Markdown and Classify",
  "description": "",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "Roy",
    "first_name": "Roy",
    "last_name": "Barber",
    "id": "d37448a0-3dae-485a-8bce-0671ce46cf30",
    "full_name": "Roy Barber"
  },
  "store_url": "https://www.langflow.store/store/component/d8f80963-439e-4818-a874-9f64b3f82b9f",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-09-30T17:18:19.263Z",
    "updated": "2024-09-30T17:18:19.302Z",
    "downloaded": "2025-08-19T17:50:07.244Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.18",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "TextInput-KBnlG",
        "type": "genericNode",
        "position": {
          "x": 42.448925005496676,
          "y": 632.6680353213949
        },
        "data": {
          "type": "TextInput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "https://www.happydance.love/sitemap.xml",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Enter the full sitemap URL",
            "icon": "type",
            "base_classes": [
              "Message"
            ],
            "display_name": "Sitemap URL",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "TextInput-KBnlG"
        },
        "selected": false,
        "width": 384,
        "height": 302,
        "dragging": false,
        "positionAbsolute": {
          "x": 42.448925005496676,
          "y": 632.6680353213949
        }
      },
      {
        "id": "SitemapFilter-FwNeR",
        "type": "genericNode",
        "position": {
          "x": 587.0265495830818,
          "y": 582.2979116314294
        },
        "data": {
          "type": "SitemapFilter",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import xml.etree.ElementTree as ET\nfrom typing import List\nimport httpx\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema import Data\n\nclass SitemapFilterComponent(Component):\n    display_name = \"Sitemap Fetch & Filter\"\n    description = \"Fetchs an XML sitemap, parses the URLs and allows you to filter the results. (e.g., '/blog/')\"\n    icon = \"book-open-text\"\n    name = \"SitemapFilter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"sitemap_url\",\n            display_name=\"Sitemap URL\",\n            info=\"Enter the URL of the sitemap to process, including http(s)://\",\n        ),\n        MessageTextInput(\n            name=\"filter_criteria\",\n            display_name=\"Filter\",\n            info=\"Value to filter URLs by (e.g., '/blog/').\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Filtered URLs\", name=\"filtered_urls\", method=\"process_sitemap\"),\n    ]\n\n    def parse_xml(self, xml_data: str) -> List[str]:\n        \"\"\"Parses the XML data and extracts all URLs from <loc> elements.\"\"\"\n        try:\n            root = ET.fromstring(xml_data)\n            urls = [url.find(\"{http://www.sitemaps.org/schemas/sitemap/0.9}loc\").text \n                    for url in root.findall(\"{http://www.sitemaps.org/schemas/sitemap/0.9}url\")]\n            return urls\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\")\n\n    async def process_sitemap(self) -> List[Data]:\n        sitemap_url = self.sitemap_url\n        filter_criteria = self.filter_criteria\n\n        async with httpx.AsyncClient() as client:\n            # Fetch the sitemap\n            try:\n                response = await client.get(sitemap_url)\n                sitemap_content = response.text\n            except Exception as e:\n                return [Data(data={\"error\": f\"Failed to fetch sitemap: {e}\"})]\n\n            # Parse and filter URLs\n            try:\n                all_urls = self.parse_xml(sitemap_content)\n                filtered_urls = [url for url in all_urls if filter_criteria in url]\n            except Exception as e:\n                return [Data(data={\"error\": f\"Failed to parse or filter sitemap: {e}\"})]\n\n        # Create a list of Data objects, each containing a filtered URL\n        result = [Data(data={\"link\": url}) for url in filtered_urls]\n        \n        self.status = result\n        return result",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "filter_criteria": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "filter_criteria",
                "value": "/resources/insights/https://www.happydance.love/resources/insights/how-good-housekee",
                "display_name": "Filter",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Value to filter URLs by (e.g., '/blog/').",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "sitemap_url": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sitemap_url",
                "value": "",
                "display_name": "Sitemap URL",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Enter the URL of the sitemap to process, including http(s)://",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Fetchs an XML sitemap, parses the URLs and allows you to filter the results. (e.g., '/blog/')",
            "icon": "book-open-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "Sitemap Filter",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "filtered_urls",
                "display_name": "Filtered URLs",
                "method": "process_sitemap",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "sitemap_url",
              "filter_criteria"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "SitemapFilter-FwNeR"
        },
        "selected": false,
        "width": 384,
        "height": 416,
        "dragging": false,
        "positionAbsolute": {
          "x": 587.0265495830818,
          "y": 582.2979116314294
        }
      },
      {
        "id": "URLToMarkdownConverter-NsAl9",
        "type": "genericNode",
        "position": {
          "x": 1202.7406686737822,
          "y": 506.2862402256809
        },
        "data": {
          "type": "URLToMarkdownConverter",
          "node": {
            "template": {
              "_type": "Component",
              "search_results": {
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "search_results",
                "value": "",
                "display_name": "Search results with URLs",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "List of retrieved URLs from a search engine to convert to Markdown",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import requests\nimport html2text\nfrom loguru import logger\nfrom bs4 import BeautifulSoup\nfrom typing import List, Dict\nfrom axiestudio.schema import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.schema.dotdict import dotdict\nfrom axiestudio.io import DataInput, MessageTextInput, Output\nimport os\nfrom urllib.parse import urlparse\n\nclass URLToMarkdownConverter(Component):\n    display_name = \"Link list to Markdown\"\n    description = \"Fetches HTML from URLs, parses it, and converts the HTML to Markdown.\"\n    icon = \"heading-1\"\n    inputs = [\n        DataInput(\n            name=\"search_results\",\n            display_name=\"Search results with URLs\",\n            info=\"List of retrieved URLs from a search engine to convert to Markdown\",\n        ),\n        MessageTextInput(\n            name=\"title_selector\",\n            display_name=\"Title Selector\",\n            info=\"HTML selector for the title element, e.g 'h1.blog-title'\",\n        ),\n        MessageTextInput(\n            name=\"content_selector\",\n            display_name=\"Content Selector\",\n            info=\"HTML selector for the content, e.g 'div.content\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Markdown Output\", name=\"markdown_output\", method=\"process_urls\"),\n    ]\n    def process_urls(self) -> List[Data]:\n        search_results: List[Data] = self.search_results\n        title_selector: str = self.title_selector\n        content_selector: str = self.content_selector\n        processed_urls = []\n        \n        for result in search_results:\n            url = result.link\n            new_item = result.data.copy()\n            \n            try:\n                logger.info(f\"Processing URL: {url}\")\n                # 1. Extract HTML from the response\n                response = requests.get(url)\n                html_content = response.text\n                # 2. Parse the HTML\n                soup = BeautifulSoup(html_content, 'html.parser')\n                # 3. Extract specific content using the provided selectors\n                title_element = soup.select_one(title_selector) if title_selector else None\n                if not title_element:\n                    # Fallback to using the page title if no specific title element is found\n                    title_element = soup.find('title')\n                content_element = soup.select_one(content_selector)\n                # Check if both elements exist\n                if title_element and content_element:\n                    # Combine the relevant parts into HTML\n                    title_text = title_element.get_text().strip()\n                    extracted_html = f\"<h1>{title_text}</h1>\\n{str(content_element)}\"\n                    \n                    # 4. Convert the extracted HTML to Markdown\n                    h = html2text.HTML2Text()\n                    h.ignore_links = True  # This will remove all hyperlinks\n                    h.ignore_images = True  # This will remove all images\n                    \n                    # Convert the content to markdown\n                    markdown_content = h.handle(extracted_html)\n                    \n                    # Remove extra newlines or empty headings (like '## ')\n                    markdown_content = markdown_content.replace(\"##\\n\\n\", \"## \").replace(\"##\\n\", \"## \")\n                    \n                    # Clean up additional unnecessary newlines\n                    markdown_content = markdown_content.replace(\"\\n\\n\", \"\\n\").strip()\n                    # Ensure headings don't have extra spaces or newlines\n                    markdown_content = markdown_content.replace(\"##  \\n\", \"## \").replace(\"##\\n\", \"## \").strip()\n                    \n                    # Generate a file name based on the URL\n                    parsed_url = urlparse(url)\n                    file_name = f\"{parsed_url.path.replace('/', '_')}.md\".strip('_')\n                    new_item[\"title\"] = title_text\n                    new_item[\"markdown\"] = markdown_content\n                    new_item[\"file_name\"] = file_name\n                    processed_urls.append(Data(data=new_item))\n                else:\n                    logger.error(f\"Missing required elements in {url}\")\n            \n            except Exception as e:\n                logger.error(f\"Error processing {url}: {e}\")\n                raise\n        \n        return processed_urls",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "content_selector": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "content_selector",
                "value": "div.cms-content",
                "display_name": "Content Selector",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "HTML selector for the content, e.g 'div.content",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "title_selector": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "title_selector",
                "value": "h1.section-blog-single__title",
                "display_name": "Title Selector",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "HTML selector for the title element, e.g 'h1.blog-title'",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Fetches HTML from URLs, parses it, and converts the HTML to Markdown.",
            "icon": "heading-1",
            "base_classes": [
              "Data"
            ],
            "display_name": "Link list to Markdown",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "markdown_output",
                "display_name": "Markdown Output",
                "method": "process_urls",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "search_results",
              "title_selector",
              "content_selector"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "URLToMarkdownConverter-NsAl9"
        },
        "selected": false,
        "width": 384,
        "height": 464,
        "dragging": false,
        "positionAbsolute": {
          "x": 1202.7406686737822,
          "y": 506.2862402256809
        }
      },
      {
        "id": "ToolCallingAgent-06f55",
        "type": "genericNode",
        "position": {
          "x": 1793.3953363755927,
          "y": 408.5745378757249
        },
        "data": {
          "type": "ToolCallingAgent",
          "node": {
            "template": {
              "_type": "Component",
              "data_list": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "data_list",
                "display_name": "Data List",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "The list of data to iterate over.",
                "title_case": false,
                "type": "other"
              },
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "llm",
                "display_name": "LLM",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "The language model to use for processing.",
                "title_case": false,
                "type": "other"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import csv\r\nimport os\r\nfrom typing import List\r\nfrom loguru import logger\r\n\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import (\r\n    MessageTextInput,\r\n    IntInput,\r\n    DataInput,\r\n    Output,\r\n    MultilineInput,\r\n    HandleInput,\r\n    StrInput,\r\n)\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass DynamicLLMDataIteratorComponent(Component):\r\n    display_name = \"Dynamic LLM Data Iterator\"\r\n    description = (\r\n        \"Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, \"\r\n        \"store the result, and optionally save the results to a CSV file.\\n\\n\"\r\n      \r\n    )\r\n    icon = \"🔁\"\r\n    base_type = \"component\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"data_list\", display_name=\"Data List\", info=\"The list of data to iterate over.\", is_list=True),\r\n        IntInput(name=\"start_index\", display_name=\"Start Index\", info=\"The index to start processing from.\", value=0),\r\n        IntInput(name=\"iteration_count\", display_name=\"Iteration Count\", info=\"Number of items to process. Set to 0 to process all remaining items.\", value=0),\r\n        MultilineInput(\r\n            name=\"prompt_template\",\r\n            display_name=\"Prompt Template\",\r\n            info=\"The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.\",\r\n            value=\"Based on the domain {Domain} and the meta {Meta}, do X\",\r\n        ),\r\n        MessageTextInput(name=\"output_key\", display_name=\"Output Key\", info=\"The key to store the LLM output in the data.\"),\r\n        HandleInput(\r\n            name=\"llm\",\r\n            display_name=\"LLM\",\r\n            input_types=[\"LanguageModel\"],\r\n            info=\"The language model to use for processing.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"file_path\",\r\n            display_name=\"CSV File Path\",\r\n            info=\"The path to save the CSV file. If not provided, no CSV will be saved.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\r\n    ]\r\n\r\n    def append_to_csv(self, file_path: str, data: dict):\r\n        file_exists = os.path.isfile(file_path)\r\n        \r\n        with open(file_path, 'a', newline='') as csvfile:\r\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\r\n            \r\n            if not file_exists:\r\n                writer.writeheader()\r\n            \r\n            writer.writerow(data)\r\n\r\n    def construct_prompt(self, template: str, data: dict) -> str:\r\n        return template.format(**data)\r\n\r\n    def process_data(self) -> List[Data]:\r\n        logger.info(\"Starting data processing\")\r\n        data_list: List[Data] = self.data_list\r\n        start_index: int = max(0, self.start_index)  # Ensure start_index is not negative\r\n        iteration_count: int = self.iteration_count\r\n        prompt_template: str = self.prompt_template\r\n        output_key: str = self.output_key\r\n        file_path: str = self.file_path\r\n\r\n        logger.debug(f\"Number of data items: {len(data_list)}\")\r\n        logger.debug(f\"Start index: {start_index}\")\r\n        logger.debug(f\"Iteration count: {iteration_count}\")\r\n        logger.debug(f\"Prompt template: {prompt_template}\")\r\n        logger.debug(f\"Output key: {output_key}\")\r\n        logger.debug(f\"CSV file path: {file_path}\")\r\n\r\n        llm = self.llm\r\n\r\n        # Adjust iteration_count based on start_index and remaining items\r\n        if iteration_count <= 0 or (start_index + iteration_count) > len(data_list):\r\n            iteration_count = len(data_list) - start_index\r\n        \r\n        end_index = start_index + iteration_count\r\n        logger.info(f\"Will process items from index {start_index} to {end_index - 1}\")\r\n\r\n        processed_data = []\r\n\r\n        for index, item in enumerate(data_list[start_index:end_index], start=start_index):\r\n            logger.debug(f\"Processing item {index}/{end_index - 1}\")\r\n            if not isinstance(item.data, dict):\r\n                logger.error(f\"Data item {index} is not a dictionary\")\r\n                raise ValueError(f\"Data item {index} is not a dictionary\")\r\n\r\n            prompt = self.construct_prompt(prompt_template, item.data)\r\n            logger.debug(f\"Generated prompt: {prompt}\")\r\n            \r\n            try:\r\n                logger.debug(\"Invoking LLM\")\r\n                llm_output = llm.invoke(prompt).content\r\n                logger.debug(f\"LLM output: {llm_output}\")\r\n                new_item = item.data.copy()\r\n                new_item[output_key] = llm_output\r\n                processed_data.append(Data(data=new_item))\r\n                logger.info(f\"Successfully processed item {index}\")\r\n                \r\n                if file_path:\r\n                    self.append_to_csv(file_path, new_item)\r\n                    logger.info(f\"Appended item {index} to CSV\")\r\n            except Exception as e:\r\n                logger.error(f\"Error processing item {index}: {str(e)}\")\r\n                self.status = f\"Error processing item {index}: {str(e)}\"\r\n                return processed_data\r\n\r\n        self.status = processed_data\r\n        logger.info(f\"Finished processing. {len(processed_data)} items processed successfully.\")\r\n        return processed_data",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "file_path": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "test",
                "name": "file_path",
                "display_name": "CSV File Path",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The path to save the CSV file. If not provided, no CSV will be saved.",
                "title_case": false,
                "type": "str"
              },
              "iteration_count": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "10",
                "name": "iteration_count",
                "display_name": "Iteration Count",
                "advanced": false,
                "dynamic": false,
                "info": "Number of items to process. Set to 0 to process all remaining items.",
                "title_case": false,
                "type": "int"
              },
              "output_key": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "context",
                "name": "output_key",
                "display_name": "Output Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The key to store the LLM output in the data.",
                "title_case": false,
                "type": "str"
              },
              "prompt_template": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "You are an expert in employer branding and content analysis. Given the following article about employer branding, please provide:\n\n1. A list of 5-7 relevant tags (single words or short phrases) that capture the key concepts, themes, or topics discussed in the article.\n\n2. A primary category for the article. Choose the most appropriate one from the following list: Strategy, Case Study, Trends, How-To, Interview, Leadership Insights, Employer Brand Philosophy, Best Practices, Industry Analysis, Talent Attraction, Employee Engagement, Diversity and Inclusion, Employer Value Proposition (EVP), Recruitment Marketing, Workplace Culture, Candidate Experience, Employer Brand Metrics, Employer Brand Storytelling, Talent Retention, Employer Brand Technology\n\n3. A brief (2-3 sentence) summary of the main points or key takeaways from the article.\n\n4. The target audience for this article (e.g., HR professionals, C-suite executives, recruitment marketers, etc.)\n\nArticle (in markdown format):\n{markdown}\n\nPlease format your response as follows:\n\nTags:\nPrimary Category:\nSummary:\nTarget Audience:",
                "name": "prompt_template",
                "display_name": "Prompt Template",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.",
                "title_case": false,
                "type": "str"
              },
              "start_index": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0",
                "name": "start_index",
                "display_name": "Start Index",
                "advanced": false,
                "dynamic": false,
                "info": "The index to start processing from.",
                "title_case": false,
                "type": "int"
              }
            },
            "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n",
            "icon": "🔁",
            "base_classes": [
              "Data"
            ],
            "display_name": "LLM Data Iterator",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "processed_data",
                "display_name": "Processed Data",
                "method": "process_data",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "data_list",
              "start_index",
              "iteration_count",
              "prompt_template",
              "output_key",
              "llm",
              "file_path"
            ],
            "beta": false,
            "edited": true,
            "official": false,
            "lf_version": "1.0.18"
          },
          "id": "ToolCallingAgent-06f55"
        },
        "selected": false,
        "width": 384,
        "height": 736,
        "positionAbsolute": {
          "x": 1793.3953363755927,
          "y": 408.5745378757249
        },
        "dragging": false
      },
      {
        "id": "OllamaModel-KDQMa",
        "type": "genericNode",
        "position": {
          "x": 1155.9109239284596,
          "y": 1089.9898102850777
        },
        "data": {
          "type": "OllamaModel",
          "node": {
            "template": {
              "_type": "Component",
              "base_url": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "base_url",
                "value": "http://localhost:11434",
                "display_name": "Base URL",
                "advanced": false,
                "dynamic": false,
                "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "format": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "format",
                "value": "",
                "display_name": "Format",
                "advanced": true,
                "dynamic": false,
                "info": "Specify the format of the output (e.g., json).",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "metadata": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "metadata",
                "value": {},
                "display_name": "Metadata",
                "advanced": true,
                "dynamic": false,
                "info": "Metadata to add to the run trace.",
                "title_case": false,
                "type": "dict",
                "_input_type": "DictInput"
              },
              "mirostat": {
                "trace_as_metadata": true,
                "options": [
                  "Disabled",
                  "Mirostat",
                  "Mirostat 2.0"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat",
                "value": "Disabled",
                "display_name": "Mirostat",
                "advanced": true,
                "dynamic": false,
                "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                "real_time_refresh": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "mirostat_eta": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_eta",
                "value": "",
                "display_name": "Mirostat Eta",
                "advanced": true,
                "dynamic": false,
                "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "mirostat_tau": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "mirostat_tau",
                "value": "",
                "display_name": "Mirostat Tau",
                "advanced": true,
                "dynamic": false,
                "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "mxbai-embed-large:latest",
                  "llama3.2:latest",
                  "llama3.1:latest"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "llama3.2:latest",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "Refer to https://ollama.com/library for more models.",
                "refresh_button": true,
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "num_ctx": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_ctx",
                "value": "",
                "display_name": "Context Window Size",
                "advanced": true,
                "dynamic": false,
                "info": "Size of the context window for generating tokens. (Default: 2048)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_gpu": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_gpu",
                "value": "",
                "display_name": "Number of GPUs",
                "advanced": true,
                "dynamic": false,
                "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "num_thread": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "num_thread",
                "value": "",
                "display_name": "Number of Threads",
                "advanced": true,
                "dynamic": false,
                "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_last_n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_last_n",
                "value": "",
                "display_name": "Repeat Last N",
                "advanced": true,
                "dynamic": false,
                "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "repeat_penalty": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "repeat_penalty",
                "value": "",
                "display_name": "Repeat Penalty",
                "advanced": true,
                "dynamic": false,
                "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "stop_tokens": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stop_tokens",
                "value": "",
                "display_name": "Stop Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system",
                "value": "",
                "display_name": "System",
                "advanced": true,
                "dynamic": false,
                "info": "System to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "system_message": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "tags": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tags",
                "value": "",
                "display_name": "Tags",
                "advanced": true,
                "dynamic": false,
                "info": "Comma-separated list of tags to add to the run trace.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.2,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Controls the creativity of model responses.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "template": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "",
                "display_name": "Template",
                "advanced": true,
                "dynamic": false,
                "info": "Template to use for generating text.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "tfs_z": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tfs_z",
                "value": "",
                "display_name": "TFS Z",
                "advanced": true,
                "dynamic": false,
                "info": "Tail free sampling value. (Default: 1)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "timeout": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "timeout",
                "value": "",
                "display_name": "Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "Timeout for the request stream.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Limits token selection to top K. (Default: 40)",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "Works together with top-k. (Default: 0.9)",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "verbose": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "verbose",
                "value": false,
                "display_name": "Verbose",
                "advanced": false,
                "dynamic": false,
                "info": "Whether to print out response text.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Generate text using Ollama Local LLMs.",
            "icon": "Ollama",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Ollama",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "base_url",
              "model_name",
              "temperature",
              "format",
              "metadata",
              "mirostat",
              "mirostat_eta",
              "mirostat_tau",
              "num_ctx",
              "num_gpu",
              "num_thread",
              "repeat_last_n",
              "repeat_penalty",
              "tfs_z",
              "timeout",
              "top_k",
              "top_p",
              "verbose",
              "tags",
              "stop_tokens",
              "system",
              "template"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "OllamaModel-KDQMa"
        },
        "selected": false,
        "width": 384,
        "height": 685,
        "positionAbsolute": {
          "x": 1155.9109239284596,
          "y": 1089.9898102850777
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "TextInput-KBnlG",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KBnlGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "SitemapFilter-FwNeR",
        "targetHandle": "{œfieldNameœ:œsitemap_urlœ,œidœ:œSitemapFilter-FwNeRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sitemap_url",
            "id": "SitemapFilter-FwNeR",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-KBnlG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-KBnlG{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KBnlGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-SitemapFilter-FwNeR{œfieldNameœ:œsitemap_urlœ,œidœ:œSitemapFilter-FwNeRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "SitemapFilter-FwNeR",
        "sourceHandle": "{œdataTypeœ:œSitemapFilterœ,œidœ:œSitemapFilter-FwNeRœ,œnameœ:œfiltered_urlsœ,œoutput_typesœ:[œDataœ]}",
        "target": "URLToMarkdownConverter-NsAl9",
        "targetHandle": "{œfieldNameœ:œsearch_resultsœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "search_results",
            "id": "URLToMarkdownConverter-NsAl9",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "SitemapFilter",
            "id": "SitemapFilter-FwNeR",
            "name": "filtered_urls",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-SitemapFilter-FwNeR{œdataTypeœ:œSitemapFilterœ,œidœ:œSitemapFilter-FwNeRœ,œnameœ:œfiltered_urlsœ,œoutput_typesœ:[œDataœ]}-URLToMarkdownConverter-NsAl9{œfieldNameœ:œsearch_resultsœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "URLToMarkdownConverter-NsAl9",
        "sourceHandle": "{œdataTypeœ:œURLToMarkdownConverterœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œnameœ:œmarkdown_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ToolCallingAgent-06f55",
        "targetHandle": "{œfieldNameœ:œdata_listœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "data_list",
            "id": "ToolCallingAgent-06f55",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "URLToMarkdownConverter",
            "id": "URLToMarkdownConverter-NsAl9",
            "name": "markdown_output",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-URLToMarkdownConverter-NsAl9{œdataTypeœ:œURLToMarkdownConverterœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œnameœ:œmarkdown_outputœ,œoutput_typesœ:[œDataœ]}-ToolCallingAgent-06f55{œfieldNameœ:œdata_listœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "OllamaModel-KDQMa",
        "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-KDQMaœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ToolCallingAgent-06f55",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ToolCallingAgent-06f55",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "OllamaModel",
            "id": "OllamaModel-KDQMa",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-OllamaModel-KDQMa{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-KDQMaœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ToolCallingAgent-06f55{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "className": ""
      }
    ],
    "viewport": {
      "x": 62.7877727267844,
      "y": -271.4472612433585,
      "zoom": 0.797544442800213
    }
  },
  "metadata": {
    "TextInput": {
      "count": 1
    },
    "SitemapFilter": {
      "count": 1
    },
    "URLToMarkdownConverter": {
      "count": 1
    },
    "ToolCallingAgent": {
      "count": 1
    },
    "OllamaModel": {
      "count": 1
    },
    "total": 5
  },
  "original": {
    "id": "d8f80963-439e-4818-a874-9f64b3f82b9f",
    "name": "Scrape, Markdown and Classify",
    "description": "",
    "is_component": false,
    "liked_by_count": "2",
    "downloads_count": "51",
    "metadata": {
      "TextInput": {
        "count": 1
      },
      "SitemapFilter": {
        "count": 1
      },
      "URLToMarkdownConverter": {
        "count": 1
      },
      "ToolCallingAgent": {
        "count": 1
      },
      "OllamaModel": {
        "count": 1
      },
      "total": 5
    },
    "last_tested_version": "1.0.18",
    "private": false,
    "data": {
      "nodes": [
        {
          "id": "TextInput-KBnlG",
          "type": "genericNode",
          "position": {
            "x": 42.448925005496676,
            "y": 632.6680353213949
          },
          "data": {
            "type": "TextInput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "https://www.happydance.love/sitemap.xml",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Text to be passed as input.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "Enter the full sitemap URL",
              "icon": "type",
              "base_classes": [
                "Message"
              ],
              "display_name": "Sitemap URL",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "TextInput-KBnlG"
          },
          "selected": false,
          "width": 384,
          "height": 302,
          "dragging": false,
          "positionAbsolute": {
            "x": 42.448925005496676,
            "y": 632.6680353213949
          }
        },
        {
          "id": "SitemapFilter-FwNeR",
          "type": "genericNode",
          "position": {
            "x": 587.0265495830818,
            "y": 582.2979116314294
          },
          "data": {
            "type": "SitemapFilter",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import xml.etree.ElementTree as ET\nfrom typing import List\nimport httpx\n\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema import Data\n\nclass SitemapFilterComponent(Component):\n    display_name = \"Sitemap Fetch & Filter\"\n    description = \"Fetchs an XML sitemap, parses the URLs and allows you to filter the results. (e.g., '/blog/')\"\n    icon = \"book-open-text\"\n    name = \"SitemapFilter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"sitemap_url\",\n            display_name=\"Sitemap URL\",\n            info=\"Enter the URL of the sitemap to process, including http(s)://\",\n        ),\n        MessageTextInput(\n            name=\"filter_criteria\",\n            display_name=\"Filter\",\n            info=\"Value to filter URLs by (e.g., '/blog/').\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Filtered URLs\", name=\"filtered_urls\", method=\"process_sitemap\"),\n    ]\n\n    def parse_xml(self, xml_data: str) -> List[str]:\n        \"\"\"Parses the XML data and extracts all URLs from <loc> elements.\"\"\"\n        try:\n            root = ET.fromstring(xml_data)\n            urls = [url.find(\"{http://www.sitemaps.org/schemas/sitemap/0.9}loc\").text \n                    for url in root.findall(\"{http://www.sitemaps.org/schemas/sitemap/0.9}url\")]\n            return urls\n        except ET.ParseError as e:\n            raise ValueError(f\"Error parsing XML: {e}\")\n\n    async def process_sitemap(self) -> List[Data]:\n        sitemap_url = self.sitemap_url\n        filter_criteria = self.filter_criteria\n\n        async with httpx.AsyncClient() as client:\n            # Fetch the sitemap\n            try:\n                response = await client.get(sitemap_url)\n                sitemap_content = response.text\n            except Exception as e:\n                return [Data(data={\"error\": f\"Failed to fetch sitemap: {e}\"})]\n\n            # Parse and filter URLs\n            try:\n                all_urls = self.parse_xml(sitemap_content)\n                filtered_urls = [url for url in all_urls if filter_criteria in url]\n            except Exception as e:\n                return [Data(data={\"error\": f\"Failed to parse or filter sitemap: {e}\"})]\n\n        # Create a list of Data objects, each containing a filtered URL\n        result = [Data(data={\"link\": url}) for url in filtered_urls]\n        \n        self.status = result\n        return result",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "filter_criteria": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "filter_criteria",
                  "value": "/resources/insights/https://www.happydance.love/resources/insights/how-good-housekee",
                  "display_name": "Filter",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Value to filter URLs by (e.g., '/blog/').",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "sitemap_url": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sitemap_url",
                  "value": "",
                  "display_name": "Sitemap URL",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Enter the URL of the sitemap to process, including http(s)://",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "Fetchs an XML sitemap, parses the URLs and allows you to filter the results. (e.g., '/blog/')",
              "icon": "book-open-text",
              "base_classes": [
                "Data"
              ],
              "display_name": "Sitemap Filter",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "filtered_urls",
                  "display_name": "Filtered URLs",
                  "method": "process_sitemap",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "sitemap_url",
                "filter_criteria"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "SitemapFilter-FwNeR"
          },
          "selected": false,
          "width": 384,
          "height": 416,
          "dragging": false,
          "positionAbsolute": {
            "x": 587.0265495830818,
            "y": 582.2979116314294
          }
        },
        {
          "id": "URLToMarkdownConverter-NsAl9",
          "type": "genericNode",
          "position": {
            "x": 1202.7406686737822,
            "y": 506.2862402256809
          },
          "data": {
            "type": "URLToMarkdownConverter",
            "node": {
              "template": {
                "_type": "Component",
                "search_results": {
                  "trace_as_metadata": true,
                  "list": false,
                  "trace_as_input": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "search_results",
                  "value": "",
                  "display_name": "Search results with URLs",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "List of retrieved URLs from a search engine to convert to Markdown",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "DataInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import requests\nimport html2text\nfrom loguru import logger\nfrom bs4 import BeautifulSoup\nfrom typing import List, Dict\nfrom axiestudio.schema import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.schema.dotdict import dotdict\nfrom axiestudio.io import DataInput, MessageTextInput, Output\nimport os\nfrom urllib.parse import urlparse\n\nclass URLToMarkdownConverter(Component):\n    display_name = \"Link list to Markdown\"\n    description = \"Fetches HTML from URLs, parses it, and converts the HTML to Markdown.\"\n    icon = \"heading-1\"\n    inputs = [\n        DataInput(\n            name=\"search_results\",\n            display_name=\"Search results with URLs\",\n            info=\"List of retrieved URLs from a search engine to convert to Markdown\",\n        ),\n        MessageTextInput(\n            name=\"title_selector\",\n            display_name=\"Title Selector\",\n            info=\"HTML selector for the title element, e.g 'h1.blog-title'\",\n        ),\n        MessageTextInput(\n            name=\"content_selector\",\n            display_name=\"Content Selector\",\n            info=\"HTML selector for the content, e.g 'div.content\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Markdown Output\", name=\"markdown_output\", method=\"process_urls\"),\n    ]\n    def process_urls(self) -> List[Data]:\n        search_results: List[Data] = self.search_results\n        title_selector: str = self.title_selector\n        content_selector: str = self.content_selector\n        processed_urls = []\n        \n        for result in search_results:\n            url = result.link\n            new_item = result.data.copy()\n            \n            try:\n                logger.info(f\"Processing URL: {url}\")\n                # 1. Extract HTML from the response\n                response = requests.get(url)\n                html_content = response.text\n                # 2. Parse the HTML\n                soup = BeautifulSoup(html_content, 'html.parser')\n                # 3. Extract specific content using the provided selectors\n                title_element = soup.select_one(title_selector) if title_selector else None\n                if not title_element:\n                    # Fallback to using the page title if no specific title element is found\n                    title_element = soup.find('title')\n                content_element = soup.select_one(content_selector)\n                # Check if both elements exist\n                if title_element and content_element:\n                    # Combine the relevant parts into HTML\n                    title_text = title_element.get_text().strip()\n                    extracted_html = f\"<h1>{title_text}</h1>\\n{str(content_element)}\"\n                    \n                    # 4. Convert the extracted HTML to Markdown\n                    h = html2text.HTML2Text()\n                    h.ignore_links = True  # This will remove all hyperlinks\n                    h.ignore_images = True  # This will remove all images\n                    \n                    # Convert the content to markdown\n                    markdown_content = h.handle(extracted_html)\n                    \n                    # Remove extra newlines or empty headings (like '## ')\n                    markdown_content = markdown_content.replace(\"##\\n\\n\", \"## \").replace(\"##\\n\", \"## \")\n                    \n                    # Clean up additional unnecessary newlines\n                    markdown_content = markdown_content.replace(\"\\n\\n\", \"\\n\").strip()\n                    # Ensure headings don't have extra spaces or newlines\n                    markdown_content = markdown_content.replace(\"##  \\n\", \"## \").replace(\"##\\n\", \"## \").strip()\n                    \n                    # Generate a file name based on the URL\n                    parsed_url = urlparse(url)\n                    file_name = f\"{parsed_url.path.replace('/', '_')}.md\".strip('_')\n                    new_item[\"title\"] = title_text\n                    new_item[\"markdown\"] = markdown_content\n                    new_item[\"file_name\"] = file_name\n                    processed_urls.append(Data(data=new_item))\n                else:\n                    logger.error(f\"Missing required elements in {url}\")\n            \n            except Exception as e:\n                logger.error(f\"Error processing {url}: {e}\")\n                raise\n        \n        return processed_urls",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "content_selector": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "content_selector",
                  "value": "div.cms-content",
                  "display_name": "Content Selector",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "HTML selector for the content, e.g 'div.content",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "title_selector": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "title_selector",
                  "value": "h1.section-blog-single__title",
                  "display_name": "Title Selector",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "HTML selector for the title element, e.g 'h1.blog-title'",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "Fetches HTML from URLs, parses it, and converts the HTML to Markdown.",
              "icon": "heading-1",
              "base_classes": [
                "Data"
              ],
              "display_name": "Link list to Markdown",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "markdown_output",
                  "display_name": "Markdown Output",
                  "method": "process_urls",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "search_results",
                "title_selector",
                "content_selector"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "URLToMarkdownConverter-NsAl9"
          },
          "selected": false,
          "width": 384,
          "height": 464,
          "dragging": false,
          "positionAbsolute": {
            "x": 1202.7406686737822,
            "y": 506.2862402256809
          }
        },
        {
          "id": "ToolCallingAgent-06f55",
          "type": "genericNode",
          "position": {
            "x": 1793.3953363755927,
            "y": 408.5745378757249
          },
          "data": {
            "type": "ToolCallingAgent",
            "node": {
              "template": {
                "_type": "Component",
                "data_list": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "data_list",
                  "display_name": "Data List",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "The list of data to iterate over.",
                  "title_case": false,
                  "type": "other"
                },
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "llm",
                  "display_name": "LLM",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "The language model to use for processing.",
                  "title_case": false,
                  "type": "other"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import csv\r\nimport os\r\nfrom typing import List\r\nfrom loguru import logger\r\n\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import (\r\n    MessageTextInput,\r\n    IntInput,\r\n    DataInput,\r\n    Output,\r\n    MultilineInput,\r\n    HandleInput,\r\n    StrInput,\r\n)\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass DynamicLLMDataIteratorComponent(Component):\r\n    display_name = \"Dynamic LLM Data Iterator\"\r\n    description = (\r\n        \"Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, \"\r\n        \"store the result, and optionally save the results to a CSV file.\\n\\n\"\r\n      \r\n    )\r\n    icon = \"🔁\"\r\n    base_type = \"component\"\r\n\r\n    inputs = [\r\n        DataInput(name=\"data_list\", display_name=\"Data List\", info=\"The list of data to iterate over.\", is_list=True),\r\n        IntInput(name=\"start_index\", display_name=\"Start Index\", info=\"The index to start processing from.\", value=0),\r\n        IntInput(name=\"iteration_count\", display_name=\"Iteration Count\", info=\"Number of items to process. Set to 0 to process all remaining items.\", value=0),\r\n        MultilineInput(\r\n            name=\"prompt_template\",\r\n            display_name=\"Prompt Template\",\r\n            info=\"The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.\",\r\n            value=\"Based on the domain {Domain} and the meta {Meta}, do X\",\r\n        ),\r\n        MessageTextInput(name=\"output_key\", display_name=\"Output Key\", info=\"The key to store the LLM output in the data.\"),\r\n        HandleInput(\r\n            name=\"llm\",\r\n            display_name=\"LLM\",\r\n            input_types=[\"LanguageModel\"],\r\n            info=\"The language model to use for processing.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"file_path\",\r\n            display_name=\"CSV File Path\",\r\n            info=\"The path to save the CSV file. If not provided, no CSV will be saved.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Processed Data\", name=\"processed_data\", method=\"process_data\"),\r\n    ]\r\n\r\n    def append_to_csv(self, file_path: str, data: dict):\r\n        file_exists = os.path.isfile(file_path)\r\n        \r\n        with open(file_path, 'a', newline='') as csvfile:\r\n            writer = csv.DictWriter(csvfile, fieldnames=data.keys())\r\n            \r\n            if not file_exists:\r\n                writer.writeheader()\r\n            \r\n            writer.writerow(data)\r\n\r\n    def construct_prompt(self, template: str, data: dict) -> str:\r\n        return template.format(**data)\r\n\r\n    def process_data(self) -> List[Data]:\r\n        logger.info(\"Starting data processing\")\r\n        data_list: List[Data] = self.data_list\r\n        start_index: int = max(0, self.start_index)  # Ensure start_index is not negative\r\n        iteration_count: int = self.iteration_count\r\n        prompt_template: str = self.prompt_template\r\n        output_key: str = self.output_key\r\n        file_path: str = self.file_path\r\n\r\n        logger.debug(f\"Number of data items: {len(data_list)}\")\r\n        logger.debug(f\"Start index: {start_index}\")\r\n        logger.debug(f\"Iteration count: {iteration_count}\")\r\n        logger.debug(f\"Prompt template: {prompt_template}\")\r\n        logger.debug(f\"Output key: {output_key}\")\r\n        logger.debug(f\"CSV file path: {file_path}\")\r\n\r\n        llm = self.llm\r\n\r\n        # Adjust iteration_count based on start_index and remaining items\r\n        if iteration_count <= 0 or (start_index + iteration_count) > len(data_list):\r\n            iteration_count = len(data_list) - start_index\r\n        \r\n        end_index = start_index + iteration_count\r\n        logger.info(f\"Will process items from index {start_index} to {end_index - 1}\")\r\n\r\n        processed_data = []\r\n\r\n        for index, item in enumerate(data_list[start_index:end_index], start=start_index):\r\n            logger.debug(f\"Processing item {index}/{end_index - 1}\")\r\n            if not isinstance(item.data, dict):\r\n                logger.error(f\"Data item {index} is not a dictionary\")\r\n                raise ValueError(f\"Data item {index} is not a dictionary\")\r\n\r\n            prompt = self.construct_prompt(prompt_template, item.data)\r\n            logger.debug(f\"Generated prompt: {prompt}\")\r\n            \r\n            try:\r\n                logger.debug(\"Invoking LLM\")\r\n                llm_output = llm.invoke(prompt).content\r\n                logger.debug(f\"LLM output: {llm_output}\")\r\n                new_item = item.data.copy()\r\n                new_item[output_key] = llm_output\r\n                processed_data.append(Data(data=new_item))\r\n                logger.info(f\"Successfully processed item {index}\")\r\n                \r\n                if file_path:\r\n                    self.append_to_csv(file_path, new_item)\r\n                    logger.info(f\"Appended item {index} to CSV\")\r\n            except Exception as e:\r\n                logger.error(f\"Error processing item {index}: {str(e)}\")\r\n                self.status = f\"Error processing item {index}: {str(e)}\"\r\n                return processed_data\r\n\r\n        self.status = processed_data\r\n        logger.info(f\"Finished processing. {len(processed_data)} items processed successfully.\")\r\n        return processed_data",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "file_path": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "test",
                  "name": "file_path",
                  "display_name": "CSV File Path",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The path to save the CSV file. If not provided, no CSV will be saved.",
                  "title_case": false,
                  "type": "str"
                },
                "iteration_count": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "10",
                  "name": "iteration_count",
                  "display_name": "Iteration Count",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Number of items to process. Set to 0 to process all remaining items.",
                  "title_case": false,
                  "type": "int"
                },
                "output_key": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "context",
                  "name": "output_key",
                  "display_name": "Output Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The key to store the LLM output in the data.",
                  "title_case": false,
                  "type": "str"
                },
                "prompt_template": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "You are an expert in employer branding and content analysis. Given the following article about employer branding, please provide:\n\n1. A list of 5-7 relevant tags (single words or short phrases) that capture the key concepts, themes, or topics discussed in the article.\n\n2. A primary category for the article. Choose the most appropriate one from the following list: Strategy, Case Study, Trends, How-To, Interview, Leadership Insights, Employer Brand Philosophy, Best Practices, Industry Analysis, Talent Attraction, Employee Engagement, Diversity and Inclusion, Employer Value Proposition (EVP), Recruitment Marketing, Workplace Culture, Candidate Experience, Employer Brand Metrics, Employer Brand Storytelling, Talent Retention, Employer Brand Technology\n\n3. A brief (2-3 sentence) summary of the main points or key takeaways from the article.\n\n4. The target audience for this article (e.g., HR professionals, C-suite executives, recruitment marketers, etc.)\n\nArticle (in markdown format):\n{markdown}\n\nPlease format your response as follows:\n\nTags:\nPrimary Category:\nSummary:\nTarget Audience:",
                  "name": "prompt_template",
                  "display_name": "Prompt Template",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The prompt template to use for each item. Use `{key}` to refer to values in the data. For example, 'Based on the domain {Domain} and the meta {Meta}, do X'.",
                  "title_case": false,
                  "type": "str"
                },
                "start_index": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0",
                  "name": "start_index",
                  "display_name": "Start Index",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The index to start processing from.",
                  "title_case": false,
                  "type": "int"
                }
              },
              "description": "Iterate over a list of data, process each item using a specified LLM with a dynamically constructed prompt, store the result, and optionally save the results to a CSV file.\n\n",
              "icon": "🔁",
              "base_classes": [
                "Data"
              ],
              "display_name": "LLM Data Iterator",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "processed_data",
                  "display_name": "Processed Data",
                  "method": "process_data",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "data_list",
                "start_index",
                "iteration_count",
                "prompt_template",
                "output_key",
                "llm",
                "file_path"
              ],
              "beta": false,
              "edited": true,
              "official": false,
              "lf_version": "1.0.18"
            },
            "id": "ToolCallingAgent-06f55"
          },
          "selected": false,
          "width": 384,
          "height": 736,
          "positionAbsolute": {
            "x": 1793.3953363755927,
            "y": 408.5745378757249
          },
          "dragging": false
        },
        {
          "id": "OllamaModel-KDQMa",
          "type": "genericNode",
          "position": {
            "x": 1155.9109239284596,
            "y": 1089.9898102850777
          },
          "data": {
            "type": "OllamaModel",
            "node": {
              "template": {
                "_type": "Component",
                "base_url": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "base_url",
                  "value": "http://localhost:11434",
                  "display_name": "Base URL",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "format": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "format",
                  "value": "",
                  "display_name": "Format",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Specify the format of the output (e.g., json).",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageInput"
                },
                "metadata": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "metadata",
                  "value": {},
                  "display_name": "Metadata",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Metadata to add to the run trace.",
                  "title_case": false,
                  "type": "dict",
                  "_input_type": "DictInput"
                },
                "mirostat": {
                  "trace_as_metadata": true,
                  "options": [
                    "Disabled",
                    "Mirostat",
                    "Mirostat 2.0"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat",
                  "value": "Disabled",
                  "display_name": "Mirostat",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Enable/disable Mirostat sampling for controlling perplexity.",
                  "real_time_refresh": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "mirostat_eta": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat_eta",
                  "value": "",
                  "display_name": "Mirostat Eta",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Learning rate for Mirostat algorithm. (Default: 0.1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "mirostat_tau": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "mirostat_tau",
                  "value": "",
                  "display_name": "Mirostat Tau",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "mxbai-embed-large:latest",
                    "llama3.2:latest",
                    "llama3.1:latest"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "llama3.2:latest",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Refer to https://ollama.com/library for more models.",
                  "refresh_button": true,
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "num_ctx": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_ctx",
                  "value": "",
                  "display_name": "Context Window Size",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Size of the context window for generating tokens. (Default: 2048)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "num_gpu": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_gpu",
                  "value": "",
                  "display_name": "Number of GPUs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "num_thread": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "num_thread",
                  "value": "",
                  "display_name": "Number of Threads",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of threads to use during computation. (Default: detected for optimal performance)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "repeat_last_n": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "repeat_last_n",
                  "value": "",
                  "display_name": "Repeat Last N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "repeat_penalty": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "repeat_penalty",
                  "value": "",
                  "display_name": "Repeat Penalty",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Penalty for repetitions in generated text. (Default: 1.1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "stop_tokens": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stop_tokens",
                  "value": "",
                  "display_name": "Stop Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tokens to signal the model to stop generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stream",
                  "value": false,
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "system": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system",
                  "value": "",
                  "display_name": "System",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System to use for generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "system_message": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_message",
                  "value": "",
                  "display_name": "System Message",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "tags": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "tags",
                  "value": "",
                  "display_name": "Tags",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Comma-separated list of tags to add to the run trace.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.2,
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Controls the creativity of model responses.",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "template": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "",
                  "display_name": "Template",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Template to use for generating text.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "tfs_z": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "tfs_z",
                  "value": "",
                  "display_name": "TFS Z",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Tail free sampling value. (Default: 1)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "timeout": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "timeout",
                  "value": "",
                  "display_name": "Timeout",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Timeout for the request stream.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_k": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_k",
                  "value": "",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Limits token selection to top K. (Default: 40)",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_p",
                  "value": "",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Works together with top-k. (Default: 0.9)",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "verbose": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "verbose",
                  "value": false,
                  "display_name": "Verbose",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Whether to print out response text.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Generate text using Ollama Local LLMs.",
              "icon": "Ollama",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "Ollama",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "system_message",
                "stream",
                "base_url",
                "model_name",
                "temperature",
                "format",
                "metadata",
                "mirostat",
                "mirostat_eta",
                "mirostat_tau",
                "num_ctx",
                "num_gpu",
                "num_thread",
                "repeat_last_n",
                "repeat_penalty",
                "tfs_z",
                "timeout",
                "top_k",
                "top_p",
                "verbose",
                "tags",
                "stop_tokens",
                "system",
                "template"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "OllamaModel-KDQMa"
          },
          "selected": false,
          "width": 384,
          "height": 685,
          "positionAbsolute": {
            "x": 1155.9109239284596,
            "y": 1089.9898102850777
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "source": "TextInput-KBnlG",
          "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KBnlGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "SitemapFilter-FwNeR",
          "targetHandle": "{œfieldNameœ:œsitemap_urlœ,œidœ:œSitemapFilter-FwNeRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sitemap_url",
              "id": "SitemapFilter-FwNeR",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "TextInput",
              "id": "TextInput-KBnlG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-TextInput-KBnlG{œdataTypeœ:œTextInputœ,œidœ:œTextInput-KBnlGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-SitemapFilter-FwNeR{œfieldNameœ:œsitemap_urlœ,œidœ:œSitemapFilter-FwNeRœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "SitemapFilter-FwNeR",
          "sourceHandle": "{œdataTypeœ:œSitemapFilterœ,œidœ:œSitemapFilter-FwNeRœ,œnameœ:œfiltered_urlsœ,œoutput_typesœ:[œDataœ]}",
          "target": "URLToMarkdownConverter-NsAl9",
          "targetHandle": "{œfieldNameœ:œsearch_resultsœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "search_results",
              "id": "URLToMarkdownConverter-NsAl9",
              "inputTypes": [
                "Data"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "SitemapFilter",
              "id": "SitemapFilter-FwNeR",
              "name": "filtered_urls",
              "output_types": [
                "Data"
              ]
            }
          },
          "id": "reactflow__edge-SitemapFilter-FwNeR{œdataTypeœ:œSitemapFilterœ,œidœ:œSitemapFilter-FwNeRœ,œnameœ:œfiltered_urlsœ,œoutput_typesœ:[œDataœ]}-URLToMarkdownConverter-NsAl9{œfieldNameœ:œsearch_resultsœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "URLToMarkdownConverter-NsAl9",
          "sourceHandle": "{œdataTypeœ:œURLToMarkdownConverterœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œnameœ:œmarkdown_outputœ,œoutput_typesœ:[œDataœ]}",
          "target": "ToolCallingAgent-06f55",
          "targetHandle": "{œfieldNameœ:œdata_listœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "data_list",
              "id": "ToolCallingAgent-06f55",
              "inputTypes": [
                "Data"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "URLToMarkdownConverter",
              "id": "URLToMarkdownConverter-NsAl9",
              "name": "markdown_output",
              "output_types": [
                "Data"
              ]
            }
          },
          "id": "reactflow__edge-URLToMarkdownConverter-NsAl9{œdataTypeœ:œURLToMarkdownConverterœ,œidœ:œURLToMarkdownConverter-NsAl9œ,œnameœ:œmarkdown_outputœ,œoutput_typesœ:[œDataœ]}-ToolCallingAgent-06f55{œfieldNameœ:œdata_listœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "OllamaModel-KDQMa",
          "sourceHandle": "{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-KDQMaœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ToolCallingAgent-06f55",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ToolCallingAgent-06f55",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "OllamaModel",
              "id": "OllamaModel-KDQMa",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-OllamaModel-KDQMa{œdataTypeœ:œOllamaModelœ,œidœ:œOllamaModel-KDQMaœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ToolCallingAgent-06f55{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-06f55œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "className": ""
        }
      ],
      "viewport": {
        "x": 62.7877727267844,
        "y": -271.4472612433585,
        "zoom": 0.797544442800213
      }
    },
    "date_created": "2024-09-30T17:18:19.263Z",
    "date_updated": "2024-09-30T17:18:19.302Z",
    "status": "Public",
    "sort": null,
    "user_updated": "d37448a0-3dae-485a-8bce-0671ce46cf30",
    "user_created": {
      "username": "Roy",
      "first_name": "Roy",
      "last_name": "Barber",
      "id": "d37448a0-3dae-485a-8bce-0671ce46cf30"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:06.393Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 36,
    "converter_version": "1.0.0"
  }
}