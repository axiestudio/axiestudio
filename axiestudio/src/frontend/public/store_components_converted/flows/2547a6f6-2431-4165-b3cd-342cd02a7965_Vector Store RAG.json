{
  "id": "2547a6f6-2431-4165-b3cd-342cd02a7965",
  "name": "Vector Store RAG",
  "description": "Visit https://docs.langflow.org/tutorials/rag-with-astradb for a detailed guide of this project.\nThis project give you both Ingestion and RAG in a single file. You'll need to visit https://astra.datastax.com/ to create an Astra DB instance, your Token and grab an API Endpoint.\nRunning this project requires you to add a file in the Files component, then define a Collection Name and click on the Play icon on the Astra DB component. \n\nAfter the ingestion ends you are ready to click on the Run button at the lower left corner and start asking questions about your data. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "Obed",
    "first_name": "Gyamfi",
    "last_name": "Obed",
    "id": "26c62a8d-57eb-4241-8262-566d8381827f",
    "full_name": "Gyamfi Obed"
  },
  "store_url": "https://www.langflow.store/store/component/2547a6f6-2431-4165-b3cd-342cd02a7965",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-07-21T08:59:25.232Z",
    "updated": "2024-07-21T08:59:25.307Z",
    "downloaded": "2025-08-19T17:50:06.760Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.9",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "data": {
          "description": "Get chat inputs from the Playground.",
          "display_name": "Chat Input",
          "id": "ChatInput-WGpXS",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "frozen": false,
            "icon": "ChatInput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "hello"
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "advanced": false,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "Session ID for the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "store_message": {
                "advanced": false,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "ChatInput"
        },
        "dragging": false,
        "height": 479,
        "id": "ChatInput-WGpXS",
        "position": {
          "x": 1003.9107843843408,
          "y": 239.88374197442118
        },
        "positionAbsolute": {
          "x": 1003.9107843843408,
          "y": 239.88374197442118
        },
        "selected": true,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Convert Data into plain text following a specified template.",
          "display_name": "Parse Data",
          "id": "ParseData-ZddC9",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data",
            "documentation": "",
            "edited": false,
            "field_order": [
              "data",
              "template",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Text",
                "method": "parse_data",
                "name": "text",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
              },
              "data": {
                "advanced": false,
                "display_name": "Data",
                "dynamic": false,
                "info": "The data to convert to text.",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "name": "data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sep": {
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "sep",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              }
            }
          },
          "type": "ParseData"
        },
        "dragging": false,
        "height": 385,
        "id": "ParseData-ZddC9",
        "position": {
          "x": 2153.671089776635,
          "y": 1353.4864029645425
        },
        "positionAbsolute": {
          "x": 2153.671089776635,
          "y": 1353.4864029645425
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-rlQvd",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n"
              },
              "context": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "context",
                "display_name": "context",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "question": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "question",
                "display_name": "question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "You are an assistant at for The Automation Ghana Group electrical product store. Your task is to provide  helpful, concise, conversational and accurate responses to customer inquiries based solely on the given products data. If question is a greeting respond and if you cannot find the information in the database, you should respond with \"I'm sorry, but I don't have the information to answer that question based on the context provided.\" Safety should always be your top priority. You should also refer to the chat history to provide context-based short and minimal responses.\nYour responses should be in the format of product name, price and website link to the product\n\nProduct Database: {context}\n\nChat History: {history}\n\nQuestion: {question}\n\nAnswer: "
              },
              "history": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "history",
                "display_name": "history",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "context",
                "history",
                "question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 611,
        "id": "Prompt-rlQvd",
        "position": {
          "x": 2705.30084340658,
          "y": 301.20141378289935
        },
        "positionAbsolute": {
          "x": 2705.30084340658,
          "y": 301.20141378289935
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-ZYG0f",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "ChatOutput",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "data_template": {
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "advanced": false,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "Session ID for the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "store_message": {
                "advanced": false,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 479,
        "id": "ChatOutput-ZYG0f",
        "position": {
          "x": 3959.573101873183,
          "y": 624.407700051725
        },
        "positionAbsolute": {
          "x": 3959.573101873183,
          "y": 624.407700051725
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "id": "OllamaEmbeddings-al4a7",
        "type": "genericNode",
        "position": {
          "x": 832.5847531372211,
          "y": 850.507083534803
        },
        "data": {
          "type": "OllamaEmbeddings",
          "node": {
            "template": {
              "_type": "Component",
              "base_url": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "http://localhost:11434",
                "name": "base_url",
                "display_name": "Ollama Base URL",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "model": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "nomic-embed-text",
                "name": "model",
                "display_name": "Ollama Model",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 0.1,
                "name": "temperature",
                "display_name": "Model Temperature",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float"
              }
            },
            "description": "Generate embeddings using Ollama models.",
            "icon": "Ollama",
            "base_classes": [
              "Embeddings"
            ],
            "display_name": "Ollama Embeddings",
            "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": true,
            "outputs": [
              {
                "types": [
                  "Embeddings"
                ],
                "selected": "Embeddings",
                "name": "embeddings",
                "display_name": "Embeddings",
                "method": "build_embeddings",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "model",
              "base_url",
              "temperature"
            ],
            "beta": false,
            "edited": false
          },
          "id": "OllamaEmbeddings-al4a7"
        },
        "selected": false,
        "width": 384,
        "height": 411,
        "positionAbsolute": {
          "x": 832.5847531372211,
          "y": 850.507083534803
        },
        "dragging": false
      },
      {
        "id": "MistralModel-olKP3",
        "type": "genericNode",
        "position": {
          "x": 3340.126284338947,
          "y": 283.14530419073776
        },
        "data": {
          "type": "MistralModel",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_mistralai import ChatMistralAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageInput, SecretStrInput, StrInput\n\n\nclass MistralAIModelComponent(LCModelComponent):\n    display_name = \"MistralAI\"\n    description = \"Generates text using MistralAI LLMs.\"\n    icon = \"MistralAI\"\n    name = \"MistralModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"open-mixtral-8x7b\",\n                \"open-mixtral-8x22b\",\n                \"mistral-small-latest\",\n                \"mistral-medium-latest\",\n                \"mistral-large-latest\",\n                \"codestral-latest\",\n            ],\n            value=\"codestral-latest\",\n        ),\n        StrInput(\n            name=\"mistral_api_base\",\n            display_name=\"Mistral API Base\",\n            advanced=True,\n            info=(\n                \"The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. \"\n                \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\"\n            ),\n        ),\n        SecretStrInput(\n            name=\"mistral_api_key\",\n            display_name=\"Mistral API Key\",\n            info=\"The Mistral API Key to use for the Mistral model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", advanced=False, value=0.5),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", advanced=True, value=5),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", advanced=True, value=60),\n        IntInput(name=\"max_concurrent_requests\", display_name=\"Max Concurrent Requests\", advanced=True, value=3),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", advanced=True, value=1),\n        IntInput(name=\"random_seed\", display_name=\"Random Seed\", value=1, advanced=True),\n        BoolInput(name=\"safe_mode\", display_name=\"Safe Mode\", advanced=True),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        mistral_api_key = self.mistral_api_key\n        temperature = self.temperature\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        mistral_api_base = self.mistral_api_base or \"https://api.mistral.ai/v1\"\n        max_retries = self.max_retries\n        timeout = self.timeout\n        max_concurrent_requests = self.max_concurrent_requests\n        top_p = self.top_p\n        random_seed = self.random_seed\n        safe_mode = self.safe_mode\n\n        if mistral_api_key:\n            api_key = SecretStr(mistral_api_key)\n        else:\n            api_key = None\n\n        output = ChatMistralAI(\n            max_tokens=max_tokens or None,\n            model_name=model_name,\n            endpoint=mistral_api_base,\n            api_key=api_key,\n            temperature=temperature,\n            max_retries=max_retries,\n            timeout=timeout,\n            max_concurrent_requests=max_concurrent_requests,\n            top_p=top_p,\n            random_seed=random_seed,\n            safe_mode=safe_mode,\n        )\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "input_value",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "max_concurrent_requests": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 3,
                "name": "max_concurrent_requests",
                "display_name": "Max Concurrent Requests",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int"
              },
              "max_retries": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 5,
                "name": "max_retries",
                "display_name": "Max Retries",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int"
              },
              "max_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "max_tokens",
                "display_name": "Max Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "title_case": false,
                "type": "int"
              },
              "mistral_api_base": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "mistral_api_base",
                "display_name": "Mistral API Base",
                "advanced": true,
                "dynamic": false,
                "info": "The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "title_case": false,
                "type": "str"
              },
              "mistral_api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "mistral_api_key",
                "display_name": "Mistral API Key",
                "advanced": false,
                "input_types": [],
                "dynamic": false,
                "info": "The Mistral API Key to use for the Mistral model.",
                "title_case": false,
                "password": true,
                "type": "str"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "open-mixtral-8x7b",
                  "open-mixtral-8x22b",
                  "mistral-small-latest",
                  "mistral-medium-latest",
                  "mistral-large-latest",
                  "codestral-latest"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "open-mixtral-8x22b",
                "name": "model_name",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "random_seed": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1,
                "name": "random_seed",
                "display_name": "Random Seed",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int"
              },
              "safe_mode": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "safe_mode",
                "display_name": "Safe Mode",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "bool"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "stream",
                "display_name": "Stream",
                "advanced": false,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool"
              },
              "system_message": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "system_message",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "0.4",
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float"
              },
              "timeout": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 60,
                "name": "timeout",
                "display_name": "Timeout",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": 1,
                "name": "top_p",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float"
              }
            },
            "description": "Generates text using MistralAI LLMs.",
            "icon": "MistralAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "MistralAI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "max_tokens",
              "model_name",
              "mistral_api_base",
              "mistral_api_key",
              "temperature",
              "stream",
              "system_message",
              "max_retries",
              "timeout",
              "max_concurrent_requests",
              "top_p",
              "random_seed",
              "safe_mode"
            ],
            "beta": false,
            "edited": false
          },
          "id": "MistralModel-olKP3"
        },
        "selected": false,
        "width": 384,
        "height": 774,
        "positionAbsolute": {
          "x": 3340.126284338947,
          "y": 283.14530419073776
        },
        "dragging": false
      },
      {
        "id": "FAISS-bnH97",
        "type": "genericNode",
        "position": {
          "x": 1501.813074585886,
          "y": 382.7315184838569
        },
        "data": {
          "type": "FAISS",
          "node": {
            "template": {
              "_type": "Component",
              "embedding": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "embedding",
                "display_name": "Embedding",
                "advanced": false,
                "input_types": [
                  "Embeddings"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other"
              },
              "ingest_data": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "ingest_data",
                "display_name": "Ingest Data",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other"
              },
              "allow_dangerous_deserialization": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "allow_dangerous_deserialization",
                "display_name": "Allow Dangerous Deserialization",
                "advanced": false,
                "dynamic": false,
                "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.",
                "title_case": false,
                "type": "bool"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import List\n\nfrom langchain_community.vectorstores import FAISS\nfrom loguru import logger\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom axiestudio.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    FAISS Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"axiestudio_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    def build_vector_store(self) -> FAISS:\n        \"\"\"\n        Builds the FAISS object.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to save the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> List[Data]:\n        \"\"\"\n        Search for documents in the FAISS vector store.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to load the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        vector_store = FAISS.load_local(\n            folder_path=path,\n            embeddings=self.embedding,\n            index_name=self.index_name,\n            allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n        )\n\n        if not vector_store:\n            raise ValueError(\"Failed to load the FAISS index.\")\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            logger.debug(data)\n            return data  # Return the search results data\n        else:\n            logger.debug(\"No search input provided. Skipping search.\")\n            return []\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "index_name": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "axiestudio_index",
                "name": "index_name",
                "display_name": "Index Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              },
              "number_of_results": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "5",
                "name": "number_of_results",
                "display_name": "Number of Results",
                "advanced": false,
                "dynamic": false,
                "info": "Number of results to return.",
                "title_case": false,
                "type": "int"
              },
              "persist_directory": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "D:\\GYAMFI OBED\\GTP_PROJECT\\database",
                "name": "persist_directory",
                "display_name": "Persist Directory",
                "advanced": false,
                "dynamic": false,
                "info": "Path to save the FAISS index. It will be relative to where Langflow is running.",
                "title_case": false,
                "type": "str"
              },
              "search_query": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "search_query",
                "display_name": "Search Query",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "FAISS Vector Store with search capabilities",
            "icon": "FAISS",
            "base_classes": [
              "Data",
              "Retriever",
              "VectorStore"
            ],
            "display_name": "FAISS",
            "documentation": "https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Retriever"
                ],
                "selected": "Retriever",
                "name": "base_retriever",
                "display_name": "Retriever",
                "method": "build_base_retriever",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "search_results",
                "display_name": "Search Results",
                "method": "search_documents",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "VectorStore"
                ],
                "selected": "VectorStore",
                "name": "vector_store",
                "display_name": "Vector Store",
                "method": "cast_vector_store",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "index_name",
              "persist_directory",
              "search_query",
              "ingest_data",
              "allow_dangerous_deserialization",
              "embedding",
              "number_of_results"
            ],
            "beta": false,
            "edited": false
          },
          "id": "FAISS-bnH97"
        },
        "selected": false,
        "width": 384,
        "height": 835,
        "positionAbsolute": {
          "x": 1501.813074585886,
          "y": 382.7315184838569
        },
        "dragging": false
      },
      {
        "id": "Memory-ZfEbX",
        "type": "genericNode",
        "position": {
          "x": 2107.8216904906744,
          "y": 269.2701579854199
        },
        "data": {
          "type": "Memory",
          "node": {
            "template": {
              "_type": "Component",
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "memory",
                "display_name": "External Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMessageHistory"
                ],
                "dynamic": false,
                "info": "Retrieve messages from an external memory. If empty, it will use the Langflow tables.",
                "title_case": false,
                "type": "other"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.inputs import HandleInput\nfrom axiestudio.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import get_messages, LCBuiltinChatMemory\nfrom axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\nfrom axiestudio.field_typing import BaseChatMemory\nfrom langchain.memory import ConversationBufferMemory\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\", \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"Session ID of the chat history.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            if sender:\n                expected_type = \"Machine\" if sender == \"Machine\" else \"User\"\n                stored = [m for m in stored if m.type == expected_type]\n            if order == \"ASC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.graph.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "n_messages": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "2",
                "name": "n_messages",
                "display_name": "Number of Messages",
                "advanced": false,
                "dynamic": false,
                "info": "Number of messages to retrieve.",
                "title_case": false,
                "type": "int"
              },
              "order": {
                "trace_as_metadata": true,
                "options": [
                  "Ascending",
                  "Descending"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Ascending",
                "name": "order",
                "display_name": "Order",
                "advanced": false,
                "dynamic": false,
                "info": "Order of the messages.",
                "title_case": false,
                "type": "str"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User",
                  "Machine and User"
                ],
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "Machine and User",
                "name": "sender",
                "display_name": "Sender Type",
                "advanced": false,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "sender_name",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "session_id",
                "display_name": "Session ID",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Session ID of the chat history.",
                "title_case": false,
                "type": "str"
              },
              "template": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "{sender_name}: {text}",
                "name": "template",
                "display_name": "Template",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Retrieves stored chat messages from Langflow tables or an external memory.",
            "icon": "message-square-more",
            "base_classes": [
              "BaseChatMemory",
              "Data",
              "Message"
            ],
            "display_name": "Chat Memory",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "messages",
                "display_name": "Messages (Data)",
                "method": "retrieve_messages",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "messages_text",
                "display_name": "Messages (Text)",
                "method": "retrieve_messages_as_text",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "BaseChatMemory"
                ],
                "selected": "BaseChatMemory",
                "name": "lc_memory",
                "display_name": "Memory",
                "method": "build_lc_memory",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "memory",
              "sender",
              "sender_name",
              "n_messages",
              "session_id",
              "order",
              "template"
            ],
            "beta": false,
            "edited": false
          },
          "id": "Memory-ZfEbX"
        },
        "selected": false,
        "width": 384,
        "height": 833,
        "positionAbsolute": {
          "x": 2107.8216904906744,
          "y": 269.2701579854199
        },
        "dragging": false
      },
      {
        "id": "TextInput-4732V",
        "type": "genericNode",
        "position": {
          "x": 490.27116081760244,
          "y": 480.9110359252828
        },
        "data": {
          "type": "TextInput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "session1\n",
                "name": "input_value",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text to be passed as input.",
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Get text inputs from the Playground.",
            "icon": "type",
            "base_classes": [
              "Message"
            ],
            "display_name": "Text Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value"
            ],
            "beta": false,
            "edited": false
          },
          "id": "TextInput-4732V"
        },
        "selected": false,
        "width": 384,
        "height": 309,
        "positionAbsolute": {
          "x": 490.27116081760244,
          "y": 480.9110359252828
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-WGpXS",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-rlQvd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-ChatInput-WGpXS{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œquestionœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "source": "ChatInput-WGpXS",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-rlQvd",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "selected": false
      },
      {
        "source": "ChatInput-WGpXS",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "FAISS-bnH97",
        "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "search_query",
            "id": "FAISS-bnH97",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-WGpXS",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-WGpXS{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-FAISS-bnH97{œfieldNameœ:œsearch_queryœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "FAISS-bnH97",
        "sourceHandle": "{œdataTypeœ:œFAISSœ,œidœ:œFAISS-bnH97œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
        "target": "ParseData-ZddC9",
        "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-ZddC9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "data",
            "id": "ParseData-ZddC9",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "FAISS",
            "id": "FAISS-bnH97",
            "name": "search_results",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-FAISS-bnH97{œdataTypeœ:œFAISSœ,œidœ:œFAISS-bnH97œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-ZddC9{œfieldNameœ:œdataœ,œidœ:œParseData-ZddC9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "ParseData-ZddC9",
        "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-ZddC9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-rlQvd",
        "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "context",
            "id": "Prompt-rlQvd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ParseData",
            "id": "ParseData-ZddC9",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ParseData-ZddC9{œdataTypeœ:œParseDataœ,œidœ:œParseData-ZddC9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œcontextœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Memory-ZfEbX",
        "sourceHandle": "{œdataTypeœ:œMemoryœ,œidœ:œMemory-ZfEbXœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-rlQvd",
        "targetHandle": "{œfieldNameœ:œhistoryœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "history",
            "id": "Prompt-rlQvd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Memory",
            "id": "Memory-ZfEbX",
            "name": "messages_text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Memory-ZfEbX{œdataTypeœ:œMemoryœ,œidœ:œMemory-ZfEbXœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œhistoryœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "OllamaEmbeddings-al4a7",
        "sourceHandle": "{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-al4a7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
        "target": "FAISS-bnH97",
        "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "embedding",
            "id": "FAISS-bnH97",
            "inputTypes": [
              "Embeddings"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "OllamaEmbeddings",
            "id": "OllamaEmbeddings-al4a7",
            "name": "embeddings",
            "output_types": [
              "Embeddings"
            ]
          }
        },
        "id": "reactflow__edge-OllamaEmbeddings-al4a7{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-al4a7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-FAISS-bnH97{œfieldNameœ:œembeddingœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "TextInput-4732V",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-4732Vœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatInput-WGpXS",
        "targetHandle": "{œfieldNameœ:œsession_idœ,œidœ:œChatInput-WGpXSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "session_id",
            "id": "ChatInput-WGpXS",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-4732V",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-4732V{œdataTypeœ:œTextInputœ,œidœ:œTextInput-4732Vœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ChatInput-WGpXS{œfieldNameœ:œsession_idœ,œidœ:œChatInput-WGpXSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Prompt-rlQvd",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-rlQvdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "MistralModel-olKP3",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-olKP3œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "MistralModel-olKP3",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-rlQvd",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-rlQvd{œdataTypeœ:œPromptœ,œidœ:œPrompt-rlQvdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MistralModel-olKP3{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-olKP3œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "MistralModel-olKP3",
        "sourceHandle": "{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-olKP3œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-ZYG0f",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ZYG0fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-ZYG0f",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "MistralModel",
            "id": "MistralModel-olKP3",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-MistralModel-olKP3{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-olKP3œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ZYG0f{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ZYG0fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      }
    ],
    "viewport": {
      "x": -1028.5121642132874,
      "y": 231.86058171509274,
      "zoom": 0.3793960435637289
    }
  },
  "metadata": {
    "ChatInput": {
      "count": 1
    },
    "ParseData": {
      "count": 1
    },
    "Prompt": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "OllamaEmbeddings": {
      "count": 1
    },
    "MistralModel": {
      "count": 1
    },
    "FAISS": {
      "count": 1
    },
    "Memory": {
      "count": 1
    },
    "TextInput": {
      "count": 1
    },
    "total": 9
  },
  "original": {
    "id": "2547a6f6-2431-4165-b3cd-342cd02a7965",
    "name": "Vector Store RAG",
    "description": "Visit https://docs.langflow.org/tutorials/rag-with-astradb for a detailed guide of this project.\nThis project give you both Ingestion and RAG in a single file. You'll need to visit https://astra.datastax.com/ to create an Astra DB instance, your Token and grab an API Endpoint.\nRunning this project requires you to add a file in the Files component, then define a Collection Name and click on the Play icon on the Astra DB component. \n\nAfter the ingestion ends you are ready to click on the Run button at the lower left corner and start asking questions about your data.",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "17",
    "metadata": {
      "ChatInput": {
        "count": 1
      },
      "ParseData": {
        "count": 1
      },
      "Prompt": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "OllamaEmbeddings": {
        "count": 1
      },
      "MistralModel": {
        "count": 1
      },
      "FAISS": {
        "count": 1
      },
      "Memory": {
        "count": 1
      },
      "TextInput": {
        "count": 1
      },
      "total": 9
    },
    "last_tested_version": "1.0.9",
    "private": false,
    "data": {
      "nodes": [
        {
          "data": {
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "id": "ChatInput-WGpXS",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Get chat inputs from the Playground.",
              "display_name": "Chat Input",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "store_message",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "frozen": false,
              "icon": "ChatInput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "files": {
                  "advanced": true,
                  "display_name": "Files",
                  "dynamic": false,
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "file_path": "",
                  "info": "Files to be sent with the message.",
                  "list": true,
                  "name": "files",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "file",
                  "value": ""
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "hello"
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "User"
                },
                "session_id": {
                  "advanced": false,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "store_message": {
                  "advanced": false,
                  "display_name": "Store Messages",
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "list": false,
                  "name": "store_message",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "ChatInput"
          },
          "dragging": false,
          "height": 479,
          "id": "ChatInput-WGpXS",
          "position": {
            "x": 1003.9107843843408,
            "y": 239.88374197442118
          },
          "positionAbsolute": {
            "x": 1003.9107843843408,
            "y": 239.88374197442118
          },
          "selected": true,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Convert Data into plain text following a specified template.",
            "display_name": "Parse Data",
            "id": "ParseData-ZddC9",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Convert Data into plain text following a specified template.",
              "display_name": "Parse Data",
              "documentation": "",
              "edited": false,
              "field_order": [
                "data",
                "template",
                "sep"
              ],
              "frozen": false,
              "icon": "braces",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Text",
                  "method": "parse_data",
                  "name": "text",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.io import DataInput, MultilineInput, Output, StrInput\nfrom axiestudio.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n"
                },
                "data": {
                  "advanced": false,
                  "display_name": "Data",
                  "dynamic": false,
                  "info": "The data to convert to text.",
                  "input_types": [
                    "Data"
                  ],
                  "list": false,
                  "name": "data",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "sep": {
                  "advanced": true,
                  "display_name": "Separator",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "sep",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                }
              }
            },
            "type": "ParseData"
          },
          "dragging": false,
          "height": 385,
          "id": "ParseData-ZddC9",
          "position": {
            "x": 2153.671089776635,
            "y": 1353.4864029645425
          },
          "positionAbsolute": {
            "x": 2153.671089776635,
            "y": 1353.4864029645425
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-rlQvd",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n"
                },
                "context": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "context",
                  "display_name": "context",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "question": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "question",
                  "display_name": "question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "You are an assistant at for The Automation Ghana Group electrical product store. Your task is to provide  helpful, concise, conversational and accurate responses to customer inquiries based solely on the given products data. If question is a greeting respond and if you cannot find the information in the database, you should respond with \"I'm sorry, but I don't have the information to answer that question based on the context provided.\" Safety should always be your top priority. You should also refer to the chat history to provide context-based short and minimal responses.\nYour responses should be in the format of product name, price and website link to the product\n\nProduct Database: {context}\n\nChat History: {history}\n\nQuestion: {question}\n\nAnswer: "
                },
                "history": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "history",
                  "display_name": "history",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "context",
                  "history",
                  "question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false
            },
            "type": "Prompt"
          },
          "dragging": false,
          "height": 611,
          "id": "Prompt-rlQvd",
          "position": {
            "x": 2705.30084340658,
            "y": 301.20141378289935
          },
          "positionAbsolute": {
            "x": 2705.30084340658,
            "y": 301.20141378289935
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "id": "ChatOutput-ZYG0f",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Display a chat message in the Playground.",
              "display_name": "Chat Output",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "store_message",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "frozen": false,
              "icon": "ChatOutput",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "data_template": {
                  "advanced": true,
                  "display_name": "Data Template",
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "data_template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Machine"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "AI"
                },
                "session_id": {
                  "advanced": false,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "Session ID for the message.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "store_message": {
                  "advanced": false,
                  "display_name": "Store Messages",
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "list": false,
                  "name": "store_message",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "ChatOutput"
          },
          "dragging": false,
          "height": 479,
          "id": "ChatOutput-ZYG0f",
          "position": {
            "x": 3959.573101873183,
            "y": 624.407700051725
          },
          "positionAbsolute": {
            "x": 3959.573101873183,
            "y": 624.407700051725
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "id": "OllamaEmbeddings-al4a7",
          "type": "genericNode",
          "position": {
            "x": 832.5847531372211,
            "y": 850.507083534803
          },
          "data": {
            "type": "OllamaEmbeddings",
            "node": {
              "template": {
                "_type": "Component",
                "base_url": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "http://localhost:11434",
                  "name": "base_url",
                  "display_name": "Ollama Base URL",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_community.embeddings import OllamaEmbeddings\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import Embeddings\nfrom axiestudio.io import FloatInput, MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"llama2\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Model Temperature\",\n            value=0.1,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(\n                model=self.model,\n                base_url=self.base_url,\n                temperature=self.temperature,\n            )  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not connect to Ollama API.\") from e\n        return output\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "model": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "nomic-embed-text",
                  "name": "model",
                  "display_name": "Ollama Model",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 0.1,
                  "name": "temperature",
                  "display_name": "Model Temperature",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float"
                }
              },
              "description": "Generate embeddings using Ollama models.",
              "icon": "Ollama",
              "base_classes": [
                "Embeddings"
              ],
              "display_name": "Ollama Embeddings",
              "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": true,
              "outputs": [
                {
                  "types": [
                    "Embeddings"
                  ],
                  "selected": "Embeddings",
                  "name": "embeddings",
                  "display_name": "Embeddings",
                  "method": "build_embeddings",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "model",
                "base_url",
                "temperature"
              ],
              "beta": false,
              "edited": false
            },
            "id": "OllamaEmbeddings-al4a7"
          },
          "selected": false,
          "width": 384,
          "height": 411,
          "positionAbsolute": {
            "x": 832.5847531372211,
            "y": 850.507083534803
          },
          "dragging": false
        },
        {
          "id": "MistralModel-olKP3",
          "type": "genericNode",
          "position": {
            "x": 3340.126284338947,
            "y": 283.14530419073776
          },
          "data": {
            "type": "MistralModel",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_mistralai import ChatMistralAI\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import BoolInput, DropdownInput, FloatInput, IntInput, MessageInput, SecretStrInput, StrInput\n\n\nclass MistralAIModelComponent(LCModelComponent):\n    display_name = \"MistralAI\"\n    description = \"Generates text using MistralAI LLMs.\"\n    icon = \"MistralAI\"\n    name = \"MistralModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"open-mixtral-8x7b\",\n                \"open-mixtral-8x22b\",\n                \"mistral-small-latest\",\n                \"mistral-medium-latest\",\n                \"mistral-large-latest\",\n                \"codestral-latest\",\n            ],\n            value=\"codestral-latest\",\n        ),\n        StrInput(\n            name=\"mistral_api_base\",\n            display_name=\"Mistral API Base\",\n            advanced=True,\n            info=(\n                \"The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. \"\n                \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\"\n            ),\n        ),\n        SecretStrInput(\n            name=\"mistral_api_key\",\n            display_name=\"Mistral API Key\",\n            info=\"The Mistral API Key to use for the Mistral model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", advanced=False, value=0.5),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(name=\"max_retries\", display_name=\"Max Retries\", advanced=True, value=5),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", advanced=True, value=60),\n        IntInput(name=\"max_concurrent_requests\", display_name=\"Max Concurrent Requests\", advanced=True, value=3),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", advanced=True, value=1),\n        IntInput(name=\"random_seed\", display_name=\"Random Seed\", value=1, advanced=True),\n        BoolInput(name=\"safe_mode\", display_name=\"Safe Mode\", advanced=True),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        mistral_api_key = self.mistral_api_key\n        temperature = self.temperature\n        model_name = self.model_name\n        max_tokens = self.max_tokens\n        mistral_api_base = self.mistral_api_base or \"https://api.mistral.ai/v1\"\n        max_retries = self.max_retries\n        timeout = self.timeout\n        max_concurrent_requests = self.max_concurrent_requests\n        top_p = self.top_p\n        random_seed = self.random_seed\n        safe_mode = self.safe_mode\n\n        if mistral_api_key:\n            api_key = SecretStr(mistral_api_key)\n        else:\n            api_key = None\n\n        output = ChatMistralAI(\n            max_tokens=max_tokens or None,\n            model_name=model_name,\n            endpoint=mistral_api_base,\n            api_key=api_key,\n            temperature=temperature,\n            max_retries=max_retries,\n            timeout=timeout,\n            max_concurrent_requests=max_concurrent_requests,\n            top_p=top_p,\n            random_seed=random_seed,\n            safe_mode=safe_mode,\n        )\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "input_value",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "max_concurrent_requests": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 3,
                  "name": "max_concurrent_requests",
                  "display_name": "Max Concurrent Requests",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int"
                },
                "max_retries": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 5,
                  "name": "max_retries",
                  "display_name": "Max Retries",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int"
                },
                "max_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "max_tokens",
                  "display_name": "Max Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                  "title_case": false,
                  "type": "int"
                },
                "mistral_api_base": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "mistral_api_base",
                  "display_name": "Mistral API Base",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The base URL of the Mistral API. Defaults to https://api.mistral.ai/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                  "title_case": false,
                  "type": "str"
                },
                "mistral_api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "mistral_api_key",
                  "display_name": "Mistral API Key",
                  "advanced": false,
                  "input_types": [],
                  "dynamic": false,
                  "info": "The Mistral API Key to use for the Mistral model.",
                  "title_case": false,
                  "password": true,
                  "type": "str"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "open-mixtral-8x7b",
                    "open-mixtral-8x22b",
                    "mistral-small-latest",
                    "mistral-medium-latest",
                    "mistral-large-latest",
                    "codestral-latest"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "open-mixtral-8x22b",
                  "name": "model_name",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "random_seed": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1,
                  "name": "random_seed",
                  "display_name": "Random Seed",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int"
                },
                "safe_mode": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "safe_mode",
                  "display_name": "Safe Mode",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "bool"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "stream",
                  "display_name": "Stream",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool"
                },
                "system_message": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "system_message",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "0.4",
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float"
                },
                "timeout": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 60,
                  "name": "timeout",
                  "display_name": "Timeout",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": 1,
                  "name": "top_p",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float"
                }
              },
              "description": "Generates text using MistralAI LLMs.",
              "icon": "MistralAI",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "MistralAI",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "max_tokens",
                "model_name",
                "mistral_api_base",
                "mistral_api_key",
                "temperature",
                "stream",
                "system_message",
                "max_retries",
                "timeout",
                "max_concurrent_requests",
                "top_p",
                "random_seed",
                "safe_mode"
              ],
              "beta": false,
              "edited": false
            },
            "id": "MistralModel-olKP3"
          },
          "selected": false,
          "width": 384,
          "height": 774,
          "positionAbsolute": {
            "x": 3340.126284338947,
            "y": 283.14530419073776
          },
          "dragging": false
        },
        {
          "id": "FAISS-bnH97",
          "type": "genericNode",
          "position": {
            "x": 1501.813074585886,
            "y": 382.7315184838569
          },
          "data": {
            "type": "FAISS",
            "node": {
              "template": {
                "_type": "Component",
                "embedding": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "embedding",
                  "display_name": "Embedding",
                  "advanced": false,
                  "input_types": [
                    "Embeddings"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other"
                },
                "ingest_data": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "ingest_data",
                  "display_name": "Ingest Data",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other"
                },
                "allow_dangerous_deserialization": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "allow_dangerous_deserialization",
                  "display_name": "Allow Dangerous Deserialization",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.",
                  "title_case": false,
                  "type": "bool"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import List\n\nfrom langchain_community.vectorstores import FAISS\nfrom loguru import logger\n\nfrom axiestudio.base.vectorstores.model import LCVectorStoreComponent\nfrom axiestudio.helpers.data import docs_to_data\nfrom axiestudio.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom axiestudio.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"\n    FAISS Vector Store with search capabilities\n    \"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"axiestudio_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    def build_vector_store(self) -> FAISS:\n        \"\"\"\n        Builds the FAISS object.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to save the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> List[Data]:\n        \"\"\"\n        Search for documents in the FAISS vector store.\n        \"\"\"\n        if not self.persist_directory:\n            raise ValueError(\"Folder path is required to load the FAISS index.\")\n        path = self.resolve_path(self.persist_directory)\n\n        vector_store = FAISS.load_local(\n            folder_path=path,\n            embeddings=self.embedding,\n            index_name=self.index_name,\n            allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n        )\n\n        if not vector_store:\n            raise ValueError(\"Failed to load the FAISS index.\")\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            logger.debug(data)\n            return data  # Return the search results data\n        else:\n            logger.debug(\"No search input provided. Skipping search.\")\n            return []\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "index_name": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "axiestudio_index",
                  "name": "index_name",
                  "display_name": "Index Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                },
                "number_of_results": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "5",
                  "name": "number_of_results",
                  "display_name": "Number of Results",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Number of results to return.",
                  "title_case": false,
                  "type": "int"
                },
                "persist_directory": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "D:\\GYAMFI OBED\\GTP_PROJECT\\database",
                  "name": "persist_directory",
                  "display_name": "Persist Directory",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Path to save the FAISS index. It will be relative to where Langflow is running.",
                  "title_case": false,
                  "type": "str"
                },
                "search_query": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "search_query",
                  "display_name": "Search Query",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "FAISS Vector Store with search capabilities",
              "icon": "FAISS",
              "base_classes": [
                "Data",
                "Retriever",
                "VectorStore"
              ],
              "display_name": "FAISS",
              "documentation": "https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Retriever"
                  ],
                  "selected": "Retriever",
                  "name": "base_retriever",
                  "display_name": "Retriever",
                  "method": "build_base_retriever",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "search_results",
                  "display_name": "Search Results",
                  "method": "search_documents",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "VectorStore"
                  ],
                  "selected": "VectorStore",
                  "name": "vector_store",
                  "display_name": "Vector Store",
                  "method": "cast_vector_store",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "index_name",
                "persist_directory",
                "search_query",
                "ingest_data",
                "allow_dangerous_deserialization",
                "embedding",
                "number_of_results"
              ],
              "beta": false,
              "edited": false
            },
            "id": "FAISS-bnH97"
          },
          "selected": false,
          "width": 384,
          "height": 835,
          "positionAbsolute": {
            "x": 1501.813074585886,
            "y": 382.7315184838569
          },
          "dragging": false
        },
        {
          "id": "Memory-ZfEbX",
          "type": "genericNode",
          "position": {
            "x": 2107.8216904906744,
            "y": 269.2701579854199
          },
          "data": {
            "type": "Memory",
            "node": {
              "template": {
                "_type": "Component",
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "memory",
                  "display_name": "External Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMessageHistory"
                  ],
                  "dynamic": false,
                  "info": "Retrieve messages from an external memory. If empty, it will use the Langflow tables.",
                  "title_case": false,
                  "type": "other"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.helpers.data import data_to_text\nfrom axiestudio.inputs import HandleInput\nfrom axiestudio.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import get_messages, LCBuiltinChatMemory\nfrom axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\nfrom axiestudio.field_typing import BaseChatMemory\nfrom langchain.memory import ConversationBufferMemory\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\", \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"Session ID of the chat history.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            if sender:\n                expected_type = \"Machine\" if sender == \"Machine\" else \"User\"\n                stored = [m for m in stored if m.type == expected_type]\n            if order == \"ASC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.graph.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "n_messages": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "2",
                  "name": "n_messages",
                  "display_name": "Number of Messages",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Number of messages to retrieve.",
                  "title_case": false,
                  "type": "int"
                },
                "order": {
                  "trace_as_metadata": true,
                  "options": [
                    "Ascending",
                    "Descending"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Ascending",
                  "name": "order",
                  "display_name": "Order",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Order of the messages.",
                  "title_case": false,
                  "type": "str"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User",
                    "Machine and User"
                  ],
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "Machine and User",
                  "name": "sender",
                  "display_name": "Sender Type",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "sender_name",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "session_id",
                  "display_name": "Session ID",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Session ID of the chat history.",
                  "title_case": false,
                  "type": "str"
                },
                "template": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "{sender_name}: {text}",
                  "name": "template",
                  "display_name": "Template",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Retrieves stored chat messages from Langflow tables or an external memory.",
              "icon": "message-square-more",
              "base_classes": [
                "BaseChatMemory",
                "Data",
                "Message"
              ],
              "display_name": "Chat Memory",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "messages",
                  "display_name": "Messages (Data)",
                  "method": "retrieve_messages",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "messages_text",
                  "display_name": "Messages (Text)",
                  "method": "retrieve_messages_as_text",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "BaseChatMemory"
                  ],
                  "selected": "BaseChatMemory",
                  "name": "lc_memory",
                  "display_name": "Memory",
                  "method": "build_lc_memory",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "memory",
                "sender",
                "sender_name",
                "n_messages",
                "session_id",
                "order",
                "template"
              ],
              "beta": false,
              "edited": false
            },
            "id": "Memory-ZfEbX"
          },
          "selected": false,
          "width": 384,
          "height": 833,
          "positionAbsolute": {
            "x": 2107.8216904906744,
            "y": 269.2701579854199
          },
          "dragging": false
        },
        {
          "id": "TextInput-4732V",
          "type": "genericNode",
          "position": {
            "x": 490.27116081760244,
            "y": 480.9110359252828
          },
          "data": {
            "type": "TextInput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "session1\n",
                  "name": "input_value",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Text to be passed as input.",
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Get text inputs from the Playground.",
              "icon": "type",
              "base_classes": [
                "Message"
              ],
              "display_name": "Text Input",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value"
              ],
              "beta": false,
              "edited": false
            },
            "id": "TextInput-4732V"
          },
          "selected": false,
          "width": 384,
          "height": 309,
          "positionAbsolute": {
            "x": 490.27116081760244,
            "y": 480.9110359252828
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-WGpXS",
              "name": "message",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "question",
              "id": "Prompt-rlQvd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-ChatInput-WGpXS{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œquestionœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "source": "ChatInput-WGpXS",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-rlQvd",
          "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "selected": false
        },
        {
          "source": "ChatInput-WGpXS",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "FAISS-bnH97",
          "targetHandle": "{œfieldNameœ:œsearch_queryœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "search_query",
              "id": "FAISS-bnH97",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-WGpXS",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-WGpXS{œdataTypeœ:œChatInputœ,œidœ:œChatInput-WGpXSœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-FAISS-bnH97{œfieldNameœ:œsearch_queryœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "FAISS-bnH97",
          "sourceHandle": "{œdataTypeœ:œFAISSœ,œidœ:œFAISS-bnH97œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}",
          "target": "ParseData-ZddC9",
          "targetHandle": "{œfieldNameœ:œdataœ,œidœ:œParseData-ZddC9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "data",
              "id": "ParseData-ZddC9",
              "inputTypes": [
                "Data"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "FAISS",
              "id": "FAISS-bnH97",
              "name": "search_results",
              "output_types": [
                "Data"
              ]
            }
          },
          "id": "reactflow__edge-FAISS-bnH97{œdataTypeœ:œFAISSœ,œidœ:œFAISS-bnH97œ,œnameœ:œsearch_resultsœ,œoutput_typesœ:[œDataœ]}-ParseData-ZddC9{œfieldNameœ:œdataœ,œidœ:œParseData-ZddC9œ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "ParseData-ZddC9",
          "sourceHandle": "{œdataTypeœ:œParseDataœ,œidœ:œParseData-ZddC9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-rlQvd",
          "targetHandle": "{œfieldNameœ:œcontextœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "context",
              "id": "Prompt-rlQvd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ParseData",
              "id": "ParseData-ZddC9",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ParseData-ZddC9{œdataTypeœ:œParseDataœ,œidœ:œParseData-ZddC9œ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œcontextœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Memory-ZfEbX",
          "sourceHandle": "{œdataTypeœ:œMemoryœ,œidœ:œMemory-ZfEbXœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-rlQvd",
          "targetHandle": "{œfieldNameœ:œhistoryœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "history",
              "id": "Prompt-rlQvd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Memory",
              "id": "Memory-ZfEbX",
              "name": "messages_text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Memory-ZfEbX{œdataTypeœ:œMemoryœ,œidœ:œMemory-ZfEbXœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-rlQvd{œfieldNameœ:œhistoryœ,œidœ:œPrompt-rlQvdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "OllamaEmbeddings-al4a7",
          "sourceHandle": "{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-al4a7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}",
          "target": "FAISS-bnH97",
          "targetHandle": "{œfieldNameœ:œembeddingœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "embedding",
              "id": "FAISS-bnH97",
              "inputTypes": [
                "Embeddings"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "OllamaEmbeddings",
              "id": "OllamaEmbeddings-al4a7",
              "name": "embeddings",
              "output_types": [
                "Embeddings"
              ]
            }
          },
          "id": "reactflow__edge-OllamaEmbeddings-al4a7{œdataTypeœ:œOllamaEmbeddingsœ,œidœ:œOllamaEmbeddings-al4a7œ,œnameœ:œembeddingsœ,œoutput_typesœ:[œEmbeddingsœ]}-FAISS-bnH97{œfieldNameœ:œembeddingœ,œidœ:œFAISS-bnH97œ,œinputTypesœ:[œEmbeddingsœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "TextInput-4732V",
          "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-4732Vœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatInput-WGpXS",
          "targetHandle": "{œfieldNameœ:œsession_idœ,œidœ:œChatInput-WGpXSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "session_id",
              "id": "ChatInput-WGpXS",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "TextInput",
              "id": "TextInput-4732V",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-TextInput-4732V{œdataTypeœ:œTextInputœ,œidœ:œTextInput-4732Vœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ChatInput-WGpXS{œfieldNameœ:œsession_idœ,œidœ:œChatInput-WGpXSœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Prompt-rlQvd",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-rlQvdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "MistralModel-olKP3",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-olKP3œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "MistralModel-olKP3",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-rlQvd",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-rlQvd{œdataTypeœ:œPromptœ,œidœ:œPrompt-rlQvdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MistralModel-olKP3{œfieldNameœ:œinput_valueœ,œidœ:œMistralModel-olKP3œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "MistralModel-olKP3",
          "sourceHandle": "{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-olKP3œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatOutput-ZYG0f",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ZYG0fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-ZYG0f",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "MistralModel",
              "id": "MistralModel-olKP3",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-MistralModel-olKP3{œdataTypeœ:œMistralModelœ,œidœ:œMistralModel-olKP3œ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-ZYG0f{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-ZYG0fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        }
      ],
      "viewport": {
        "x": -1028.5121642132874,
        "y": 231.86058171509274,
        "zoom": 0.3793960435637289
      }
    },
    "date_created": "2024-07-21T08:59:25.232Z",
    "date_updated": "2024-07-21T08:59:25.307Z",
    "status": "Public",
    "sort": null,
    "user_updated": "26c62a8d-57eb-4241-8262-566d8381827f",
    "user_created": {
      "username": "Obed",
      "first_name": "Gyamfi",
      "last_name": "Obed",
      "id": "26c62a8d-57eb-4241-8262-566d8381827f"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:08:55.378Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 88,
    "converter_version": "1.0.0"
  }
}