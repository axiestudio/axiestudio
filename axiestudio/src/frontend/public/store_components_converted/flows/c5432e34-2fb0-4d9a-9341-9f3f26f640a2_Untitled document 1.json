{
  "id": "c5432e34-2fb0-4d9a-9341-9f3f26f640a2",
  "name": "Untitled document (1)",
  "description": "Language Models, Unleashed. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "kucingHana",
    "first_name": "hanif",
    "last_name": "Rinardi",
    "id": "7755d01a-69ab-44d2-9c36-b44a51fd53a5",
    "full_name": "hanif Rinardi"
  },
  "store_url": "https://www.langflow.store/store/component/c5432e34-2fb0-4d9a-9341-9f3f26f640a2",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-06-16T09:31:41.984Z",
    "updated": "2024-06-16T09:31:42.029Z",
    "downloaded": "2025-08-19T17:50:05.643Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "0.6.19",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "GoogleGenerativeAI-zBOow",
        "type": "genericNode",
        "position": {
          "x": -75.68572409847599,
          "y": -73.1263376818669
        },
        "data": {
          "type": "GoogleGenerativeAI",
          "node": {
            "template": {
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Optional\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI  # type: ignore\nfrom axiestudio import CustomComponent\nfrom axiestudio.field_typing import BaseLanguageModel, RangeSpec, TemplateField\nfrom pydantic.v1.types import SecretStr\n\n\nclass GoogleGenerativeAIComponent(CustomComponent):\n    display_name: str = \"Google Generative AI\"\n    description: str = \"A component that uses Google Generative AI to generate text.\"\n    documentation: str = \"http://docs.axiestudio.org/components/custom\"\n\n    def build_config(self):\n        return {\n            \"google_api_key\": TemplateField(\n                display_name=\"Google API Key\",\n                info=\"The Google API Key to use for the Google Generative AI.\",\n            ),\n            \"max_output_tokens\": TemplateField(\n                display_name=\"Max Output Tokens\",\n                info=\"The maximum number of tokens to generate.\",\n            ),\n            \"temperature\": TemplateField(\n                display_name=\"Temperature\",\n                info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            ),\n            \"top_k\": TemplateField(\n                display_name=\"Top K\",\n                info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n                range_spec=RangeSpec(min=0, max=2, step=0.1),\n                advanced=True,\n            ),\n            \"top_p\": TemplateField(\n                display_name=\"Top P\",\n                info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n                advanced=True,\n            ),\n            \"n\": TemplateField(\n                display_name=\"N\",\n                info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n                advanced=True,\n            ),\n            \"model\": TemplateField(\n                display_name=\"Model\",\n                info=\"The name of the model to use. Supported examples: gemini-pro\",\n                options=[\"gemini-pro\", \"gemini-pro-vision\"],\n            ),\n            \"code\": TemplateField(\n                advanced=True,\n            ),\n        }\n\n    def build(\n        self,\n        google_api_key: str,\n        model: str,\n        max_output_tokens: Optional[int] = None,\n        temperature: float = 0.1,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = 1,\n    ) -> BaseLanguageModel:\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,  # type: ignore\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,  # type: ignore\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key),\n        )\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "title_case": true
              },
              "google_api_key": {
                "type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "AIzaSyCSdcyAtK3yMSvnR6UciccuRP-5DBpNaXM",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "google_api_key",
                "display_name": "Google API Key",
                "advanced": false,
                "dynamic": false,
                "info": "The Google API Key to use for the Google Generative AI.",
                "title_case": true
              },
              "max_output_tokens": {
                "type": "int",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "5000",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "max_output_tokens",
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": true
              },
              "model": {
                "type": "str",
                "required": false,
                "placeholder": "",
                "list": true,
                "show": true,
                "multiline": false,
                "value": "gemini-pro",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "options": [
                  "gemini-pro",
                  "gemini-pro-vision"
                ],
                "name": "model",
                "display_name": "Model",
                "advanced": false,
                "dynamic": false,
                "info": "The name of the model to use. Supported examples: gemini-pro",
                "title_case": true
              },
              "n": {
                "type": "int",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "n",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": true
              },
              "temperature": {
                "type": "float",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "0.8",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "temperature",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",
                "rangeSpec": {
                  "min": -1,
                  "max": 1,
                  "step": 0.1
                },
                "title_case": true
              },
              "top_k": {
                "type": "int",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "top_k",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": true
              },
              "top_p": {
                "type": "float",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "top_p",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "rangeSpec": {
                  "min": -1,
                  "max": 1,
                  "step": 0.1
                },
                "title_case": true
              },
              "_type": "CustomComponent"
            },
            "description": "A component that uses Google Generative AI to generate text.",
            "base_classes": [
              "BaseLanguageModel"
            ],
            "display_name": "Google Generative AI",
            "documentation": "http://docs.axiestudio.org/components/custom",
            "custom_fields": {
              "google_api_key": null,
              "model": null,
              "max_output_tokens": null,
              "temperature": null,
              "top_k": null,
              "top_p": null,
              "n": null
            },
            "output_types": [
              "BaseLanguageModel"
            ],
            "field_formatters": {},
            "beta": true
          },
          "id": "GoogleGenerativeAI-zBOow"
        },
        "selected": false,
        "width": 384,
        "height": 571,
        "positionAbsolute": {
          "x": -75.68572409847599,
          "y": -73.1263376818669
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-XRVmU",
        "type": "genericNode",
        "position": {
          "x": 628.5662036478654,
          "y": 305.04636893769685
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "llm": {
                "type": "BaseLanguageModel",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "llm",
                "display_name": "LLM",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": true
              },
              "memory": {
                "type": "BaseMemory",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": false,
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "memory",
                "display_name": "Memory",
                "advanced": false,
                "dynamic": false,
                "info": "Memory to load context from. If none is provided, a ConversationBufferMemory will be used.",
                "title_case": true
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio import CustomComponent\nfrom langchain.chains import ConversationChain\nfrom typing import Optional, Union, Callable\nfrom axiestudio.field_typing import BaseLanguageModel, BaseMemory, Chain\n\n\nclass ConversationChainComponent(CustomComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n\n    def build_config(self):\n        return {\n            \"prompt\": {\"display_name\": \"Prompt\"},\n            \"llm\": {\"display_name\": \"LLM\"},\n            \"memory\": {\n                \"display_name\": \"Memory\",\n                \"info\": \"Memory to load context from. If none is provided, a ConversationBufferMemory will be used.\",\n            },\n            \"code\": {\"show\": False},\n        }\n\n    def build(\n        self,\n        llm: BaseLanguageModel,\n        memory: Optional[BaseMemory] = None,\n    ) -> Union[Chain, Callable]:\n        if memory is None:\n            return ConversationChain(llm=llm)\n        return ConversationChain(llm=llm, memory=memory)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": false,
                "dynamic": true,
                "info": "",
                "title_case": true
              },
              "_type": "CustomComponent"
            },
            "description": "Chain to have a conversation and load context from memory.",
            "base_classes": [
              "Chain",
              "Callable"
            ],
            "display_name": "ConversationChain",
            "documentation": "",
            "custom_fields": {
              "llm": null,
              "memory": null
            },
            "output_types": [
              "Chain",
              "Callable"
            ],
            "field_formatters": {},
            "beta": true
          },
          "id": "ConversationChain-XRVmU"
        },
        "selected": true,
        "width": 384,
        "height": 397,
        "positionAbsolute": {
          "x": 628.5662036478654,
          "y": 305.04636893769685
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "GoogleGenerativeAI-zBOow",
        "sourceHandle": "{œbaseClassesœ:[œBaseLanguageModelœ],œdataTypeœ:œGoogleGenerativeAIœ,œidœ:œGoogleGenerativeAI-zBOowœ}",
        "target": "ConversationChain-XRVmU",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-XRVmUœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-XRVmU",
            "inputTypes": null,
            "type": "BaseLanguageModel"
          },
          "sourceHandle": {
            "baseClasses": [
              "BaseLanguageModel"
            ],
            "dataType": "GoogleGenerativeAI",
            "id": "GoogleGenerativeAI-zBOow"
          }
        },
        "style": {
          "stroke": "#555"
        },
        "className": "stroke-foreground  stroke-connection",
        "animated": false,
        "id": "reactflow__edge-GoogleGenerativeAI-zBOow{œbaseClassesœ:[œBaseLanguageModelœ],œdataTypeœ:œGoogleGenerativeAIœ,œidœ:œGoogleGenerativeAI-zBOowœ}-ConversationChain-XRVmU{œfieldNameœ:œllmœ,œidœ:œConversationChain-XRVmUœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}"
      }
    ],
    "viewport": {
      "x": 258.50642727289375,
      "y": 145.6688158801128,
      "zoom": 0.5743491774985184
    }
  },
  "metadata": {
    "GoogleGenerativeAI": {
      "count": 1
    },
    "ConversationChain": {
      "count": 1
    },
    "total": 2
  },
  "original": {
    "id": "c5432e34-2fb0-4d9a-9341-9f3f26f640a2",
    "name": "Untitled document (1)",
    "description": "Language Models, Unleashed.",
    "is_component": false,
    "liked_by_count": "1",
    "downloads_count": "2",
    "metadata": {
      "GoogleGenerativeAI": {
        "count": 1
      },
      "ConversationChain": {
        "count": 1
      },
      "total": 2
    },
    "last_tested_version": "0.6.19",
    "private": false,
    "data": {
      "nodes": [
        {
          "id": "GoogleGenerativeAI-zBOow",
          "type": "genericNode",
          "position": {
            "x": -75.68572409847599,
            "y": -73.1263376818669
          },
          "data": {
            "type": "GoogleGenerativeAI",
            "node": {
              "template": {
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import Optional\n\nfrom langchain_google_genai import ChatGoogleGenerativeAI  # type: ignore\nfrom axiestudio import CustomComponent\nfrom axiestudio.field_typing import BaseLanguageModel, RangeSpec, TemplateField\nfrom pydantic.v1.types import SecretStr\n\n\nclass GoogleGenerativeAIComponent(CustomComponent):\n    display_name: str = \"Google Generative AI\"\n    description: str = \"A component that uses Google Generative AI to generate text.\"\n    documentation: str = \"http://docs.axiestudio.org/components/custom\"\n\n    def build_config(self):\n        return {\n            \"google_api_key\": TemplateField(\n                display_name=\"Google API Key\",\n                info=\"The Google API Key to use for the Google Generative AI.\",\n            ),\n            \"max_output_tokens\": TemplateField(\n                display_name=\"Max Output Tokens\",\n                info=\"The maximum number of tokens to generate.\",\n            ),\n            \"temperature\": TemplateField(\n                display_name=\"Temperature\",\n                info=\"Run inference with this temperature. Must by in the closed interval [0.0, 1.0].\",\n            ),\n            \"top_k\": TemplateField(\n                display_name=\"Top K\",\n                info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n                range_spec=RangeSpec(min=0, max=2, step=0.1),\n                advanced=True,\n            ),\n            \"top_p\": TemplateField(\n                display_name=\"Top P\",\n                info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n                advanced=True,\n            ),\n            \"n\": TemplateField(\n                display_name=\"N\",\n                info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n                advanced=True,\n            ),\n            \"model\": TemplateField(\n                display_name=\"Model\",\n                info=\"The name of the model to use. Supported examples: gemini-pro\",\n                options=[\"gemini-pro\", \"gemini-pro-vision\"],\n            ),\n            \"code\": TemplateField(\n                advanced=True,\n            ),\n        }\n\n    def build(\n        self,\n        google_api_key: str,\n        model: str,\n        max_output_tokens: Optional[int] = None,\n        temperature: float = 0.1,\n        top_k: Optional[int] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = 1,\n    ) -> BaseLanguageModel:\n        return ChatGoogleGenerativeAI(\n            model=model,\n            max_output_tokens=max_output_tokens or None,  # type: ignore\n            temperature=temperature,\n            top_k=top_k or None,\n            top_p=top_p or None,  # type: ignore\n            n=n or 1,\n            google_api_key=SecretStr(google_api_key),\n        )\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "title_case": true
                },
                "google_api_key": {
                  "type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "AIzaSyCSdcyAtK3yMSvnR6UciccuRP-5DBpNaXM",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "google_api_key",
                  "display_name": "Google API Key",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The Google API Key to use for the Google Generative AI.",
                  "title_case": true
                },
                "max_output_tokens": {
                  "type": "int",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "5000",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "max_output_tokens",
                  "display_name": "Max Output Tokens",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate.",
                  "title_case": true
                },
                "model": {
                  "type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": true,
                  "show": true,
                  "multiline": false,
                  "value": "gemini-pro",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "options": [
                    "gemini-pro",
                    "gemini-pro-vision"
                  ],
                  "name": "model",
                  "display_name": "Model",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The name of the model to use. Supported examples: gemini-pro",
                  "title_case": true
                },
                "n": {
                  "type": "int",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "n",
                  "display_name": "N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                  "title_case": true
                },
                "temperature": {
                  "type": "float",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "0.8",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "temperature",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Run inference with this temperature. Must by in the closed interval [0.0, 1.0].",
                  "rangeSpec": {
                    "min": -1,
                    "max": 1,
                    "step": 0.1
                  },
                  "title_case": true
                },
                "top_k": {
                  "type": "int",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "top_k",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                  "title_case": true
                },
                "top_p": {
                  "type": "float",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "top_p",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum cumulative probability of tokens to consider when sampling.",
                  "rangeSpec": {
                    "min": -1,
                    "max": 1,
                    "step": 0.1
                  },
                  "title_case": true
                },
                "_type": "CustomComponent"
              },
              "description": "A component that uses Google Generative AI to generate text.",
              "base_classes": [
                "BaseLanguageModel"
              ],
              "display_name": "Google Generative AI",
              "documentation": "http://docs.axiestudio.org/components/custom",
              "custom_fields": {
                "google_api_key": null,
                "model": null,
                "max_output_tokens": null,
                "temperature": null,
                "top_k": null,
                "top_p": null,
                "n": null
              },
              "output_types": [
                "BaseLanguageModel"
              ],
              "field_formatters": {},
              "beta": true
            },
            "id": "GoogleGenerativeAI-zBOow"
          },
          "selected": false,
          "width": 384,
          "height": 571,
          "positionAbsolute": {
            "x": -75.68572409847599,
            "y": -73.1263376818669
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-XRVmU",
          "type": "genericNode",
          "position": {
            "x": 628.5662036478654,
            "y": 305.04636893769685
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "llm": {
                  "type": "BaseLanguageModel",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "llm",
                  "display_name": "LLM",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": true
                },
                "memory": {
                  "type": "BaseMemory",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": false,
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "memory",
                  "display_name": "Memory",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Memory to load context from. If none is provided, a ConversationBufferMemory will be used.",
                  "title_case": true
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio import CustomComponent\nfrom langchain.chains import ConversationChain\nfrom typing import Optional, Union, Callable\nfrom axiestudio.field_typing import BaseLanguageModel, BaseMemory, Chain\n\n\nclass ConversationChainComponent(CustomComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n\n    def build_config(self):\n        return {\n            \"prompt\": {\"display_name\": \"Prompt\"},\n            \"llm\": {\"display_name\": \"LLM\"},\n            \"memory\": {\n                \"display_name\": \"Memory\",\n                \"info\": \"Memory to load context from. If none is provided, a ConversationBufferMemory will be used.\",\n            },\n            \"code\": {\"show\": False},\n        }\n\n    def build(\n        self,\n        llm: BaseLanguageModel,\n        memory: Optional[BaseMemory] = None,\n    ) -> Union[Chain, Callable]:\n        if memory is None:\n            return ConversationChain(llm=llm)\n        return ConversationChain(llm=llm, memory=memory)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": false,
                  "dynamic": true,
                  "info": "",
                  "title_case": true
                },
                "_type": "CustomComponent"
              },
              "description": "Chain to have a conversation and load context from memory.",
              "base_classes": [
                "Chain",
                "Callable"
              ],
              "display_name": "ConversationChain",
              "documentation": "",
              "custom_fields": {
                "llm": null,
                "memory": null
              },
              "output_types": [
                "Chain",
                "Callable"
              ],
              "field_formatters": {},
              "beta": true
            },
            "id": "ConversationChain-XRVmU"
          },
          "selected": true,
          "width": 384,
          "height": 397,
          "positionAbsolute": {
            "x": 628.5662036478654,
            "y": 305.04636893769685
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "source": "GoogleGenerativeAI-zBOow",
          "sourceHandle": "{œbaseClassesœ:[œBaseLanguageModelœ],œdataTypeœ:œGoogleGenerativeAIœ,œidœ:œGoogleGenerativeAI-zBOowœ}",
          "target": "ConversationChain-XRVmU",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-XRVmUœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-XRVmU",
              "inputTypes": null,
              "type": "BaseLanguageModel"
            },
            "sourceHandle": {
              "baseClasses": [
                "BaseLanguageModel"
              ],
              "dataType": "GoogleGenerativeAI",
              "id": "GoogleGenerativeAI-zBOow"
            }
          },
          "style": {
            "stroke": "#555"
          },
          "className": "stroke-foreground  stroke-connection",
          "animated": false,
          "id": "reactflow__edge-GoogleGenerativeAI-zBOow{œbaseClassesœ:[œBaseLanguageModelœ],œdataTypeœ:œGoogleGenerativeAIœ,œidœ:œGoogleGenerativeAI-zBOowœ}-ConversationChain-XRVmU{œfieldNameœ:œllmœ,œidœ:œConversationChain-XRVmUœ,œinputTypesœ:null,œtypeœ:œBaseLanguageModelœ}"
        }
      ],
      "viewport": {
        "x": 258.50642727289375,
        "y": 145.6688158801128,
        "zoom": 0.5743491774985184
      }
    },
    "date_created": "2024-06-16T09:31:41.984Z",
    "date_updated": "2024-06-16T09:31:42.029Z",
    "status": "Public",
    "sort": null,
    "user_updated": "7755d01a-69ab-44d2-9c36-b44a51fd53a5",
    "user_created": {
      "username": "kucingHana",
      "first_name": "hanif",
      "last_name": "Rinardi",
      "id": "7755d01a-69ab-44d2-9c36-b44a51fd53a5"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:05.194Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 12,
    "converter_version": "1.0.0"
  }
}