{
  "id": "6af6684f-82e8-4b86-8ee6-64217203e190",
  "name": "自訂 LangChain (暴力測試) (2024/09/09)",
  "description": "自訂ChatOpenAI\n自訂Retrieval QA Chain\n自訂Supervisor and Worker (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "zrkluke",
    "first_name": "Luke",
    "last_name": "Kong",
    "id": "5b2787d1-b627-4a21-a59d-bcfd331da110",
    "full_name": "Luke Kong"
  },
  "store_url": "https://www.langflow.store/store/component/6af6684f-82e8-4b86-8ee6-64217203e190",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-09-16T03:33:13.767Z",
    "updated": "2024-09-16T03:36:14.503Z",
    "downloaded": "2025-08-19T17:50:07.046Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.17",
    "private": true,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "Custom_Test-wGV4P",
        "type": "genericNode",
        "position": {
          "x": 448.17219736586367,
          "y": -376.3172417648409
        },
        "data": {
          "type": "Custom_Test",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "# from axiestudio.field_typing import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\n# from axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\n\n\nclass CustomComponent(Component):\n    display_name = \"Custom_Test\"\n    description = \"***自訂ChatOpenAI***\"\n    icon = \"Custom_Test\"\n    name = \"Custom_Test\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def call_llm(self) -> str:\n        from langchain_openai import OpenAI, ChatOpenAI\n        from langchain_core.output_parsers import StrOutputParser\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        \n        llm = ChatOpenAI(\n            model='gpt-4o-mini',\n            api_key=api_key,\n        )\n        chain = llm | StrOutputParser()\n        \n        response = chain.invoke(self.input_value)\n        \n        return response\n\n\n    def build_message_output(self) -> Message:\n        \n        response = self.call_llm()\n        \n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input Value",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "***自訂ChatOpenAI***",
            "icon": "Custom_Test",
            "base_classes": [
              "Message"
            ],
            "display_name": "自訂ChatOpenAI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "build_message_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "api_key"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.17"
          },
          "id": "Custom_Test-wGV4P"
        },
        "selected": false,
        "width": 384,
        "height": 384,
        "positionAbsolute": {
          "x": 448.17219736586367,
          "y": -376.3172417648409
        },
        "dragging": false
      },
      {
        "id": "ChatInput-jQVv1",
        "type": "genericNode",
        "position": {
          "x": -122.91503467875475,
          "y": 71.91191399505772
        },
        "data": {
          "type": "ChatInput",
          "node": {
            "template": {
              "_type": "Component",
              "files": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "files",
                "value": "",
                "display_name": "Files",
                "advanced": true,
                "dynamic": false,
                "info": "Files to be sent with the message.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "MS有什麼新的投資報告?",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "User",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "User",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Get chat inputs from the Playground.",
            "icon": "ChatInput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "id": "ChatInput-jQVv1"
        },
        "selected": false,
        "width": 384,
        "height": 298,
        "positionAbsolute": {
          "x": -122.91503467875475,
          "y": 71.91191399505772
        },
        "dragging": false
      },
      {
        "id": "ChatOutput-bf0It",
        "type": "genericNode",
        "position": {
          "x": 1048.2420510522936,
          "y": 152.21159433179923
        },
        "data": {
          "type": "ChatOutput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "data_template",
                "value": "{text}",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "Machine",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "AI",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "ChatOutput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "id": "ChatOutput-bf0It"
        },
        "selected": false,
        "width": 384,
        "height": 298,
        "positionAbsolute": {
          "x": 1048.2420510522936,
          "y": 152.21159433179923
        },
        "dragging": false
      },
      {
        "id": "Custom_Test-ziK94",
        "type": "genericNode",
        "position": {
          "x": 471.0281179658191,
          "y": 81.1097230947799
        },
        "data": {
          "type": "Custom_Test",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "chroma_host": {
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chroma_host",
                "value": "CHROMA_HOST",
                "display_name": "Chroma Host",
                "advanced": false,
                "dynamic": false,
                "info": "The base URL of Chroma.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "chroma_port": {
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chroma_port",
                "value": "CHROMA_PORT",
                "display_name": "Chroma Port",
                "advanced": false,
                "dynamic": false,
                "info": "The port of Chroma.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "# from axiestudio.field_typing import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\n# from axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\nfrom axiestudio.field_typing import LanguageModel\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStoreRetriever\n\n\nclass CustomComponent(Component):\n    display_name = \"Custom_Test\"\n    description = \"***自訂Retrieval QA Chain***\"\n    icon = \"Custom_Test\"\n    name = \"Custom_Test\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        StrInput(\n            name=\"chroma_host\",\n            display_name=\"Chroma Host\",\n            advanced=False,\n            info=\"The base URL of Chroma.\",\n        ),\n        StrInput(\n            name=\"chroma_port\",\n            display_name=\"Chroma Port\",\n            advanced=False,\n            info=\"The port of Chroma.\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def build_model(self) -> LanguageModel:\n        from langchain_openai import ChatOpenAI\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        llm = ChatOpenAI(\n            model='gpt-4o-mini',\n            api_key=api_key,\n        )\n\n        return llm\n\n\n    def build_embedding(self) -> Embeddings:\n        from langchain_openai import OpenAIEmbeddings\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        embedding = OpenAIEmbeddings(\n            api_key=api_key,\n        )\n        \n        return embedding\n\n\n    def create_retriever(self) -> VectorStoreRetriever:\n        import chromadb\n        from langchain_chroma import Chroma\n    \n        chroma_client = chromadb.HttpClient(\n            host=self.chroma_host,\n            port=self.chroma_port,\n        )\n    \n        vector_store = Chroma(\n            embedding_function=self.build_embedding(),\n            collection_name='chroma-collection',\n            client=chroma_client,\n        )\n    \n        retriever = vector_store.as_retriever()\n    \n        return retriever\n\n\n    def create_retrieval_qa_chain(self):\n        from langchain_core.prompts import ChatPromptTemplate\n        from langchain.chains.combine_documents import create_stuff_documents_chain\n        from langchain.chains.retrieval import create_retrieval_chain\n        \n        llm = self.build_model()\n        retriever = self.create_retriever()\n        \n        system_prompt = (\n            \"Use the given context to answer the question. \"\n            \"If you don't know the answer, say you don't know. \"\n            \"Use three sentence maximum and keep the answer concise. \"\n            \"Context: {context}\"\n        )\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                (\"human\", \"{input}\"),\n            ]\n        )\n        question_answer_chain = create_stuff_documents_chain(\n            llm=llm,\n            prompt=prompt,\n        )\n        retrieval_qa_chain = create_retrieval_chain(\n            retriever=retriever,\n            combine_docs_chain=question_answer_chain,\n        )\n        \n        return retrieval_qa_chain\n\n\n    def get_response(self) -> str:\n\n        chain = self.create_retrieval_qa_chain()\n        \n        response = chain.invoke({'input': self.input_value})\n        \n        return response['answer']\n\n\n    def build_message_output(self) -> Message:\n        \n        response = self.get_response()\n        \n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input Value",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "***自訂Retrieval QA Chain***",
            "icon": "Custom_Test",
            "base_classes": [
              "Message"
            ],
            "display_name": "自訂Retrieval QA Chain",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "build_message_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "api_key",
              "chroma_host",
              "chroma_port"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.17"
          },
          "id": "Custom_Test-ziK94"
        },
        "selected": false,
        "width": 384,
        "height": 556,
        "positionAbsolute": {
          "x": 471.0281179658191,
          "y": 81.1097230947799
        },
        "dragging": false
      },
      {
        "id": "Supervisor-PQCW1",
        "type": "genericNode",
        "position": {
          "x": 462.25167287873967,
          "y": 722.5217666380199
        },
        "data": {
          "type": "Supervisor",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema.message import Message\nfrom pydantic.v1 import SecretStr\n\nfrom typing import Any, Dict, List, Union, Annotated, Sequence, TypedDict\nimport types\nimport functools\nimport operator\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\nfrom langchain_openai import (\n    ChatOpenAI, AzureChatOpenAI, OpenAIEmbeddings, AzureOpenAIEmbeddings\n)\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_core.tools import create_retriever_tool, Tool\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\nfrom langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langgraph.graph import END, StateGraph, START\n\nimport json\n\n\nclass Supervisor(Component):\n    display_name = \"Supervisor and Worker\"\n    description = \"***自訂Supervisor and Worker***\"\n    icon = \"Supervisor\"\n    \n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        SecretStrInput(\n            name=\"pinecone_api_key\",\n            display_name=\"Pinecone API Key\",\n            info=\"The Pinecone API Key.\",\n            advanced=False,\n            value=\"PINECONE_API_KEY\",\n        ),\n        StrInput(\n            name=\"pinecone_index_name\",\n            display_name=\"Pinecone Index Name\",\n            advanced=False,\n            info=\"The index name of Pinecone.\",\n        ),\n        StrInput(\n            name=\"pinecone_namespace\",\n            display_name=\"Pinecone Namespace\",\n            advanced=False,\n            info=\"The namespace of Pinecone.\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def create_chatOpenAI_0(self) -> BaseChatOpenAI:\n        \n        openai_api_key = self.openai_api_key\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        return ChatOpenAI(\n            openai_api_key=api_key,\n            model='gpt-4o-mini',\n            temperature=0.9,\n        )\n\n\n    def create_openAIEmbeddings_0(self) -> Embeddings:\n        \n        openai_api_key = self.openai_api_key\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        return OpenAIEmbeddings(\n            api_key=api_key,\n            chunk_size=100,\n        )\n\n\n    '''Create tools'''\n    def create_retrieverTool(self, embedding: Embeddings, name: str, description: str) -> Tool:\n        \n        pinecone_api_key = self.pinecone_api_key\n        index_name = self.pinecone_index_name\n        namespace = self.pinecone_namespace\n        search_type = 'mmr'\n        search_kwargs = {\n            'filter': {},\n            'k':  4,\n            'fetch_k': 5,\n            'lambda_mult': 0.9,\n        }\n        \n        pinecone_vector_store = PineconeVectorStore(\n            pinecone_api_key=pinecone_api_key,\n            embedding=embedding,\n            index_name=index_name,\n            namespace=namespace,\n            text_key='text',\n        )\n        pinecone_retriever = pinecone_vector_store.as_retriever(\n            search_type=search_type,\n            search_kwargs=search_kwargs,\n        )\n        retrieverTool = create_retriever_tool(\n            retriever=pinecone_retriever,\n            name=name,\n            description=description,\n        )\n    \n        # TODO: returnSourceDocuments\n        return retrieverTool\n\n\n    '''Create Agent Worker'''\n    def create_agent(self, llm: BaseChatOpenAI, tools: List[Tool], system_prompt: str) -> AgentExecutor:\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    system_prompt,\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            ]\n        )\n        agent = create_openai_tools_agent(llm, tools, prompt)\n        executor = AgentExecutor(agent=agent, tools=tools)\n        \n        return executor\n    \n    \n    def agent_node(self, state, agent: AgentExecutor, name):\n        result = agent.invoke(state)\n        \n        return {\"messages\": [AIMessage(content=result[\"output\"], name=name)]}\n    \n    \n    def create_worker(self, llm: BaseChatOpenAI, tools: List[Tool], prompt: str, prompt_values: Dict, name: str):\n        worker_prompt = prompt.format(**prompt_values)\n        worker_agent = self.create_agent(llm, tools, worker_prompt)\n        worker_node = functools.partial(self.agent_node, agent=worker_agent, name=name)\n    \n        return worker_node\n\n\n    '''Create Agent Supervisor'''\n    def create_supervisor(self, llm: BaseChatOpenAI, team_members: List[str], system_prompt: str):\n        options = [\"FINISH\"] + team_members\n        function_def = {\n            \"name\": \"route\",\n            \"description\": \"Select the next role.\",\n            \"parameters\": {\n                \"title\": \"routeSchema\",\n                \"type\": \"object\",\n                \"properties\": {\n                    \"next\": {\n                        \"title\": \"Next\",\n                        \"anyOf\": [\n                            {\"enum\": options},\n                        ],\n                    }\n                },\n                \"required\": [\"next\"],\n            },\n        }\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                (\n                    \"human\",\n                    \"Given the conversation above, who should act next?\"\n                    \" Or should we FINISH? Select one of: {options}\",\n                ),\n            ]\n        ).partial(options=str(options), team_members=\", \".join(team_members))\n    \n        supervisor_chain = (\n            prompt\n            | llm.bind_functions(functions=[function_def], function_call=\"route\")\n            | JsonOutputFunctionsParser()\n        )\n    \n        # TODO: Summarization\n        return supervisor_chain\n\n\n    '''Construct Graph'''\n    def construct_graph(self, team_members: List[tuple[str, Any]], supervisor: tuple[str, Any]):\n        class AgentState(TypedDict):\n            messages: Annotated[Sequence[BaseMessage], operator.add]\n            next: str   # indicates where to route to next\n    \n        workflow = StateGraph(AgentState)\n        for member in team_members:\n            workflow.add_node(member[0], member[1])\n        workflow.add_node(supervisor[0], supervisor[1])\n    \n        # Workers always report back to the supervisor\n        for member in team_members:\n            workflow.add_edge(member[0], supervisor[0])\n    \n        conditional_map = {member[0]: member[0] for member in team_members}\n        conditional_map[\"FINISH\"] = END\n        workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n        workflow.add_edge(START, supervisor[0])\n    \n        graph = workflow.compile()\n    \n        return graph\n    \n\n    def main(self):\n        chatOpenAI_0 = self.create_chatOpenAI_0()\n        openAIEmbeddings_0 = self.create_openAIEmbeddings_0()\n    \n        '''Create tools'''\n        retrieverTool_0 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='jpmrates',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='JPM',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n        retrieverTool_1 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='gsrates',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='GS',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n        retrieverTool_2 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='msboth',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='MS',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n    \n        '''Create Agent Worker'''\n        worker_0 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_0],\n            prompt='''作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - JPM to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'JPM'},\n            name='JPM',\n        )\n        worker_1 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_1],\n            prompt='''\n            作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - GS to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'GS'},\n            name='GS',\n        )\n        worker_2 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_2],\n            prompt='''作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - MS to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'MS'},\n            name='MS',\n        )\n        team_members = [\n          # (name, instance)\n            ('JPM', worker_0),\n            ('GS', worker_1),\n            ('MS', worker_2),\n        ]\n    \n        '''Create Agent Supervisor'''\n        # JSON缺少team_members參數\n        names = [member[0] for member in team_members]\n        supervisor_0 = self.create_supervisor(\n            llm=chatOpenAI_0,\n            team_members=names,\n            system_prompt='''您是supervisor，負責管理以下工作人員之間的對話：{team_members}。\\n\n            鑑於以下使用者請求，請與工作人員一起回應以進行下一步操作。\\n\n            每個工作人員將執行一項任務並回應其結果和狀態。\\n\n            您負責總結 {team_members} 的答案，以繁體中文回答，並確保您每次都會這樣做。\\n\n            完成後，響應“FINISH”。\\n有策略地選擇以盡量減少所採取的步驟數。'''\n        )\n        supervisor = ('supervisor', supervisor_0)\n    \n        '''Construct Graph'''\n        graph = self.construct_graph(team_members=team_members, supervisor=supervisor)\n    \n        return graph\n\n\n    def get_response(self) -> str:\n\n        graph = self.main()\n        \n        input_dict = {\"messages\": [HumanMessage(content=self.input_value)]}\n        config_dict = {\"recursion_limit\": 100}\n        \n        result_str = \"\"\n        \n        for s in graph.stream(\n            input=input_dict,\n            config=config_dict,\n        ):\n            if \"__end__\" not in s:\n                print(s)\n                print(\"----\")\n            if 'supervisor' in s:\n                result_str += f\"**(Supervisor select {s['supervisor']['next']})**\\n\\n\"\n            else:\n                messages = list(s.values())[0]['messages']\n                for message in messages:\n                    if isinstance(message, AIMessage):\n                        result_str += f\"**{message.name}**: {message.content}\\n\\n\"\n                \n        return result_str\n\n\n    # async def get_response(self) -> str:\n\n    #     graph = self.main()\n        \n    #     input_dict = {\"messages\": [HumanMessage(content=self.input_value)]}\n    #     config_dict = {\"recursion_limit\": 100}\n        \n    #     response = await graph.ainvoke(\n    #         input=input_dict,\n    #         config=config_dict,\n    #     )\n    #     result_str = \"\"\n    #     for message in response['messages']:\n    #         if isinstance(message, AIMessage):\n    #             result_str += f\"{message.name} Agent: {message.content}\\n\\n\"\n                \n    #     return result_str\n\n    \n    # 卡住無法執行\n    # async def astream_events(self, graph, input_dict, config_dict) -> str:\n    #     async for event in graph.astream_events(\n    #         input=input_dict,\n    #         config=config_dict,\n    #         version=\"v1\"\n    #     ):\n    #         if event.get(\"event\") != \"on_chat_model_stream\":\n    #             continue\n\n    #         yield event.get(\"data\").get(\"chunk\")\n\n\n    # async def build_message_output(self) -> Message:\n        \n    #     response = await self.get_response()\n    #     message = Message(\n    #         text=response,\n    #         sender=\"AI\",\n    #         sender_name=\"AI\",\n    #     )\n    \n    #     return message\n        \n\n    def build_message_output(self) -> Message:\n        \n        response = self.get_response()\n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n    \n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input Value",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "openai_api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "openai_api_key",
                "value": "",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "pinecone_api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "pinecone_api_key",
                "value": "",
                "display_name": "Pinecone API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Pinecone API Key.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "pinecone_index_name": {
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "pinecone_index_name",
                "value": "PINECONE_INDEX_NAME",
                "display_name": "Pinecone Index Name",
                "advanced": false,
                "dynamic": false,
                "info": "The index name of Pinecone.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "pinecone_namespace": {
                "trace_as_metadata": true,
                "load_from_db": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "pinecone_namespace",
                "value": "PINECONE_NAMESPACE",
                "display_name": "Pinecone Namespace",
                "advanced": false,
                "dynamic": false,
                "info": "The namespace of Pinecone.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              }
            },
            "description": "***自訂Supervisor and Worker***",
            "icon": "Supervisor",
            "base_classes": [
              "Message"
            ],
            "display_name": "(9/12)自訂Supervisor and Worker",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "build_message_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "openai_api_key",
              "pinecone_api_key",
              "pinecone_index_name",
              "pinecone_namespace"
            ],
            "beta": false,
            "edited": true
          },
          "id": "Supervisor-PQCW1"
        },
        "selected": false,
        "width": 384,
        "height": 642,
        "positionAbsolute": {
          "x": 462.25167287873967,
          "y": 722.5217666380199
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "ChatInput-jQVv1",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jQVv1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Supervisor-PQCW1",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œSupervisor-PQCW1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "Supervisor-PQCW1",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-jQVv1",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-jQVv1{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jQVv1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Supervisor-PQCW1{œfieldNameœ:œinput_valueœ,œidœ:œSupervisor-PQCW1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Supervisor-PQCW1",
        "sourceHandle": "{œdataTypeœ:œSupervisorœ,œidœ:œSupervisor-PQCW1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-bf0It",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-bf0Itœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-bf0It",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Supervisor",
            "id": "Supervisor-PQCW1",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Supervisor-PQCW1{œdataTypeœ:œSupervisorœ,œidœ:œSupervisor-PQCW1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-bf0It{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-bf0Itœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      }
    ],
    "viewport": {
      "x": 329.3390800313199,
      "y": 230.5994439388263,
      "zoom": 0.4522348719827254
    }
  },
  "metadata": {
    "Custom_Test": {
      "count": 2
    },
    "ChatInput": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "Supervisor": {
      "count": 1
    },
    "total": 5
  },
  "original": {
    "id": "6af6684f-82e8-4b86-8ee6-64217203e190",
    "name": "自訂 LangChain (暴力測試) (2024/09/09)",
    "description": "自訂ChatOpenAI\n自訂Retrieval QA Chain\n自訂Supervisor and Worker",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "0",
    "metadata": {
      "Custom_Test": {
        "count": 2
      },
      "ChatInput": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "Supervisor": {
        "count": 1
      },
      "total": 5
    },
    "last_tested_version": "1.0.17",
    "private": true,
    "data": {
      "nodes": [
        {
          "id": "Custom_Test-wGV4P",
          "type": "genericNode",
          "position": {
            "x": 448.17219736586367,
            "y": -376.3172417648409
          },
          "data": {
            "type": "Custom_Test",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "# from axiestudio.field_typing import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\n# from axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\n\n\nclass CustomComponent(Component):\n    display_name = \"Custom_Test\"\n    description = \"***自訂ChatOpenAI***\"\n    icon = \"Custom_Test\"\n    name = \"Custom_Test\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def call_llm(self) -> str:\n        from langchain_openai import OpenAI, ChatOpenAI\n        from langchain_core.output_parsers import StrOutputParser\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        \n        llm = ChatOpenAI(\n            model='gpt-4o-mini',\n            api_key=api_key,\n        )\n        chain = llm | StrOutputParser()\n        \n        response = chain.invoke(self.input_value)\n        \n        return response\n\n\n    def build_message_output(self) -> Message:\n        \n        response = self.call_llm()\n        \n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input Value",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "***自訂ChatOpenAI***",
              "icon": "Custom_Test",
              "base_classes": [
                "Message"
              ],
              "display_name": "自訂ChatOpenAI",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "build_message_output",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "api_key"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.17"
            },
            "id": "Custom_Test-wGV4P"
          },
          "selected": false,
          "width": 384,
          "height": 384,
          "positionAbsolute": {
            "x": 448.17219736586367,
            "y": -376.3172417648409
          },
          "dragging": false
        },
        {
          "id": "ChatInput-jQVv1",
          "type": "genericNode",
          "position": {
            "x": -122.91503467875475,
            "y": 71.91191399505772
          },
          "data": {
            "type": "ChatInput",
            "node": {
              "template": {
                "_type": "Component",
                "files": {
                  "trace_as_metadata": true,
                  "file_path": "",
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "files",
                  "value": "",
                  "display_name": "Files",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Files to be sent with the message.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "MS有什麼新的投資報告?",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender",
                  "value": "User",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender_name",
                  "value": "User",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "session_id",
                  "value": "",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "should_store_message": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "should_store_message",
                  "value": true,
                  "display_name": "Store Messages",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Get chat inputs from the Playground.",
              "icon": "ChatInput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Input",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "id": "ChatInput-jQVv1"
          },
          "selected": false,
          "width": 384,
          "height": 298,
          "positionAbsolute": {
            "x": -122.91503467875475,
            "y": 71.91191399505772
          },
          "dragging": false
        },
        {
          "id": "ChatOutput-bf0It",
          "type": "genericNode",
          "position": {
            "x": 1048.2420510522936,
            "y": 152.21159433179923
          },
          "data": {
            "type": "ChatOutput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "data_template": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "data_template",
                  "value": "{text}",
                  "display_name": "Data Template",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender",
                  "value": "Machine",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender_name",
                  "value": "AI",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "session_id",
                  "value": "",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "should_store_message": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "should_store_message",
                  "value": true,
                  "display_name": "Store Messages",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Display a chat message in the Playground.",
              "icon": "ChatOutput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Output",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "id": "ChatOutput-bf0It"
          },
          "selected": false,
          "width": 384,
          "height": 298,
          "positionAbsolute": {
            "x": 1048.2420510522936,
            "y": 152.21159433179923
          },
          "dragging": false
        },
        {
          "id": "Custom_Test-ziK94",
          "type": "genericNode",
          "position": {
            "x": 471.0281179658191,
            "y": 81.1097230947799
          },
          "data": {
            "type": "Custom_Test",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "chroma_host": {
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "chroma_host",
                  "value": "CHROMA_HOST",
                  "display_name": "Chroma Host",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The base URL of Chroma.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "chroma_port": {
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "chroma_port",
                  "value": "CHROMA_PORT",
                  "display_name": "Chroma Port",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The port of Chroma.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "# from axiestudio.field_typing import Data\nfrom axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\n# from axiestudio.schema import Data\nfrom axiestudio.schema.message import Message\nfrom axiestudio.field_typing import LanguageModel\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_core.vectorstores import VectorStoreRetriever\n\n\nclass CustomComponent(Component):\n    display_name = \"Custom_Test\"\n    description = \"***自訂Retrieval QA Chain***\"\n    icon = \"Custom_Test\"\n    name = \"Custom_Test\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        StrInput(\n            name=\"chroma_host\",\n            display_name=\"Chroma Host\",\n            advanced=False,\n            info=\"The base URL of Chroma.\",\n        ),\n        StrInput(\n            name=\"chroma_port\",\n            display_name=\"Chroma Port\",\n            advanced=False,\n            info=\"The port of Chroma.\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def build_model(self) -> LanguageModel:\n        from langchain_openai import ChatOpenAI\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        llm = ChatOpenAI(\n            model='gpt-4o-mini',\n            api_key=api_key,\n        )\n\n        return llm\n\n\n    def build_embedding(self) -> Embeddings:\n        from langchain_openai import OpenAIEmbeddings\n        from pydantic.v1 import SecretStr\n        \n        openai_api_key = self.api_key\n        \n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        embedding = OpenAIEmbeddings(\n            api_key=api_key,\n        )\n        \n        return embedding\n\n\n    def create_retriever(self) -> VectorStoreRetriever:\n        import chromadb\n        from langchain_chroma import Chroma\n    \n        chroma_client = chromadb.HttpClient(\n            host=self.chroma_host,\n            port=self.chroma_port,\n        )\n    \n        vector_store = Chroma(\n            embedding_function=self.build_embedding(),\n            collection_name='chroma-collection',\n            client=chroma_client,\n        )\n    \n        retriever = vector_store.as_retriever()\n    \n        return retriever\n\n\n    def create_retrieval_qa_chain(self):\n        from langchain_core.prompts import ChatPromptTemplate\n        from langchain.chains.combine_documents import create_stuff_documents_chain\n        from langchain.chains.retrieval import create_retrieval_chain\n        \n        llm = self.build_model()\n        retriever = self.create_retriever()\n        \n        system_prompt = (\n            \"Use the given context to answer the question. \"\n            \"If you don't know the answer, say you don't know. \"\n            \"Use three sentence maximum and keep the answer concise. \"\n            \"Context: {context}\"\n        )\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                (\"human\", \"{input}\"),\n            ]\n        )\n        question_answer_chain = create_stuff_documents_chain(\n            llm=llm,\n            prompt=prompt,\n        )\n        retrieval_qa_chain = create_retrieval_chain(\n            retriever=retriever,\n            combine_docs_chain=question_answer_chain,\n        )\n        \n        return retrieval_qa_chain\n\n\n    def get_response(self) -> str:\n\n        chain = self.create_retrieval_qa_chain()\n        \n        response = chain.invoke({'input': self.input_value})\n        \n        return response['answer']\n\n\n    def build_message_output(self) -> Message:\n        \n        response = self.get_response()\n        \n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input Value",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "***自訂Retrieval QA Chain***",
              "icon": "Custom_Test",
              "base_classes": [
                "Message"
              ],
              "display_name": "自訂Retrieval QA Chain",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "build_message_output",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "api_key",
                "chroma_host",
                "chroma_port"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.17"
            },
            "id": "Custom_Test-ziK94"
          },
          "selected": false,
          "width": 384,
          "height": 556,
          "positionAbsolute": {
            "x": 471.0281179658191,
            "y": 81.1097230947799
          },
          "dragging": false
        },
        {
          "id": "Supervisor-PQCW1",
          "type": "genericNode",
          "position": {
            "x": 462.25167287873967,
            "y": 722.5217666380199
          },
          "data": {
            "type": "Supervisor",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\nfrom axiestudio.io import MessageTextInput, Output\nfrom axiestudio.schema.message import Message\nfrom pydantic.v1 import SecretStr\n\nfrom typing import Any, Dict, List, Union, Annotated, Sequence, TypedDict\nimport types\nimport functools\nimport operator\n\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_openai.chat_models.base import BaseChatOpenAI\nfrom langchain_openai import (\n    ChatOpenAI, AzureChatOpenAI, OpenAIEmbeddings, AzureOpenAIEmbeddings\n)\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_core.tools import create_retriever_tool, Tool\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage\nfrom langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langgraph.graph import END, StateGraph, START\n\nimport json\n\n\nclass Supervisor(Component):\n    display_name = \"Supervisor and Worker\"\n    description = \"***自訂Supervisor and Worker***\"\n    icon = \"Supervisor\"\n    \n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Input Value\",\n            value=\"Hello, World!\"\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        SecretStrInput(\n            name=\"pinecone_api_key\",\n            display_name=\"Pinecone API Key\",\n            info=\"The Pinecone API Key.\",\n            advanced=False,\n            value=\"PINECONE_API_KEY\",\n        ),\n        StrInput(\n            name=\"pinecone_index_name\",\n            display_name=\"Pinecone Index Name\",\n            advanced=False,\n            info=\"The index name of Pinecone.\",\n        ),\n        StrInput(\n            name=\"pinecone_namespace\",\n            display_name=\"Pinecone Namespace\",\n            advanced=False,\n            info=\"The namespace of Pinecone.\",\n        ),\n    ]\n    \n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"build_message_output\"),\n    ]\n\n\n    def create_chatOpenAI_0(self) -> BaseChatOpenAI:\n        \n        openai_api_key = self.openai_api_key\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        return ChatOpenAI(\n            openai_api_key=api_key,\n            model='gpt-4o-mini',\n            temperature=0.9,\n        )\n\n\n    def create_openAIEmbeddings_0(self) -> Embeddings:\n        \n        openai_api_key = self.openai_api_key\n        api_key = SecretStr(openai_api_key) if openai_api_key else None\n        \n        return OpenAIEmbeddings(\n            api_key=api_key,\n            chunk_size=100,\n        )\n\n\n    '''Create tools'''\n    def create_retrieverTool(self, embedding: Embeddings, name: str, description: str) -> Tool:\n        \n        pinecone_api_key = self.pinecone_api_key\n        index_name = self.pinecone_index_name\n        namespace = self.pinecone_namespace\n        search_type = 'mmr'\n        search_kwargs = {\n            'filter': {},\n            'k':  4,\n            'fetch_k': 5,\n            'lambda_mult': 0.9,\n        }\n        \n        pinecone_vector_store = PineconeVectorStore(\n            pinecone_api_key=pinecone_api_key,\n            embedding=embedding,\n            index_name=index_name,\n            namespace=namespace,\n            text_key='text',\n        )\n        pinecone_retriever = pinecone_vector_store.as_retriever(\n            search_type=search_type,\n            search_kwargs=search_kwargs,\n        )\n        retrieverTool = create_retriever_tool(\n            retriever=pinecone_retriever,\n            name=name,\n            description=description,\n        )\n    \n        # TODO: returnSourceDocuments\n        return retrieverTool\n\n\n    '''Create Agent Worker'''\n    def create_agent(self, llm: BaseChatOpenAI, tools: List[Tool], system_prompt: str) -> AgentExecutor:\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    system_prompt,\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            ]\n        )\n        agent = create_openai_tools_agent(llm, tools, prompt)\n        executor = AgentExecutor(agent=agent, tools=tools)\n        \n        return executor\n    \n    \n    def agent_node(self, state, agent: AgentExecutor, name):\n        result = agent.invoke(state)\n        \n        return {\"messages\": [AIMessage(content=result[\"output\"], name=name)]}\n    \n    \n    def create_worker(self, llm: BaseChatOpenAI, tools: List[Tool], prompt: str, prompt_values: Dict, name: str):\n        worker_prompt = prompt.format(**prompt_values)\n        worker_agent = self.create_agent(llm, tools, worker_prompt)\n        worker_node = functools.partial(self.agent_node, agent=worker_agent, name=name)\n    \n        return worker_node\n\n\n    '''Create Agent Supervisor'''\n    def create_supervisor(self, llm: BaseChatOpenAI, team_members: List[str], system_prompt: str):\n        options = [\"FINISH\"] + team_members\n        function_def = {\n            \"name\": \"route\",\n            \"description\": \"Select the next role.\",\n            \"parameters\": {\n                \"title\": \"routeSchema\",\n                \"type\": \"object\",\n                \"properties\": {\n                    \"next\": {\n                        \"title\": \"Next\",\n                        \"anyOf\": [\n                            {\"enum\": options},\n                        ],\n                    }\n                },\n                \"required\": [\"next\"],\n            },\n        }\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", system_prompt),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                (\n                    \"human\",\n                    \"Given the conversation above, who should act next?\"\n                    \" Or should we FINISH? Select one of: {options}\",\n                ),\n            ]\n        ).partial(options=str(options), team_members=\", \".join(team_members))\n    \n        supervisor_chain = (\n            prompt\n            | llm.bind_functions(functions=[function_def], function_call=\"route\")\n            | JsonOutputFunctionsParser()\n        )\n    \n        # TODO: Summarization\n        return supervisor_chain\n\n\n    '''Construct Graph'''\n    def construct_graph(self, team_members: List[tuple[str, Any]], supervisor: tuple[str, Any]):\n        class AgentState(TypedDict):\n            messages: Annotated[Sequence[BaseMessage], operator.add]\n            next: str   # indicates where to route to next\n    \n        workflow = StateGraph(AgentState)\n        for member in team_members:\n            workflow.add_node(member[0], member[1])\n        workflow.add_node(supervisor[0], supervisor[1])\n    \n        # Workers always report back to the supervisor\n        for member in team_members:\n            workflow.add_edge(member[0], supervisor[0])\n    \n        conditional_map = {member[0]: member[0] for member in team_members}\n        conditional_map[\"FINISH\"] = END\n        workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n        workflow.add_edge(START, supervisor[0])\n    \n        graph = workflow.compile()\n    \n        return graph\n    \n\n    def main(self):\n        chatOpenAI_0 = self.create_chatOpenAI_0()\n        openAIEmbeddings_0 = self.create_openAIEmbeddings_0()\n    \n        '''Create tools'''\n        retrieverTool_0 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='jpmrates',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='JPM',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n        retrieverTool_1 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='gsrates',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='GS',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n        retrieverTool_2 = self.create_retrieverTool(\n            embedding=openAIEmbeddings_0,\n            # index_name='msboth',\n            # namespace='my-first-namespace',\n            # search_type='mmr',\n            # search_kwargs={\n            #     'filter': {},\n            #     'k':  4,\n            #     'fetch_k': 5,\n            #     'lambda_mult': 0.9,\n            # },\n            name='MS',\n            description='Search and return documents about any issue or bugfix. Always give priority to this tool',\n        )\n    \n        '''Create Agent Worker'''\n        worker_0 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_0],\n            prompt='''作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - JPM to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'JPM'},\n            name='JPM',\n        )\n        worker_1 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_1],\n            prompt='''\n            作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - GS to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'GS'},\n            name='GS',\n        )\n        worker_2 = self.create_worker(\n            llm=chatOpenAI_0,\n            tools=[retrieverTool_2],\n            prompt='''作為 {bank} 的代表，您的職責是提供{invest}券商報告摘要，並以繁體中文回答。\n            您的目標是提供最高品質的幫助，確保您的答案全面且基於事實，不帶任何假設。\\n\\n\n            Always use the tool provided - MS to look for answers. \n            Check if you need to pass the result to Quality Assurance Specialist for review.''',\n            prompt_values={'bank': 'cathay', 'invest': 'MS'},\n            name='MS',\n        )\n        team_members = [\n          # (name, instance)\n            ('JPM', worker_0),\n            ('GS', worker_1),\n            ('MS', worker_2),\n        ]\n    \n        '''Create Agent Supervisor'''\n        # JSON缺少team_members參數\n        names = [member[0] for member in team_members]\n        supervisor_0 = self.create_supervisor(\n            llm=chatOpenAI_0,\n            team_members=names,\n            system_prompt='''您是supervisor，負責管理以下工作人員之間的對話：{team_members}。\\n\n            鑑於以下使用者請求，請與工作人員一起回應以進行下一步操作。\\n\n            每個工作人員將執行一項任務並回應其結果和狀態。\\n\n            您負責總結 {team_members} 的答案，以繁體中文回答，並確保您每次都會這樣做。\\n\n            完成後，響應“FINISH”。\\n有策略地選擇以盡量減少所採取的步驟數。'''\n        )\n        supervisor = ('supervisor', supervisor_0)\n    \n        '''Construct Graph'''\n        graph = self.construct_graph(team_members=team_members, supervisor=supervisor)\n    \n        return graph\n\n\n    def get_response(self) -> str:\n\n        graph = self.main()\n        \n        input_dict = {\"messages\": [HumanMessage(content=self.input_value)]}\n        config_dict = {\"recursion_limit\": 100}\n        \n        result_str = \"\"\n        \n        for s in graph.stream(\n            input=input_dict,\n            config=config_dict,\n        ):\n            if \"__end__\" not in s:\n                print(s)\n                print(\"----\")\n            if 'supervisor' in s:\n                result_str += f\"**(Supervisor select {s['supervisor']['next']})**\\n\\n\"\n            else:\n                messages = list(s.values())[0]['messages']\n                for message in messages:\n                    if isinstance(message, AIMessage):\n                        result_str += f\"**{message.name}**: {message.content}\\n\\n\"\n                \n        return result_str\n\n\n    # async def get_response(self) -> str:\n\n    #     graph = self.main()\n        \n    #     input_dict = {\"messages\": [HumanMessage(content=self.input_value)]}\n    #     config_dict = {\"recursion_limit\": 100}\n        \n    #     response = await graph.ainvoke(\n    #         input=input_dict,\n    #         config=config_dict,\n    #     )\n    #     result_str = \"\"\n    #     for message in response['messages']:\n    #         if isinstance(message, AIMessage):\n    #             result_str += f\"{message.name} Agent: {message.content}\\n\\n\"\n                \n    #     return result_str\n\n    \n    # 卡住無法執行\n    # async def astream_events(self, graph, input_dict, config_dict) -> str:\n    #     async for event in graph.astream_events(\n    #         input=input_dict,\n    #         config=config_dict,\n    #         version=\"v1\"\n    #     ):\n    #         if event.get(\"event\") != \"on_chat_model_stream\":\n    #             continue\n\n    #         yield event.get(\"data\").get(\"chunk\")\n\n\n    # async def build_message_output(self) -> Message:\n        \n    #     response = await self.get_response()\n    #     message = Message(\n    #         text=response,\n    #         sender=\"AI\",\n    #         sender_name=\"AI\",\n    #     )\n    \n    #     return message\n        \n\n    def build_message_output(self) -> Message:\n        \n        response = self.get_response()\n        message = Message(\n            text=response,\n            sender=\"AI\",\n            sender_name=\"AI\",\n        )\n    \n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input Value",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "openai_api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "openai_api_key",
                  "value": "",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "pinecone_api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "pinecone_api_key",
                  "value": "",
                  "display_name": "Pinecone API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The Pinecone API Key.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "pinecone_index_name": {
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "pinecone_index_name",
                  "value": "PINECONE_INDEX_NAME",
                  "display_name": "Pinecone Index Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The index name of Pinecone.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "pinecone_namespace": {
                  "trace_as_metadata": true,
                  "load_from_db": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "pinecone_namespace",
                  "value": "PINECONE_NAMESPACE",
                  "display_name": "Pinecone Namespace",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The namespace of Pinecone.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                }
              },
              "description": "***自訂Supervisor and Worker***",
              "icon": "Supervisor",
              "base_classes": [
                "Message"
              ],
              "display_name": "(9/12)自訂Supervisor and Worker",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "build_message_output",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "openai_api_key",
                "pinecone_api_key",
                "pinecone_index_name",
                "pinecone_namespace"
              ],
              "beta": false,
              "edited": true
            },
            "id": "Supervisor-PQCW1"
          },
          "selected": false,
          "width": 384,
          "height": 642,
          "positionAbsolute": {
            "x": 462.25167287873967,
            "y": 722.5217666380199
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "source": "ChatInput-jQVv1",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jQVv1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Supervisor-PQCW1",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œSupervisor-PQCW1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "Supervisor-PQCW1",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-jQVv1",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-jQVv1{œdataTypeœ:œChatInputœ,œidœ:œChatInput-jQVv1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Supervisor-PQCW1{œfieldNameœ:œinput_valueœ,œidœ:œSupervisor-PQCW1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Supervisor-PQCW1",
          "sourceHandle": "{œdataTypeœ:œSupervisorœ,œidœ:œSupervisor-PQCW1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatOutput-bf0It",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-bf0Itœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-bf0It",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Supervisor",
              "id": "Supervisor-PQCW1",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Supervisor-PQCW1{œdataTypeœ:œSupervisorœ,œidœ:œSupervisor-PQCW1œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-bf0It{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-bf0Itœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        }
      ],
      "viewport": {
        "x": 329.3390800313199,
        "y": 230.5994439388263,
        "zoom": 0.4522348719827254
      }
    },
    "date_created": "2024-09-16T03:33:13.767Z",
    "date_updated": "2024-09-16T03:36:14.503Z",
    "status": "Public",
    "sort": null,
    "user_updated": "5b2787d1-b627-4a21-a59d-bcfd331da110",
    "user_created": {
      "username": "zrkluke",
      "first_name": "Luke",
      "last_name": "Kong",
      "id": "5b2787d1-b627-4a21-a59d-bcfd331da110"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:08:59.571Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 54,
    "converter_version": "1.0.0"
  }
}