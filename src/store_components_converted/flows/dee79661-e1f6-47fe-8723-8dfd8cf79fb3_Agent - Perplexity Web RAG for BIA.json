{
  "id": "dee79661-e1f6-47fe-8723-8dfd8cf79fb3",
  "name": "Agent - Perplexity Web RAG for BIA",
  "description": "This Agent runs tasks in a predefined sequence to augment the local RAG. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "jingconsult",
    "first_name": "Jing",
    "last_name": "Consulting",
    "id": "6decf44a-a4d8-438a-92d3-df07d49ad213",
    "full_name": "Jing Consulting"
  },
  "store_url": "https://www.langflow.store/store/component/dee79661-e1f6-47fe-8723-8dfd8cf79fb3",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-10-22T04:51:04.601Z",
    "updated": "2024-10-22T04:51:04.743Z",
    "downloaded": "2025-08-19T17:50:07.545Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.17",
    "private": true,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "data": {
          "description": "Represents a group of agents, defining how they should collaborate and the tasks they should perform.",
          "display_name": "Sequential Crew",
          "id": "SequentialCrewComponent-WXzHJ",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Represents a group of agents with tasks that are executed sequentially.",
            "display_name": "Sequential Crew",
            "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
            "edited": false,
            "field_order": [
              "verbose",
              "memory",
              "use_cache",
              "max_rpm",
              "share_crew",
              "function_calling_llm",
              "tasks"
            ],
            "frozen": false,
            "icon": "CrewAI",
            "lf_version": "1.0.17",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Output",
                "method": "build_output",
                "name": "output",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from crewai import Agent, Crew, Process, Task  # type: ignore\n\nfrom axiestudio.base.agents.crewai.crew import BaseCrewComponent\nfrom axiestudio.io import HandleInput\nfrom axiestudio.schema.message import Message\n\n\nclass SequentialCrewComponent(BaseCrewComponent):\n    display_name: str = \"Sequential Crew\"\n    description: str = \"Represents a group of agents with tasks that are executed sequentially.\"\n    documentation: str = \"https://docs.crewai.com/how-to/Sequential/\"\n    icon = \"CrewAI\"\n\n    inputs = BaseCrewComponent._base_inputs + [\n        HandleInput(name=\"tasks\", display_name=\"Tasks\", input_types=[\"SequentialTask\"], is_list=True),\n    ]\n\n    def get_tasks_and_agents(self) -> tuple[list[Task], list[Agent]]:\n        return self.tasks, [task.agent for task in self.tasks]\n\n    def build_crew(self) -> Message:\n        tasks, agents = self.get_tasks_and_agents()\n        crew = Crew(\n            agents=agents,\n            tasks=tasks,\n            process=Process.sequential,\n            verbose=self.verbose,\n            memory=self.memory,\n            cache=self.use_cache,\n            max_rpm=self.max_rpm,\n            share_crew=self.share_crew,\n            function_calling_llm=self.function_calling_llm,\n            step_callback=self.get_step_callback(),\n            task_callback=self.get_task_callback(),\n        )\n        return crew\n"
              },
              "function_calling_llm": {
                "advanced": true,
                "display_name": "Function Calling LLM",
                "dynamic": false,
                "info": "Turns the ReAct CrewAI agent into a function-calling agent",
                "input_types": [
                  "LanguageModel"
                ],
                "list": false,
                "name": "function_calling_llm",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "max_rpm": {
                "advanced": true,
                "display_name": "Max RPM",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "max_rpm",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 100
              },
              "memory": {
                "advanced": true,
                "display_name": "Memory",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "memory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "share_crew": {
                "advanced": true,
                "display_name": "Share Crew",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "share_crew",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "tasks": {
                "advanced": false,
                "display_name": "Tasks",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "SequentialTask"
                ],
                "list": true,
                "name": "tasks",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "use_cache": {
                "advanced": true,
                "display_name": "Cache",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "use_cache",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "verbose": {
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 0
              }
            }
          },
          "type": "SequentialCrewComponent"
        },
        "dragging": false,
        "height": 283,
        "id": "SequentialCrewComponent-WXzHJ",
        "position": {
          "x": 1031.9693200138174,
          "y": 230.02911653515076
        },
        "positionAbsolute": {
          "x": 1031.9693200138174,
          "y": 230.02911653515076
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output",
          "id": "ChatOutput-41Gjf",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "frozen": false,
            "icon": "ChatOutput",
            "lf_version": "1.0.17",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Message",
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
              },
              "data_template": {
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "advanced": false,
                "display_name": "Text",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "advanced": true,
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "ChatOutput"
        },
        "dragging": false,
        "height": 297,
        "id": "ChatOutput-41Gjf",
        "position": {
          "x": 1646.728086508338,
          "y": 359.11640497700887
        },
        "selected": false,
        "type": "genericNode",
        "width": 384,
        "positionAbsolute": {
          "x": 1646.728086508338,
          "y": 359.11640497700887
        }
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-EfX9F",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Research Question: {research_question}\n\nUnderstand the {research_question} being asked from the user's query and find the answer to the {research_question}."
              },
              "research_question": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "research_question",
                "display_name": "research_question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "research_question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 411,
        "id": "Prompt-EfX9F",
        "position": {
          "x": -1465.2544640946471,
          "y": -2.9577069816386086
        },
        "positionAbsolute": {
          "x": -1465.2544640946471,
          "y": -2.9577069816386086
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-9Vgna",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Research Question: {research_question}\n\nUnderstand the {research_question} being asked from the user's query, review the content you are given by other agent."
              },
              "research_question": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "research_question",
                "display_name": "research_question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "research_question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 411,
        "id": "Prompt-9Vgna",
        "position": {
          "x": -874.0409173475637,
          "y": 132.49847672160848
        },
        "positionAbsolute": {
          "x": -874.0409173475637,
          "y": 132.49847672160848
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "id": "SequentialTaskAgentComponent-R7vg9",
          "node": {
            "base_classes": [
              "SequentialTask"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Creates a CrewAI Task and its associated Agent.",
            "display_name": "Sequential Task Agent",
            "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
            "edited": false,
            "field_order": [
              "role",
              "goal",
              "backstory",
              "tools",
              "llm",
              "memory",
              "verbose",
              "allow_delegation",
              "allow_code_execution",
              "agent_kwargs",
              "task_description",
              "expected_output",
              "async_execution",
              "previous_task"
            ],
            "frozen": false,
            "icon": "CrewAI",
            "lf_version": "1.0.17",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Sequential Task",
                "method": "build_agent_and_task",
                "name": "task_output",
                "selected": "SequentialTask",
                "types": [
                  "SequentialTask"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "agent_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Agent kwargs",
                "dynamic": false,
                "info": "Additional kwargs for the agent.",
                "list": true,
                "name": "agent_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "allow_code_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Code Execution",
                "dynamic": false,
                "info": "Whether the agent is allowed to execute code.",
                "list": false,
                "name": "allow_code_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "allow_delegation": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Delegation",
                "dynamic": false,
                "info": "Whether the agent is allowed to delegate tasks to other agents.",
                "list": false,
                "name": "allow_delegation",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "async_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Async Execution",
                "dynamic": false,
                "info": "Boolean flag indicating asynchronous task execution.",
                "list": false,
                "name": "async_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "backstory": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Backstory",
                "dynamic": false,
                "info": "The backstory of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "backstory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are a seasoned media discourse analyst and literature review research assistant. \nYou can frame the precise {research_question} out of the user's query, based on the {research_question}, you can swiftly and accurately sift through the internet and specialty websites/domains for semantic components that formulate answers to the {research_question}. During this process, you also diligently record the specific URL references to be cited properly in the formulated answer by complying with referencing style like 'APA 7th edition' for web pages or content."
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
              },
              "expected_output": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Expected Task Output",
                "dynamic": false,
                "info": "Clear definition of expected task outcome.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "expected_output",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "A mini research report generated that has the following signposts and content structure to follow:\n\nThe Research Question:\n- {research_question}\n\nWeb Intelligence & Sources:\n- top 5 sources researched with proper referencing style (both in-text citation and end of text reference)\n- why are they selected in the research as per relevance or merit to the {research_question}?\n- what do these sources suggest collectively (both quantitatively and qualitatively)?\n- are there any other sources or domains that should be monitored, as per relevance or merit to the {research_question} in the long run?\n\nReflection:\n- what could have been articulated by the user's query for you to infer a better/pertinent {research_question}, simply put, what information would you like the user to provide in their future query to help refine your research?\n- what compromises were committed during the web source selection process, and why?\n\nReferences:\n- provide any web sources included in this research by the APA 7th Edition referencing style.\n\n"
              },
              "goal": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Goal",
                "dynamic": false,
                "info": "The objective of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "goal",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Search the Internet to find the top 5 most relevant and up-to-date information sources and their urls to synthesize the answer to the {research_question}."
              },
              "llm": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Language Model",
                "dynamic": false,
                "info": "Language model that will run the agent.",
                "input_types": [
                  "LanguageModel"
                ],
                "list": false,
                "name": "llm",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "memory": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Memory",
                "dynamic": false,
                "info": "Whether the agent should have memory or not",
                "list": false,
                "name": "memory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "previous_task": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Previous Task",
                "dynamic": false,
                "info": "The previous task in the sequence (for chaining).",
                "input_types": [
                  "SequentialTask"
                ],
                "list": false,
                "name": "previous_task",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "role": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Role",
                "dynamic": false,
                "info": "The role of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "role",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Research Assistant"
              },
              "task_description": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Task Description",
                "dynamic": false,
                "info": "Descriptive text detailing task's purpose and execution.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "task_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "Tools at agent's disposal",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": []
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "SequentialTaskAgentComponent"
        },
        "dragging": false,
        "height": 780,
        "id": "SequentialTaskAgentComponent-R7vg9",
        "position": {
          "x": -892.1358052625828,
          "y": -1014.5316861231794
        },
        "positionAbsolute": {
          "x": -892.1358052625828,
          "y": -1014.5316861231794
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "id": "SequentialTaskAgentComponent-SWeik",
          "node": {
            "base_classes": [
              "SequentialTask"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Creates a CrewAI Task and its associated Agent.",
            "display_name": "Sequential Task Agent",
            "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
            "edited": false,
            "field_order": [
              "role",
              "goal",
              "backstory",
              "tools",
              "llm",
              "memory",
              "verbose",
              "allow_delegation",
              "allow_code_execution",
              "agent_kwargs",
              "task_description",
              "expected_output",
              "async_execution",
              "previous_task"
            ],
            "frozen": false,
            "icon": "CrewAI",
            "lf_version": "1.0.17",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Sequential Task",
                "method": "build_agent_and_task",
                "name": "task_output",
                "selected": "SequentialTask",
                "types": [
                  "SequentialTask"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "agent_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Agent kwargs",
                "dynamic": false,
                "info": "Additional kwargs for the agent.",
                "list": true,
                "name": "agent_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "allow_code_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Code Execution",
                "dynamic": false,
                "info": "Whether the agent is allowed to execute code.",
                "list": false,
                "name": "allow_code_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "allow_delegation": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Delegation",
                "dynamic": false,
                "info": "Whether the agent is allowed to delegate tasks to other agents.",
                "list": false,
                "name": "allow_delegation",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "async_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Async Execution",
                "dynamic": false,
                "info": "Boolean flag indicating asynchronous task execution.",
                "list": false,
                "name": "async_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "backstory": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Backstory",
                "dynamic": false,
                "info": "The backstory of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "backstory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are the editor of the most reputable journal in the world, and you are scrupulous about academic referencing compliance."
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
              },
              "expected_output": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Expected Task Output",
                "dynamic": false,
                "info": "Clear definition of expected task outcome.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "expected_output",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Reviewed and updated version of the input by the Research Assistant with zero ungrounded information or synthesis. Should always make sure there are matching in- text citations in the main content of the response as those listed at the end of the text references."
              },
              "goal": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Goal",
                "dynamic": false,
                "info": "The objective of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "goal",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You should review the Information provided by the Research Assistant to make it more palatable to the answer of the {research_question} found in the user's query.\n\nYou should check the validity of the sources based on their referencing value and their conditional traits:\n\nE.g.\nif from journal article database, check the paper ranking metrics such as peer-review, most cited etc; \nif from average web page, post or news website, check the author's and or news agency's credibility, political stance, and bias of their framing practice; \nif from social media, check the author's credibility in the topical domain, follower's demographics etc.\nif the above conditional check presents better outcomes, use them to update the input you get from the Research Assistant.\n\nYou should also check the referencing output by the Research Assistant to make it more rigorously complied with prevalent academic referencing styles such as APA 7th edition (if the strict APA style referencing format impossible to comply with at least number the listed references with their titles and urls)."
              },
              "llm": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Language Model",
                "dynamic": false,
                "info": "Language model that will run the agent.",
                "input_types": [
                  "LanguageModel"
                ],
                "list": false,
                "name": "llm",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "memory": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Memory",
                "dynamic": false,
                "info": "Whether the agent should have memory or not",
                "list": false,
                "name": "memory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "previous_task": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Previous Task",
                "dynamic": false,
                "info": "The previous task in the sequence (for chaining).",
                "input_types": [
                  "SequentialTask"
                ],
                "list": false,
                "name": "previous_task",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "role": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Role",
                "dynamic": false,
                "info": "The role of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "role",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Editor"
              },
              "task_description": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Task Description",
                "dynamic": false,
                "info": "Descriptive text detailing task's purpose and execution.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "task_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "Tools at agent's disposal",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": []
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "SequentialTaskAgentComponent"
        },
        "dragging": false,
        "height": 780,
        "id": "SequentialTaskAgentComponent-SWeik",
        "position": {
          "x": -240.8641271716038,
          "y": -883.2318331921405
        },
        "positionAbsolute": {
          "x": -240.8641271716038,
          "y": -883.2318331921405
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "data": {
          "id": "SequentialTaskAgentComponent-ROOgc",
          "node": {
            "base_classes": [
              "SequentialTask"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Creates a CrewAI Task and its associated Agent.",
            "display_name": "Sequential Task Agent",
            "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
            "edited": false,
            "field_order": [
              "role",
              "goal",
              "backstory",
              "tools",
              "llm",
              "memory",
              "verbose",
              "allow_delegation",
              "allow_code_execution",
              "agent_kwargs",
              "task_description",
              "expected_output",
              "async_execution",
              "previous_task"
            ],
            "frozen": false,
            "icon": "CrewAI",
            "lf_version": "1.0.17",
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Sequential Task",
                "method": "build_agent_and_task",
                "name": "task_output",
                "selected": "SequentialTask",
                "types": [
                  "SequentialTask"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "agent_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Agent kwargs",
                "dynamic": false,
                "info": "Additional kwargs for the agent.",
                "list": true,
                "name": "agent_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "allow_code_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Code Execution",
                "dynamic": false,
                "info": "Whether the agent is allowed to execute code.",
                "list": false,
                "name": "allow_code_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "allow_delegation": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Allow Delegation",
                "dynamic": false,
                "info": "Whether the agent is allowed to delegate tasks to other agents.",
                "list": false,
                "name": "allow_delegation",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "async_execution": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Async Execution",
                "dynamic": false,
                "info": "Boolean flag indicating asynchronous task execution.",
                "list": false,
                "name": "async_execution",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "backstory": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Backstory",
                "dynamic": false,
                "info": "The backstory of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "backstory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are a highly talented visual designer. You understand the power of media representation but you are very ethical about how the representation of the information should be enhanced without distortion or excessive visual distraction. You can quickly determine what the best visual display o data and information can be in the eyes of the business executives and in the language of the business."
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
              },
              "expected_output": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Expected Task Output",
                "dynamic": false,
                "info": "Clear definition of expected task outcome.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "expected_output",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Based on the signpost, content structure, the substance of the content, and the referencing style compliance, which have been initiated from the user's query, then processed by Research Assistant and eventually curated by the Editor. Now you need to produce a visually appealing mini research report to address the user's {research_question} with proper references as per in-text citation and end-of-text reference formatting stipulate."
              },
              "goal": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Goal",
                "dynamic": false,
                "info": "The objective of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "goal",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "You are aesthetically more superior than anyone else in the world, you are to generate business-reader friendly representation of the information received from the Editor. However, you should not invent any information to distort the essence of the information received."
              },
              "llm": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Language Model",
                "dynamic": false,
                "info": "Language model that will run the agent.",
                "input_types": [
                  "LanguageModel"
                ],
                "list": false,
                "name": "llm",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "memory": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Memory",
                "dynamic": false,
                "info": "Whether the agent should have memory or not",
                "list": false,
                "name": "memory",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "previous_task": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Previous Task",
                "dynamic": false,
                "info": "The previous task in the sequence (for chaining).",
                "input_types": [
                  "SequentialTask"
                ],
                "list": false,
                "name": "previous_task",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "role": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Role",
                "dynamic": false,
                "info": "The role of the agent.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "role",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Reporter"
              },
              "task_description": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "display_name": "Task Description",
                "dynamic": false,
                "info": "Descriptive text detailing task's purpose and execution.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "task_description",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "Tools at agent's disposal",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "name": "tools",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": []
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "verbose",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              }
            }
          },
          "type": "SequentialTaskAgentComponent"
        },
        "dragging": false,
        "height": 780,
        "id": "SequentialTaskAgentComponent-ROOgc",
        "position": {
          "x": 408.6173168676321,
          "y": -502.6871719881892
        },
        "positionAbsolute": {
          "x": 408.6173168676321,
          "y": -502.6871719881892
        },
        "selected": false,
        "type": "genericNode",
        "width": 384
      },
      {
        "id": "PerplexityModel-7o5nD",
        "type": "genericNode",
        "position": {
          "x": -1921.9475012706832,
          "y": -727.8964375527534
        },
        "data": {
          "type": "PerplexityModel",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Perplexity API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Perplexity API Key to use for the Perplexity model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import FloatInput, SecretStrInput, DropdownInput, IntInput\n\n\nclass PerplexityComponent(LCModelComponent):\n    display_name = \"Perplexity\"\n    description = \"Generate text using Perplexity LLMs.\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"llama-3.1-sonar-small-128k-online\",\n                \"llama-3.1-sonar-large-128k-online\",\n                \"llama-3.1-sonar-huge-128k-online\",\n                \"llama-3.1-sonar-small-128k-chat\",\n                \"llama-3.1-sonar-large-128k-chat\",\n                \"llama-3.1-8b-instruct\",\n                \"llama-3.1-70b-instruct\",\n            ],\n            value=\"llama-3.1-sonar-small-128k-online\",\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Perplexity API Key\",\n            info=\"The Perplexity API Key to use for the Perplexity model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.75),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = SecretStr(self.api_key).get_secret_value()\n        temperature = self.temperature\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatPerplexity(\n            model=model,\n            temperature=temperature or 0.75,\n            pplx_api_key=api_key,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            max_output_tokens=max_output_tokens,\n        )\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": "",
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "llama-3.1-sonar-small-128k-online",
                  "llama-3.1-sonar-large-128k-online",
                  "llama-3.1-sonar-huge-128k-online",
                  "llama-3.1-sonar-small-128k-chat",
                  "llama-3.1-sonar-large-128k-chat",
                  "llama-3.1-8b-instruct",
                  "llama-3.1-70b-instruct"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "llama-3.1-sonar-small-128k-online",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": "0.1",
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Perplexity LLMs.",
            "icon": "Perplexity",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Perplexity",
            "documentation": "https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "model_name",
              "max_output_tokens",
              "api_key",
              "temperature",
              "top_p",
              "n",
              "top_k"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "id": "PerplexityModel-7o5nD"
        },
        "selected": false,
        "width": 384,
        "height": 685,
        "positionAbsolute": {
          "x": -1921.9475012706832,
          "y": -727.8964375527534
        },
        "dragging": false
      },
      {
        "id": "ChatInput-779gP",
        "type": "genericNode",
        "position": {
          "x": -2150.7427455468287,
          "y": 366.5281393158039
        },
        "data": {
          "type": "ChatInput",
          "node": {
            "template": {
              "_type": "Component",
              "files": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "files",
                "value": "",
                "display_name": "Files",
                "advanced": true,
                "dynamic": false,
                "info": "Files to be sent with the message.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "How is the implementation of renewable energy projects going in Australia, any financial reports on their investment magnitude?",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "User",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "User",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Get chat inputs from the Playground.",
            "icon": "ChatInput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "id": "ChatInput-779gP"
        },
        "selected": false,
        "width": 384,
        "height": 297,
        "positionAbsolute": {
          "x": -2150.7427455468287,
          "y": 366.5281393158039
        },
        "dragging": false
      },
      {
        "id": "Prompt-GxrVd",
        "type": "genericNode",
        "position": {
          "x": -197.78316824636346,
          "y": 325.5888954599259
        },
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-GxrVd",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Research Question: {research_question}\n\nDepending on the {research_question} being asked by the user, retouch and adapt the content to present to the business executive and lay person in a universally comprehensible reporting fashion."
              },
              "research_question": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "research_question",
                "display_name": "research_question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "research_question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.17"
          },
          "type": "Prompt"
        },
        "selected": false,
        "width": 384,
        "height": 411,
        "positionAbsolute": {
          "x": -197.78316824636346,
          "y": 325.5888954599259
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "SequentialCrewComponent",
            "id": "SequentialCrewComponent-WXzHJ",
            "name": "output",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-41Gjf",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-SequentialCrewComponent-WXzHJ{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-WXzHJœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-41Gjf{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-41Gjfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "source": "SequentialCrewComponent-WXzHJ",
        "sourceHandle": "{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-WXzHJœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-41Gjf",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-41Gjfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-EfX9F",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "task_description",
            "id": "SequentialTaskAgentComponent-R7vg9",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-EfX9F{œdataTypeœ:œPromptœ,œidœ:œPrompt-EfX9Fœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-R7vg9{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "source": "Prompt-EfX9F",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-EfX9Fœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "SequentialTaskAgentComponent-R7vg9",
        "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-9Vgna",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "task_description",
            "id": "SequentialTaskAgentComponent-SWeik",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-9Vgna{œdataTypeœ:œPromptœ,œidœ:œPrompt-9Vgnaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "source": "Prompt-9Vgna",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-9Vgnaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "SequentialTaskAgentComponent-SWeik",
        "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "data": {
          "sourceHandle": {
            "dataType": "SequentialTaskAgentComponent",
            "id": "SequentialTaskAgentComponent-R7vg9",
            "name": "task_output",
            "output_types": [
              "SequentialTask"
            ]
          },
          "targetHandle": {
            "fieldName": "previous_task",
            "id": "SequentialTaskAgentComponent-SWeik",
            "inputTypes": [
              "SequentialTask"
            ],
            "type": "other"
          }
        },
        "id": "reactflow__edge-SequentialTaskAgentComponent-R7vg9{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "source": "SequentialTaskAgentComponent-R7vg9",
        "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
        "target": "SequentialTaskAgentComponent-SWeik",
        "targetHandle": "{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "PerplexityModel-7o5nD",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "SequentialTaskAgentComponent-R7vg9",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "SequentialTaskAgentComponent-R7vg9",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-7o5nD",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-R7vg9{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "PerplexityModel-7o5nD",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "SequentialTaskAgentComponent-SWeik",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "SequentialTaskAgentComponent-SWeik",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-7o5nD",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "ChatInput-779gP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-EfX9F",
        "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-EfX9Fœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "research_question",
            "id": "Prompt-EfX9F",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-779gP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-EfX9F{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-EfX9Fœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "ChatInput-779gP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-9Vgna",
        "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-9Vgnaœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "research_question",
            "id": "Prompt-9Vgna",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-779gP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-9Vgna{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-9Vgnaœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "SequentialTaskAgentComponent-SWeik",
        "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
        "target": "SequentialTaskAgentComponent-ROOgc",
        "targetHandle": "{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "previous_task",
            "id": "SequentialTaskAgentComponent-ROOgc",
            "inputTypes": [
              "SequentialTask"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "SequentialTaskAgentComponent",
            "id": "SequentialTaskAgentComponent-SWeik",
            "name": "task_output",
            "output_types": [
              "SequentialTask"
            ]
          }
        },
        "id": "reactflow__edge-SequentialTaskAgentComponent-SWeik{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "SequentialTaskAgentComponent-ROOgc",
        "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
        "target": "SequentialCrewComponent-WXzHJ",
        "targetHandle": "{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-WXzHJœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "tasks",
            "id": "SequentialCrewComponent-WXzHJ",
            "inputTypes": [
              "SequentialTask"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "SequentialTaskAgentComponent",
            "id": "SequentialTaskAgentComponent-ROOgc",
            "name": "task_output",
            "output_types": [
              "SequentialTask"
            ]
          }
        },
        "id": "reactflow__edge-SequentialTaskAgentComponent-ROOgc{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialCrewComponent-WXzHJ{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-WXzHJœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "PerplexityModel-7o5nD",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "SequentialTaskAgentComponent-ROOgc",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "SequentialTaskAgentComponent-ROOgc",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-7o5nD",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "className": ""
      },
      {
        "source": "ChatInput-779gP",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-GxrVd",
        "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-GxrVdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "research_question",
            "id": "Prompt-GxrVd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-779gP",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-GxrVd{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-GxrVdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Prompt-GxrVd",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-GxrVdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "SequentialTaskAgentComponent-ROOgc",
        "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "task_description",
            "id": "SequentialTaskAgentComponent-ROOgc",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-GxrVd",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-GxrVd{œdataTypeœ:œPromptœ,œidœ:œPrompt-GxrVdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      }
    ],
    "viewport": {
      "x": 124.47539546235168,
      "y": 167.7043541784392,
      "zoom": 0.47097578935072504
    }
  },
  "metadata": {
    "SequentialCrewComponent": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "Prompt": {
      "count": 3
    },
    "SequentialTaskAgentComponent": {
      "count": 3
    },
    "PerplexityModel": {
      "count": 1
    },
    "ChatInput": {
      "count": 1
    },
    "total": 10
  },
  "original": {
    "id": "dee79661-e1f6-47fe-8723-8dfd8cf79fb3",
    "name": "Agent - Perplexity Web RAG for BIA",
    "description": "This Agent runs tasks in a predefined sequence to augment the local RAG.",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "0",
    "metadata": {
      "SequentialCrewComponent": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "Prompt": {
        "count": 3
      },
      "SequentialTaskAgentComponent": {
        "count": 3
      },
      "PerplexityModel": {
        "count": 1
      },
      "ChatInput": {
        "count": 1
      },
      "total": 10
    },
    "last_tested_version": "1.0.17",
    "private": true,
    "data": {
      "nodes": [
        {
          "data": {
            "description": "Represents a group of agents, defining how they should collaborate and the tasks they should perform.",
            "display_name": "Sequential Crew",
            "id": "SequentialCrewComponent-WXzHJ",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Represents a group of agents with tasks that are executed sequentially.",
              "display_name": "Sequential Crew",
              "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
              "edited": false,
              "field_order": [
                "verbose",
                "memory",
                "use_cache",
                "max_rpm",
                "share_crew",
                "function_calling_llm",
                "tasks"
              ],
              "frozen": false,
              "icon": "CrewAI",
              "lf_version": "1.0.17",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Output",
                  "method": "build_output",
                  "name": "output",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from crewai import Agent, Crew, Process, Task  # type: ignore\n\nfrom axiestudio.base.agents.crewai.crew import BaseCrewComponent\nfrom axiestudio.io import HandleInput\nfrom axiestudio.schema.message import Message\n\n\nclass SequentialCrewComponent(BaseCrewComponent):\n    display_name: str = \"Sequential Crew\"\n    description: str = \"Represents a group of agents with tasks that are executed sequentially.\"\n    documentation: str = \"https://docs.crewai.com/how-to/Sequential/\"\n    icon = \"CrewAI\"\n\n    inputs = BaseCrewComponent._base_inputs + [\n        HandleInput(name=\"tasks\", display_name=\"Tasks\", input_types=[\"SequentialTask\"], is_list=True),\n    ]\n\n    def get_tasks_and_agents(self) -> tuple[list[Task], list[Agent]]:\n        return self.tasks, [task.agent for task in self.tasks]\n\n    def build_crew(self) -> Message:\n        tasks, agents = self.get_tasks_and_agents()\n        crew = Crew(\n            agents=agents,\n            tasks=tasks,\n            process=Process.sequential,\n            verbose=self.verbose,\n            memory=self.memory,\n            cache=self.use_cache,\n            max_rpm=self.max_rpm,\n            share_crew=self.share_crew,\n            function_calling_llm=self.function_calling_llm,\n            step_callback=self.get_step_callback(),\n            task_callback=self.get_task_callback(),\n        )\n        return crew\n"
                },
                "function_calling_llm": {
                  "advanced": true,
                  "display_name": "Function Calling LLM",
                  "dynamic": false,
                  "info": "Turns the ReAct CrewAI agent into a function-calling agent",
                  "input_types": [
                    "LanguageModel"
                  ],
                  "list": false,
                  "name": "function_calling_llm",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "max_rpm": {
                  "advanced": true,
                  "display_name": "Max RPM",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "max_rpm",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "int",
                  "value": 100
                },
                "memory": {
                  "advanced": true,
                  "display_name": "Memory",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "memory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "share_crew": {
                  "advanced": true,
                  "display_name": "Share Crew",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "share_crew",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "tasks": {
                  "advanced": false,
                  "display_name": "Tasks",
                  "dynamic": false,
                  "info": "",
                  "input_types": [
                    "SequentialTask"
                  ],
                  "list": true,
                  "name": "tasks",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "use_cache": {
                  "advanced": true,
                  "display_name": "Cache",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "use_cache",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                },
                "verbose": {
                  "advanced": true,
                  "display_name": "Verbose",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "verbose",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "int",
                  "value": 0
                }
              }
            },
            "type": "SequentialCrewComponent"
          },
          "dragging": false,
          "height": 283,
          "id": "SequentialCrewComponent-WXzHJ",
          "position": {
            "x": 1031.9693200138174,
            "y": 230.02911653515076
          },
          "positionAbsolute": {
            "x": 1031.9693200138174,
            "y": 230.02911653515076
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "id": "ChatOutput-41Gjf",
            "node": {
              "base_classes": [
                "Message"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Display a chat message in the Playground.",
              "display_name": "Chat Output",
              "documentation": "",
              "edited": false,
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "frozen": false,
              "icon": "ChatOutput",
              "lf_version": "1.0.17",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Message",
                  "method": "message_response",
                  "name": "message",
                  "selected": "Message",
                  "types": [
                    "Message"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"
                },
                "data_template": {
                  "advanced": true,
                  "display_name": "Data Template",
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "data_template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "{text}"
                },
                "input_value": {
                  "advanced": false,
                  "display_name": "Text",
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "input_value",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "sender": {
                  "advanced": true,
                  "display_name": "Sender Type",
                  "dynamic": false,
                  "info": "Type of sender.",
                  "name": "sender",
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Machine"
                },
                "sender_name": {
                  "advanced": true,
                  "display_name": "Sender Name",
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "sender_name",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "AI"
                },
                "session_id": {
                  "advanced": true,
                  "display_name": "Session ID",
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "name": "session_id",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "should_store_message": {
                  "advanced": true,
                  "display_name": "Store Messages",
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "list": false,
                  "name": "should_store_message",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "ChatOutput"
          },
          "dragging": false,
          "height": 297,
          "id": "ChatOutput-41Gjf",
          "position": {
            "x": 1646.728086508338,
            "y": 359.11640497700887
          },
          "selected": false,
          "type": "genericNode",
          "width": 384,
          "positionAbsolute": {
            "x": 1646.728086508338,
            "y": 359.11640497700887
          }
        },
        {
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-EfX9F",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "Research Question: {research_question}\n\nUnderstand the {research_question} being asked from the user's query and find the answer to the {research_question}."
                },
                "research_question": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "research_question",
                  "display_name": "research_question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "research_question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "type": "Prompt"
          },
          "dragging": false,
          "height": 411,
          "id": "Prompt-EfX9F",
          "position": {
            "x": -1465.2544640946471,
            "y": -2.9577069816386086
          },
          "positionAbsolute": {
            "x": -1465.2544640946471,
            "y": -2.9577069816386086
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-9Vgna",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "Research Question: {research_question}\n\nUnderstand the {research_question} being asked from the user's query, review the content you are given by other agent."
                },
                "research_question": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "research_question",
                  "display_name": "research_question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "research_question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "type": "Prompt"
          },
          "dragging": false,
          "height": 411,
          "id": "Prompt-9Vgna",
          "position": {
            "x": -874.0409173475637,
            "y": 132.49847672160848
          },
          "positionAbsolute": {
            "x": -874.0409173475637,
            "y": 132.49847672160848
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "id": "SequentialTaskAgentComponent-R7vg9",
            "node": {
              "base_classes": [
                "SequentialTask"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Creates a CrewAI Task and its associated Agent.",
              "display_name": "Sequential Task Agent",
              "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
              "edited": false,
              "field_order": [
                "role",
                "goal",
                "backstory",
                "tools",
                "llm",
                "memory",
                "verbose",
                "allow_delegation",
                "allow_code_execution",
                "agent_kwargs",
                "task_description",
                "expected_output",
                "async_execution",
                "previous_task"
              ],
              "frozen": false,
              "icon": "CrewAI",
              "lf_version": "1.0.17",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Sequential Task",
                  "method": "build_agent_and_task",
                  "name": "task_output",
                  "selected": "SequentialTask",
                  "types": [
                    "SequentialTask"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "agent_kwargs": {
                  "_input_type": "DictInput",
                  "advanced": true,
                  "display_name": "Agent kwargs",
                  "dynamic": false,
                  "info": "Additional kwargs for the agent.",
                  "list": true,
                  "name": "agent_kwargs",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "dict",
                  "value": {}
                },
                "allow_code_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Code Execution",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to execute code.",
                  "list": false,
                  "name": "allow_code_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "allow_delegation": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Delegation",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to delegate tasks to other agents.",
                  "list": false,
                  "name": "allow_delegation",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "async_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Async Execution",
                  "dynamic": false,
                  "info": "Boolean flag indicating asynchronous task execution.",
                  "list": false,
                  "name": "async_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "backstory": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Backstory",
                  "dynamic": false,
                  "info": "The backstory of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "backstory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "You are a seasoned media discourse analyst and literature review research assistant. \nYou can frame the precise {research_question} out of the user's query, based on the {research_question}, you can swiftly and accurately sift through the internet and specialty websites/domains for semantic components that formulate answers to the {research_question}. During this process, you also diligently record the specific URL references to be cited properly in the formulated answer by complying with referencing style like 'APA 7th edition' for web pages or content."
                },
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
                },
                "expected_output": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Expected Task Output",
                  "dynamic": false,
                  "info": "Clear definition of expected task outcome.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "expected_output",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "A mini research report generated that has the following signposts and content structure to follow:\n\nThe Research Question:\n- {research_question}\n\nWeb Intelligence & Sources:\n- top 5 sources researched with proper referencing style (both in-text citation and end of text reference)\n- why are they selected in the research as per relevance or merit to the {research_question}?\n- what do these sources suggest collectively (both quantitatively and qualitatively)?\n- are there any other sources or domains that should be monitored, as per relevance or merit to the {research_question} in the long run?\n\nReflection:\n- what could have been articulated by the user's query for you to infer a better/pertinent {research_question}, simply put, what information would you like the user to provide in their future query to help refine your research?\n- what compromises were committed during the web source selection process, and why?\n\nReferences:\n- provide any web sources included in this research by the APA 7th Edition referencing style.\n\n"
                },
                "goal": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Goal",
                  "dynamic": false,
                  "info": "The objective of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "goal",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Search the Internet to find the top 5 most relevant and up-to-date information sources and their urls to synthesize the answer to the {research_question}."
                },
                "llm": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Language Model",
                  "dynamic": false,
                  "info": "Language model that will run the agent.",
                  "input_types": [
                    "LanguageModel"
                  ],
                  "list": false,
                  "name": "llm",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "memory": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Memory",
                  "dynamic": false,
                  "info": "Whether the agent should have memory or not",
                  "list": false,
                  "name": "memory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                },
                "previous_task": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Previous Task",
                  "dynamic": false,
                  "info": "The previous task in the sequence (for chaining).",
                  "input_types": [
                    "SequentialTask"
                  ],
                  "list": false,
                  "name": "previous_task",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "role": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Role",
                  "dynamic": false,
                  "info": "The role of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "role",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Research Assistant"
                },
                "task_description": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Task Description",
                  "dynamic": false,
                  "info": "Descriptive text detailing task's purpose and execution.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "task_description",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "tools": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Tools",
                  "dynamic": false,
                  "info": "Tools at agent's disposal",
                  "input_types": [
                    "Tool"
                  ],
                  "list": true,
                  "name": "tools",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": []
                },
                "verbose": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Verbose",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "verbose",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "SequentialTaskAgentComponent"
          },
          "dragging": false,
          "height": 780,
          "id": "SequentialTaskAgentComponent-R7vg9",
          "position": {
            "x": -892.1358052625828,
            "y": -1014.5316861231794
          },
          "positionAbsolute": {
            "x": -892.1358052625828,
            "y": -1014.5316861231794
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "id": "SequentialTaskAgentComponent-SWeik",
            "node": {
              "base_classes": [
                "SequentialTask"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Creates a CrewAI Task and its associated Agent.",
              "display_name": "Sequential Task Agent",
              "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
              "edited": false,
              "field_order": [
                "role",
                "goal",
                "backstory",
                "tools",
                "llm",
                "memory",
                "verbose",
                "allow_delegation",
                "allow_code_execution",
                "agent_kwargs",
                "task_description",
                "expected_output",
                "async_execution",
                "previous_task"
              ],
              "frozen": false,
              "icon": "CrewAI",
              "lf_version": "1.0.17",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Sequential Task",
                  "method": "build_agent_and_task",
                  "name": "task_output",
                  "selected": "SequentialTask",
                  "types": [
                    "SequentialTask"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "agent_kwargs": {
                  "_input_type": "DictInput",
                  "advanced": true,
                  "display_name": "Agent kwargs",
                  "dynamic": false,
                  "info": "Additional kwargs for the agent.",
                  "list": true,
                  "name": "agent_kwargs",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "dict",
                  "value": {}
                },
                "allow_code_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Code Execution",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to execute code.",
                  "list": false,
                  "name": "allow_code_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "allow_delegation": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Delegation",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to delegate tasks to other agents.",
                  "list": false,
                  "name": "allow_delegation",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "async_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Async Execution",
                  "dynamic": false,
                  "info": "Boolean flag indicating asynchronous task execution.",
                  "list": false,
                  "name": "async_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "backstory": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Backstory",
                  "dynamic": false,
                  "info": "The backstory of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "backstory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "You are the editor of the most reputable journal in the world, and you are scrupulous about academic referencing compliance."
                },
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
                },
                "expected_output": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Expected Task Output",
                  "dynamic": false,
                  "info": "Clear definition of expected task outcome.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "expected_output",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Reviewed and updated version of the input by the Research Assistant with zero ungrounded information or synthesis. Should always make sure there are matching in- text citations in the main content of the response as those listed at the end of the text references."
                },
                "goal": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Goal",
                  "dynamic": false,
                  "info": "The objective of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "goal",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "You should review the Information provided by the Research Assistant to make it more palatable to the answer of the {research_question} found in the user's query.\n\nYou should check the validity of the sources based on their referencing value and their conditional traits:\n\nE.g.\nif from journal article database, check the paper ranking metrics such as peer-review, most cited etc; \nif from average web page, post or news website, check the author's and or news agency's credibility, political stance, and bias of their framing practice; \nif from social media, check the author's credibility in the topical domain, follower's demographics etc.\nif the above conditional check presents better outcomes, use them to update the input you get from the Research Assistant.\n\nYou should also check the referencing output by the Research Assistant to make it more rigorously complied with prevalent academic referencing styles such as APA 7th edition (if the strict APA style referencing format impossible to comply with at least number the listed references with their titles and urls)."
                },
                "llm": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Language Model",
                  "dynamic": false,
                  "info": "Language model that will run the agent.",
                  "input_types": [
                    "LanguageModel"
                  ],
                  "list": false,
                  "name": "llm",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "memory": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Memory",
                  "dynamic": false,
                  "info": "Whether the agent should have memory or not",
                  "list": false,
                  "name": "memory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                },
                "previous_task": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Previous Task",
                  "dynamic": false,
                  "info": "The previous task in the sequence (for chaining).",
                  "input_types": [
                    "SequentialTask"
                  ],
                  "list": false,
                  "name": "previous_task",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "role": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Role",
                  "dynamic": false,
                  "info": "The role of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "role",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Editor"
                },
                "task_description": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Task Description",
                  "dynamic": false,
                  "info": "Descriptive text detailing task's purpose and execution.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "task_description",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "tools": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Tools",
                  "dynamic": false,
                  "info": "Tools at agent's disposal",
                  "input_types": [
                    "Tool"
                  ],
                  "list": true,
                  "name": "tools",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": []
                },
                "verbose": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Verbose",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "verbose",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "SequentialTaskAgentComponent"
          },
          "dragging": false,
          "height": 780,
          "id": "SequentialTaskAgentComponent-SWeik",
          "position": {
            "x": -240.8641271716038,
            "y": -883.2318331921405
          },
          "positionAbsolute": {
            "x": -240.8641271716038,
            "y": -883.2318331921405
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "data": {
            "id": "SequentialTaskAgentComponent-ROOgc",
            "node": {
              "base_classes": [
                "SequentialTask"
              ],
              "beta": false,
              "conditional_paths": [],
              "custom_fields": {},
              "description": "Creates a CrewAI Task and its associated Agent.",
              "display_name": "Sequential Task Agent",
              "documentation": "https://docs.crewai.com/how-to/LLM-Connections/",
              "edited": false,
              "field_order": [
                "role",
                "goal",
                "backstory",
                "tools",
                "llm",
                "memory",
                "verbose",
                "allow_delegation",
                "allow_code_execution",
                "agent_kwargs",
                "task_description",
                "expected_output",
                "async_execution",
                "previous_task"
              ],
              "frozen": false,
              "icon": "CrewAI",
              "lf_version": "1.0.17",
              "output_types": [],
              "outputs": [
                {
                  "cache": true,
                  "display_name": "Sequential Task",
                  "method": "build_agent_and_task",
                  "name": "task_output",
                  "selected": "SequentialTask",
                  "types": [
                    "SequentialTask"
                  ],
                  "value": "__UNDEFINED__"
                }
              ],
              "pinned": false,
              "template": {
                "_type": "Component",
                "agent_kwargs": {
                  "_input_type": "DictInput",
                  "advanced": true,
                  "display_name": "Agent kwargs",
                  "dynamic": false,
                  "info": "Additional kwargs for the agent.",
                  "list": true,
                  "name": "agent_kwargs",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "dict",
                  "value": {}
                },
                "allow_code_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Code Execution",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to execute code.",
                  "list": false,
                  "name": "allow_code_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "allow_delegation": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Allow Delegation",
                  "dynamic": false,
                  "info": "Whether the agent is allowed to delegate tasks to other agents.",
                  "list": false,
                  "name": "allow_delegation",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "async_execution": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Async Execution",
                  "dynamic": false,
                  "info": "Boolean flag indicating asynchronous task execution.",
                  "list": false,
                  "name": "async_execution",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": false
                },
                "backstory": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Backstory",
                  "dynamic": false,
                  "info": "The backstory of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "backstory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "You are a highly talented visual designer. You understand the power of media representation but you are very ethical about how the representation of the information should be enhanced without distortion or excessive visual distraction. You can quickly determine what the best visual display o data and information can be in the eyes of the business executives and in the language of the business."
                },
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from crewai import Agent, Task\n\nfrom axiestudio.base.agents.crewai.tasks import SequentialTask\nfrom axiestudio.custom import Component\nfrom axiestudio.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"
                },
                "expected_output": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Expected Task Output",
                  "dynamic": false,
                  "info": "Clear definition of expected task outcome.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "expected_output",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Based on the signpost, content structure, the substance of the content, and the referencing style compliance, which have been initiated from the user's query, then processed by Research Assistant and eventually curated by the Editor. Now you need to produce a visually appealing mini research report to address the user's {research_question} with proper references as per in-text citation and end-of-text reference formatting stipulate."
                },
                "goal": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Goal",
                  "dynamic": false,
                  "info": "The objective of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "goal",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "You are aesthetically more superior than anyone else in the world, you are to generate business-reader friendly representation of the information received from the Editor. However, you should not invent any information to distort the essence of the information received."
                },
                "llm": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Language Model",
                  "dynamic": false,
                  "info": "Language model that will run the agent.",
                  "input_types": [
                    "LanguageModel"
                  ],
                  "list": false,
                  "name": "llm",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "memory": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Memory",
                  "dynamic": false,
                  "info": "Whether the agent should have memory or not",
                  "list": false,
                  "name": "memory",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                },
                "previous_task": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Previous Task",
                  "dynamic": false,
                  "info": "The previous task in the sequence (for chaining).",
                  "input_types": [
                    "SequentialTask"
                  ],
                  "list": false,
                  "name": "previous_task",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": ""
                },
                "role": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Role",
                  "dynamic": false,
                  "info": "The role of the agent.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "role",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": "Reporter"
                },
                "task_description": {
                  "_input_type": "MultilineInput",
                  "advanced": false,
                  "display_name": "Task Description",
                  "dynamic": false,
                  "info": "Descriptive text detailing task's purpose and execution.",
                  "input_types": [
                    "Message"
                  ],
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "task_description",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "type": "str",
                  "value": ""
                },
                "tools": {
                  "_input_type": "HandleInput",
                  "advanced": false,
                  "display_name": "Tools",
                  "dynamic": false,
                  "info": "Tools at agent's disposal",
                  "input_types": [
                    "Tool"
                  ],
                  "list": true,
                  "name": "tools",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "other",
                  "value": []
                },
                "verbose": {
                  "_input_type": "BoolInput",
                  "advanced": true,
                  "display_name": "Verbose",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "name": "verbose",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_metadata": true,
                  "type": "bool",
                  "value": true
                }
              }
            },
            "type": "SequentialTaskAgentComponent"
          },
          "dragging": false,
          "height": 780,
          "id": "SequentialTaskAgentComponent-ROOgc",
          "position": {
            "x": 408.6173168676321,
            "y": -502.6871719881892
          },
          "positionAbsolute": {
            "x": 408.6173168676321,
            "y": -502.6871719881892
          },
          "selected": false,
          "type": "genericNode",
          "width": 384
        },
        {
          "id": "PerplexityModel-7o5nD",
          "type": "genericNode",
          "position": {
            "x": -1921.9475012706832,
            "y": -727.8964375527534
          },
          "data": {
            "type": "PerplexityModel",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "Perplexity API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The Perplexity API Key to use for the Perplexity model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import FloatInput, SecretStrInput, DropdownInput, IntInput\n\n\nclass PerplexityComponent(LCModelComponent):\n    display_name = \"Perplexity\"\n    description = \"Generate text using Perplexity LLMs.\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"llama-3.1-sonar-small-128k-online\",\n                \"llama-3.1-sonar-large-128k-online\",\n                \"llama-3.1-sonar-huge-128k-online\",\n                \"llama-3.1-sonar-small-128k-chat\",\n                \"llama-3.1-sonar-large-128k-chat\",\n                \"llama-3.1-8b-instruct\",\n                \"llama-3.1-70b-instruct\",\n            ],\n            value=\"llama-3.1-sonar-small-128k-online\",\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Perplexity API Key\",\n            info=\"The Perplexity API Key to use for the Perplexity model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.75),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = SecretStr(self.api_key).get_secret_value()\n        temperature = self.temperature\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatPerplexity(\n            model=model,\n            temperature=temperature or 0.75,\n            pplx_api_key=api_key,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            max_output_tokens=max_output_tokens,\n        )\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageInput"
                },
                "max_output_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "max_output_tokens",
                  "value": "",
                  "display_name": "Max Output Tokens",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "llama-3.1-sonar-small-128k-online",
                    "llama-3.1-sonar-large-128k-online",
                    "llama-3.1-sonar-huge-128k-online",
                    "llama-3.1-sonar-small-128k-chat",
                    "llama-3.1-sonar-large-128k-chat",
                    "llama-3.1-8b-instruct",
                    "llama-3.1-70b-instruct"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "llama-3.1-sonar-small-128k-online",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "n": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "n",
                  "value": "",
                  "display_name": "N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stream",
                  "value": false,
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "system_message": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_message",
                  "value": "",
                  "display_name": "System Message",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": "0.1",
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "top_k": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_k",
                  "value": "",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_p",
                  "value": "",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum cumulative probability of tokens to consider when sampling.",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Generate text using Perplexity LLMs.",
              "icon": "Perplexity",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "Perplexity",
              "documentation": "https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "system_message",
                "stream",
                "model_name",
                "max_output_tokens",
                "api_key",
                "temperature",
                "top_p",
                "n",
                "top_k"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "id": "PerplexityModel-7o5nD"
          },
          "selected": false,
          "width": 384,
          "height": 685,
          "positionAbsolute": {
            "x": -1921.9475012706832,
            "y": -727.8964375527534
          },
          "dragging": false
        },
        {
          "id": "ChatInput-779gP",
          "type": "genericNode",
          "position": {
            "x": -2150.7427455468287,
            "y": 366.5281393158039
          },
          "data": {
            "type": "ChatInput",
            "node": {
              "template": {
                "_type": "Component",
                "files": {
                  "trace_as_metadata": true,
                  "file_path": "",
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "files",
                  "value": "",
                  "display_name": "Files",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Files to be sent with the message.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "How is the implementation of renewable energy projects going in Australia, any financial reports on their investment magnitude?",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender",
                  "value": "User",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender_name",
                  "value": "User",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "session_id",
                  "value": "",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "should_store_message": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "should_store_message",
                  "value": true,
                  "display_name": "Store Messages",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Get chat inputs from the Playground.",
              "icon": "ChatInput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Input",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "id": "ChatInput-779gP"
          },
          "selected": false,
          "width": 384,
          "height": 297,
          "positionAbsolute": {
            "x": -2150.7427455468287,
            "y": 366.5281393158039
          },
          "dragging": false
        },
        {
          "id": "Prompt-GxrVd",
          "type": "genericNode",
          "position": {
            "x": -197.78316824636346,
            "y": 325.5888954599259
          },
          "data": {
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "id": "Prompt-GxrVd",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "advanced": true,
                  "dynamic": true,
                  "fileTypes": [],
                  "file_path": "",
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "multiline": true,
                  "name": "code",
                  "password": false,
                  "placeholder": "",
                  "required": true,
                  "show": true,
                  "title_case": false,
                  "type": "code",
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
                },
                "template": {
                  "advanced": false,
                  "display_name": "Template",
                  "dynamic": false,
                  "info": "",
                  "list": false,
                  "load_from_db": false,
                  "name": "template",
                  "placeholder": "",
                  "required": false,
                  "show": true,
                  "title_case": false,
                  "trace_as_input": true,
                  "type": "prompt",
                  "value": "Research Question: {research_question}\n\nDepending on the {research_question} being asked by the user, retouch and adapt the content to present to the business executive and lay person in a universally comprehensible reporting fashion."
                },
                "research_question": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "research_question",
                  "display_name": "research_question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "research_question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.17"
            },
            "type": "Prompt"
          },
          "selected": false,
          "width": 384,
          "height": 411,
          "positionAbsolute": {
            "x": -197.78316824636346,
            "y": 325.5888954599259
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "SequentialCrewComponent",
              "id": "SequentialCrewComponent-WXzHJ",
              "name": "output",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-41Gjf",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-SequentialCrewComponent-WXzHJ{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-WXzHJœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-41Gjf{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-41Gjfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "source": "SequentialCrewComponent-WXzHJ",
          "sourceHandle": "{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-WXzHJœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatOutput-41Gjf",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-41Gjfœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "className": "",
          "data": {
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-EfX9F",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "task_description",
              "id": "SequentialTaskAgentComponent-R7vg9",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-Prompt-EfX9F{œdataTypeœ:œPromptœ,œidœ:œPrompt-EfX9Fœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-R7vg9{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "source": "Prompt-EfX9F",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-EfX9Fœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "SequentialTaskAgentComponent-R7vg9",
          "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "data": {
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-9Vgna",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            },
            "targetHandle": {
              "fieldName": "task_description",
              "id": "SequentialTaskAgentComponent-SWeik",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            }
          },
          "id": "reactflow__edge-Prompt-9Vgna{œdataTypeœ:œPromptœ,œidœ:œPrompt-9Vgnaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "source": "Prompt-9Vgna",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-9Vgnaœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "SequentialTaskAgentComponent-SWeik",
          "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "data": {
            "sourceHandle": {
              "dataType": "SequentialTaskAgentComponent",
              "id": "SequentialTaskAgentComponent-R7vg9",
              "name": "task_output",
              "output_types": [
                "SequentialTask"
              ]
            },
            "targetHandle": {
              "fieldName": "previous_task",
              "id": "SequentialTaskAgentComponent-SWeik",
              "inputTypes": [
                "SequentialTask"
              ],
              "type": "other"
            }
          },
          "id": "reactflow__edge-SequentialTaskAgentComponent-R7vg9{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "source": "SequentialTaskAgentComponent-R7vg9",
          "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
          "target": "SequentialTaskAgentComponent-SWeik",
          "targetHandle": "{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "PerplexityModel-7o5nD",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "SequentialTaskAgentComponent-R7vg9",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "SequentialTaskAgentComponent-R7vg9",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-7o5nD",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-R7vg9{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-R7vg9œ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "PerplexityModel-7o5nD",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "SequentialTaskAgentComponent-SWeik",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "SequentialTaskAgentComponent-SWeik",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-7o5nD",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-SWeik{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "ChatInput-779gP",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-EfX9F",
          "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-EfX9Fœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "research_question",
              "id": "Prompt-EfX9F",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-779gP",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-EfX9F{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-EfX9Fœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "ChatInput-779gP",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-9Vgna",
          "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-9Vgnaœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "research_question",
              "id": "Prompt-9Vgna",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-779gP",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-9Vgna{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-9Vgnaœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "SequentialTaskAgentComponent-SWeik",
          "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
          "target": "SequentialTaskAgentComponent-ROOgc",
          "targetHandle": "{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "previous_task",
              "id": "SequentialTaskAgentComponent-ROOgc",
              "inputTypes": [
                "SequentialTask"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "SequentialTaskAgentComponent",
              "id": "SequentialTaskAgentComponent-SWeik",
              "name": "task_output",
              "output_types": [
                "SequentialTask"
              ]
            }
          },
          "id": "reactflow__edge-SequentialTaskAgentComponent-SWeik{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-SWeikœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "SequentialTaskAgentComponent-ROOgc",
          "sourceHandle": "{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}",
          "target": "SequentialCrewComponent-WXzHJ",
          "targetHandle": "{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-WXzHJœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "tasks",
              "id": "SequentialCrewComponent-WXzHJ",
              "inputTypes": [
                "SequentialTask"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "SequentialTaskAgentComponent",
              "id": "SequentialTaskAgentComponent-ROOgc",
              "name": "task_output",
              "output_types": [
                "SequentialTask"
              ]
            }
          },
          "id": "reactflow__edge-SequentialTaskAgentComponent-ROOgc{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialCrewComponent-WXzHJ{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-WXzHJœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "PerplexityModel-7o5nD",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "SequentialTaskAgentComponent-ROOgc",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "SequentialTaskAgentComponent-ROOgc",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-7o5nD",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-7o5nD{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-7o5nDœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "className": ""
        },
        {
          "source": "ChatInput-779gP",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-GxrVd",
          "targetHandle": "{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-GxrVdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "research_question",
              "id": "Prompt-GxrVd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-779gP",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-779gP{œdataTypeœ:œChatInputœ,œidœ:œChatInput-779gPœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-GxrVd{œfieldNameœ:œresearch_questionœ,œidœ:œPrompt-GxrVdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Prompt-GxrVd",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-GxrVdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "SequentialTaskAgentComponent-ROOgc",
          "targetHandle": "{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "task_description",
              "id": "SequentialTaskAgentComponent-ROOgc",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-GxrVd",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-GxrVd{œdataTypeœ:œPromptœ,œidœ:œPrompt-GxrVdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-ROOgc{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-ROOgcœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        }
      ],
      "viewport": {
        "x": 124.47539546235168,
        "y": 167.7043541784392,
        "zoom": 0.47097578935072504
      }
    },
    "date_created": "2024-10-22T04:51:04.601Z",
    "date_updated": "2024-10-22T04:51:04.743Z",
    "status": "Public",
    "sort": null,
    "user_updated": "6decf44a-a4d8-438a-92d3-df07d49ad213",
    "user_created": {
      "username": "jingconsult",
      "first_name": "Jing",
      "last_name": "Consulting",
      "id": "6decf44a-a4d8-438a-92d3-df07d49ad213"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:06.869Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 92,
    "converter_version": "1.0.0"
  }
}