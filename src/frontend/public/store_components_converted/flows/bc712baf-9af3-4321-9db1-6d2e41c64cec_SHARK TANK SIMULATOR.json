{
  "id": "bc712baf-9af3-4321-9db1-6d2e41c64cec",
  "name": "SHARK TANK SIMULATOR",
  "description": "Conversational Cartography Unlocked. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "pinch",
    "first_name": "",
    "last_name": "",
    "id": "c027f23d-fd13-4937-bfd1-526abc02c679",
    "full_name": "pinch"
  },
  "store_url": "https://www.langflow.store/store/component/bc712baf-9af3-4321-9db1-6d2e41c64cec",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-09-12T20:38:20.501Z",
    "updated": "2024-09-12T20:38:20.547Z",
    "downloaded": "2025-08-19T17:50:07.078Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.18",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "TextInput-Yc63h",
        "type": "genericNode",
        "position": {
          "x": -1062.542202331307,
          "y": 190.86376285597254
        },
        "data": {
          "type": "TextInput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "StorySpinner: AI-powered personalized bedtime stories for kids. Parents input daily family activities, milestones, and learning moments through a simple app interface. Our AI then crafts unique, engaging stories featuring family members and real-life lessons, helping children process their day and learn valuable skills. The app includes voice recording for parents to narrate, custom illustrations, and tracks child development. Freemium model with basic stories free, premium features include advanced customization, educational themes, and multiple family profiles.",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Get text inputs from the Playground.",
            "icon": "type",
            "base_classes": [
              "Message"
            ],
            "display_name": "Text Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "TextInput-Yc63h"
        },
        "selected": false,
        "width": 384,
        "height": 302,
        "positionAbsolute": {
          "x": -1062.542202331307,
          "y": 190.86376285597254
        },
        "dragging": false
      },
      {
        "id": "Prompt-vJuNj",
        "type": "genericNode",
        "position": {
          "x": -551.0416733010861,
          "y": 174.50832573737466
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "Present your app idea for Shark Tank:\n{text}\n\nInclude: App name, problem solved, target audience, functionality, revenue model, and development stage.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "text": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "text",
                "display_name": "text",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "text"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "Prompt-vJuNj"
        },
        "selected": false,
        "width": 384,
        "height": 416,
        "positionAbsolute": {
          "x": -551.0416733010861,
          "y": 174.50832573737466
        },
        "dragging": false
      },
      {
        "id": "PerplexityModel-VNJZu",
        "type": "genericNode",
        "position": {
          "x": 37.97983437284597,
          "y": 38.736238845646795
        },
        "data": {
          "type": "PerplexityModel",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Perplexity API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The Perplexity API Key to use for the Perplexity model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import FloatInput, SecretStrInput, DropdownInput, IntInput\n\n\nclass PerplexityComponent(LCModelComponent):\n    display_name = \"Perplexity\"\n    description = \"Generate text using Perplexity LLMs.\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"llama-3.1-sonar-small-128k-online\",\n                \"llama-3.1-sonar-large-128k-online\",\n                \"llama-3.1-sonar-huge-128k-online\",\n                \"llama-3.1-sonar-small-128k-chat\",\n                \"llama-3.1-sonar-large-128k-chat\",\n                \"llama-3.1-8b-instruct\",\n                \"llama-3.1-70b-instruct\",\n            ],\n            value=\"llama-3.1-sonar-small-128k-online\",\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Perplexity API Key\",\n            info=\"The Perplexity API Key to use for the Perplexity model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.75),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = SecretStr(self.api_key).get_secret_value()\n        temperature = self.temperature\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatPerplexity(\n            model=model,\n            temperature=temperature or 0.75,\n            pplx_api_key=api_key,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            max_output_tokens=max_output_tokens,\n        )\n\n        return output  # type: ignore\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "max_output_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_output_tokens",
                "value": 300,
                "display_name": "Max Output Tokens",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "llama-3.1-sonar-small-128k-online",
                  "llama-3.1-sonar-large-128k-online",
                  "llama-3.1-sonar-huge-128k-online",
                  "llama-3.1-sonar-small-128k-chat",
                  "llama-3.1-sonar-large-128k-chat",
                  "llama-3.1-8b-instruct",
                  "llama-3.1-70b-instruct"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "llama-3.1-sonar-small-128k-online",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "n": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "n",
                "value": "",
                "display_name": "N",
                "advanced": true,
                "dynamic": false,
                "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.75,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              },
              "top_k": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_k",
                "value": "",
                "display_name": "Top K",
                "advanced": true,
                "dynamic": false,
                "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "top_p": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "top_p",
                "value": "",
                "display_name": "Top P",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum cumulative probability of tokens to consider when sampling.",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generate text using Perplexity LLMs.",
            "icon": "Perplexity",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "Perplexity",
            "documentation": "https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "model_name",
              "max_output_tokens",
              "api_key",
              "temperature",
              "top_p",
              "n",
              "top_k"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "PerplexityModel-VNJZu"
        },
        "selected": false,
        "width": 384,
        "height": 691,
        "positionAbsolute": {
          "x": 37.97983437284597,
          "y": 38.736238845646795
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-tPMpw",
        "type": "genericNode",
        "position": {
          "x": 1287.8653789967448,
          "y": -302.8673503368918
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "_type": "Component",
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "memory",
                "value": "",
                "display_name": "Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMemory"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The input value to pass to the chain.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "You're a tech investor. Ask about the app's technology and scalability.",
            "base_classes": [
              "Message"
            ],
            "display_name": "TECH SHARK",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "invoke_chain",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "llm",
              "memory"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ConversationChain-tPMpw"
        },
        "selected": false,
        "width": 384,
        "height": 426,
        "positionAbsolute": {
          "x": 1287.8653789967448,
          "y": -302.8673503368918
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-YiE8J",
        "type": "genericNode",
        "position": {
          "x": 1288.9283207486133,
          "y": 220.07227499194485
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "_type": "Component",
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "memory",
                "value": "",
                "display_name": "Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMemory"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The input value to pass to the chain.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "You're a marketing expert. Inquire about target audience and user acquisition.",
            "base_classes": [
              "Message"
            ],
            "display_name": "MARKETING SHARK",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "invoke_chain",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "llm",
              "memory"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ConversationChain-YiE8J"
        },
        "selected": false,
        "width": 384,
        "height": 426,
        "positionAbsolute": {
          "x": 1288.9283207486133,
          "y": 220.07227499194485
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-rW5xG",
        "type": "genericNode",
        "position": {
          "x": 617.418578352231,
          "y": 196.63243366596066
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "_type": "Component",
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "memory",
                "value": "",
                "display_name": "Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMemory"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The input value to pass to the chain.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "You're the Shark Tank host. Introduce the app idea and invite investors to ask questions.",
            "base_classes": [
              "Message"
            ],
            "display_name": "MODERATOR",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "invoke_chain",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "llm",
              "memory"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ConversationChain-rW5xG"
        },
        "selected": false,
        "width": 384,
        "height": 426,
        "positionAbsolute": {
          "x": 617.418578352231,
          "y": 196.63243366596066
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-Ci72Y",
        "type": "genericNode",
        "position": {
          "x": 1288.9261752052364,
          "y": 790.3949839859689
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "_type": "Component",
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "memory",
                "value": "",
                "display_name": "Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMemory"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The input value to pass to the chain.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "You're a financial analyst. Question the revenue model and profitability.",
            "base_classes": [
              "Message"
            ],
            "display_name": "FINANCE SHARK",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "invoke_chain",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "llm",
              "memory"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ConversationChain-Ci72Y"
        },
        "selected": false,
        "width": 384,
        "height": 426,
        "positionAbsolute": {
          "x": 1288.9261752052364,
          "y": 790.3949839859689
        },
        "dragging": false
      },
      {
        "id": "ConversationChain-rMZmk",
        "type": "genericNode",
        "position": {
          "x": 1977.0152984118686,
          "y": 108.66977958186413
        },
        "data": {
          "type": "ConversationChain",
          "node": {
            "template": {
              "_type": "Component",
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "memory": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "memory",
                "value": "",
                "display_name": "Memory",
                "advanced": false,
                "input_types": [
                  "BaseChatMemory"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The input value to pass to the chain.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "You're the final decision-maker. Review all comments and provide an investment verdict with reasoning.",
            "base_classes": [
              "Message"
            ],
            "display_name": "EVALUATOR",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "invoke_chain",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "llm",
              "memory"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ConversationChain-rMZmk"
        },
        "selected": false,
        "width": 384,
        "height": 454,
        "positionAbsolute": {
          "x": 1977.0152984118686,
          "y": 108.66977958186413
        },
        "dragging": false
      },
      {
        "id": "TextOutput-ULQAB",
        "type": "genericNode",
        "position": {
          "x": 2562.144823867685,
          "y": 191.66939275379167
        },
        "data": {
          "type": "TextOutput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Text to be passed as output.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Display a text output in the Playground.",
            "icon": "type",
            "base_classes": [
              "Message"
            ],
            "display_name": "Text Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "TextOutput-ULQAB"
        },
        "selected": false,
        "width": 384,
        "height": 302,
        "positionAbsolute": {
          "x": 2562.144823867685,
          "y": 191.66939275379167
        },
        "dragging": false
      },
      {
        "id": "Prompt-r6Pwm",
        "type": "genericNode",
        "position": {
          "x": 1976.7405461341646,
          "y": 704.2555092028784
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "Moderator: {moderator_output}\nTech Shark: {tech_shark_output}\nMarketing Shark: {marketing_shark_output}\nFinance Shark: {finance_shark_output}\nBased on the above discussion, provide your final evaluation.",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "moderator_output": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "moderator_output",
                "display_name": "moderator_output",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "tech_shark_output": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "tech_shark_output",
                "display_name": "tech_shark_output",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "marketing_shark_output": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "marketing_shark_output",
                "display_name": "marketing_shark_output",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "finance_shark_output": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "finance_shark_output",
                "display_name": "finance_shark_output",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "HUMAN MESSAGE PROMPT TEMPLATE",
            "documentation": "",
            "custom_fields": {
              "template": [
                "moderator_output",
                "tech_shark_output",
                "marketing_shark_output",
                "finance_shark_output"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false
          },
          "id": "Prompt-r6Pwm"
        },
        "selected": true,
        "width": 384,
        "height": 674,
        "positionAbsolute": {
          "x": 1976.7405461341646,
          "y": 704.2555092028784
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "TextInput-Yc63h",
        "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-Yc63hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-vJuNj",
        "targetHandle": "{œfieldNameœ:œtextœ,œidœ:œPrompt-vJuNjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "text",
            "id": "Prompt-vJuNj",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "TextInput",
            "id": "TextInput-Yc63h",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-TextInput-Yc63h{œdataTypeœ:œTextInputœ,œidœ:œTextInput-Yc63hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vJuNj{œfieldNameœ:œtextœ,œidœ:œPrompt-vJuNjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "source": "Prompt-vJuNj",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-vJuNjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "PerplexityModel-VNJZu",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œPerplexityModel-VNJZuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "PerplexityModel-VNJZu",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-vJuNj",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-vJuNj{œdataTypeœ:œPromptœ,œidœ:œPrompt-vJuNjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-PerplexityModel-VNJZu{œfieldNameœ:œinput_valueœ,œidœ:œPerplexityModel-VNJZuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConversationChain-rW5xG",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ConversationChain-rW5xG",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-rW5xG{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ConversationChain-rW5xG",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-rW5xG",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-rW5xG{œfieldNameœ:œllmœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConversationChain-tPMpw",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ConversationChain-tPMpw",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-tPMpw{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConversationChain-YiE8J",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ConversationChain-YiE8J",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-YiE8J{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ConversationChain-Ci72Y",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ConversationChain-Ci72Y",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-Ci72Y{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ConversationChain-tPMpw",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-tPMpw",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-tPMpw{œfieldNameœ:œllmœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ConversationChain-YiE8J",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-YiE8J",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-YiE8J{œfieldNameœ:œllmœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ConversationChain-Ci72Y",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-Ci72Y",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-Ci72Y{œfieldNameœ:œllmœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "source": "PerplexityModel-VNJZu",
        "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ConversationChain-rMZmk",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-rMZmkœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ConversationChain-rMZmk",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "PerplexityModel",
            "id": "PerplexityModel-VNJZu",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-rMZmk{œfieldNameœ:œllmœ,œidœ:œConversationChain-rMZmkœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "source": "ConversationChain-rMZmk",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rMZmkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "TextOutput-ULQAB",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ULQABœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "TextOutput-ULQAB",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rMZmk",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rMZmk{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rMZmkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-TextOutput-ULQAB{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ULQABœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-r6Pwm",
        "targetHandle": "{œfieldNameœ:œfinance_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "finance_shark_output",
            "id": "Prompt-r6Pwm",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œfinance_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-r6Pwm",
        "targetHandle": "{œfieldNameœ:œmarketing_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "marketing_shark_output",
            "id": "Prompt-r6Pwm",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œmarketing_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-r6Pwm",
        "targetHandle": "{œfieldNameœ:œmoderator_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "moderator_output",
            "id": "Prompt-r6Pwm",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œmoderator_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ConversationChain-rW5xG",
        "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-r6Pwm",
        "targetHandle": "{œfieldNameœ:œtech_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "tech_shark_output",
            "id": "Prompt-r6Pwm",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ConversationChain",
            "id": "ConversationChain-rW5xG",
            "name": "text",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œtech_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
      }
    ],
    "viewport": {
      "x": -9.486561032303939,
      "y": 261.85411858955314,
      "zoom": 0.4400666492342772
    }
  },
  "metadata": {
    "TextInput": {
      "count": 1
    },
    "Prompt": {
      "count": 2
    },
    "PerplexityModel": {
      "count": 1
    },
    "ConversationChain": {
      "count": 5
    },
    "TextOutput": {
      "count": 1
    },
    "total": 10
  },
  "original": {
    "id": "bc712baf-9af3-4321-9db1-6d2e41c64cec",
    "name": "SHARK TANK SIMULATOR",
    "description": "Conversational Cartography Unlocked.",
    "is_component": false,
    "liked_by_count": "1",
    "downloads_count": "11",
    "metadata": {
      "TextInput": {
        "count": 1
      },
      "Prompt": {
        "count": 2
      },
      "PerplexityModel": {
        "count": 1
      },
      "ConversationChain": {
        "count": 5
      },
      "TextOutput": {
        "count": 1
      },
      "total": 10
    },
    "last_tested_version": "1.0.18",
    "private": false,
    "data": {
      "nodes": [
        {
          "id": "TextInput-Yc63h",
          "type": "genericNode",
          "position": {
            "x": -1062.542202331307,
            "y": 190.86376285597254
          },
          "data": {
            "type": "TextInput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "StorySpinner: AI-powered personalized bedtime stories for kids. Parents input daily family activities, milestones, and learning moments through a simple app interface. Our AI then crafts unique, engaging stories featuring family members and real-life lessons, helping children process their day and learn valuable skills. The app includes voice recording for parents to narrate, custom illustrations, and tracks child development. Freemium model with basic stories free, premium features include advanced customization, educational themes, and multiple family profiles.",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Text to be passed as input.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "Get text inputs from the Playground.",
              "icon": "type",
              "base_classes": [
                "Message"
              ],
              "display_name": "Text Input",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "TextInput-Yc63h"
          },
          "selected": false,
          "width": 384,
          "height": 302,
          "positionAbsolute": {
            "x": -1062.542202331307,
            "y": 190.86376285597254
          },
          "dragging": false
        },
        {
          "id": "Prompt-vJuNj",
          "type": "genericNode",
          "position": {
            "x": -551.0416733010861,
            "y": 174.50832573737466
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "Present your app idea for Shark Tank:\n{text}\n\nInclude: App name, problem solved, target audience, functionality, revenue model, and development stage.",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "_input_type": "PromptInput"
                },
                "text": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "text",
                  "display_name": "text",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "text"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "Prompt-vJuNj"
          },
          "selected": false,
          "width": 384,
          "height": 416,
          "positionAbsolute": {
            "x": -551.0416733010861,
            "y": 174.50832573737466
          },
          "dragging": false
        },
        {
          "id": "PerplexityModel-VNJZu",
          "type": "genericNode",
          "position": {
            "x": 37.97983437284597,
            "y": 38.736238845646795
          },
          "data": {
            "type": "PerplexityModel",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "Perplexity API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The Perplexity API Key to use for the Perplexity model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain_community.chat_models import ChatPerplexity\nfrom pydantic.v1 import SecretStr\n\nfrom axiestudio.base.models.model import LCModelComponent\nfrom axiestudio.field_typing import LanguageModel\nfrom axiestudio.io import FloatInput, SecretStrInput, DropdownInput, IntInput\n\n\nclass PerplexityComponent(LCModelComponent):\n    display_name = \"Perplexity\"\n    description = \"Generate text using Perplexity LLMs.\"\n    documentation = \"https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/\"\n    icon = \"Perplexity\"\n    name = \"PerplexityModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=[\n                \"llama-3.1-sonar-small-128k-online\",\n                \"llama-3.1-sonar-large-128k-online\",\n                \"llama-3.1-sonar-huge-128k-online\",\n                \"llama-3.1-sonar-small-128k-chat\",\n                \"llama-3.1-sonar-large-128k-chat\",\n                \"llama-3.1-8b-instruct\",\n                \"llama-3.1-70b-instruct\",\n            ],\n            value=\"llama-3.1-sonar-small-128k-online\",\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"Perplexity API Key\",\n            info=\"The Perplexity API Key to use for the Perplexity model.\",\n            advanced=False,\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.75),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"The maximum cumulative probability of tokens to consider when sampling.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n\",\n            display_name=\"N\",\n            info=\"Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        api_key = SecretStr(self.api_key).get_secret_value()\n        temperature = self.temperature\n        model = self.model_name\n        max_output_tokens = self.max_output_tokens\n        top_k = self.top_k\n        top_p = self.top_p\n        n = self.n\n\n        output = ChatPerplexity(\n            model=model,\n            temperature=temperature or 0.75,\n            pplx_api_key=api_key,\n            top_k=top_k or None,\n            top_p=top_p or None,\n            n=n or 1,\n            max_output_tokens=max_output_tokens,\n        )\n\n        return output  # type: ignore\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageInput"
                },
                "max_output_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "max_output_tokens",
                  "value": 300,
                  "display_name": "Max Output Tokens",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "llama-3.1-sonar-small-128k-online",
                    "llama-3.1-sonar-large-128k-online",
                    "llama-3.1-sonar-huge-128k-online",
                    "llama-3.1-sonar-small-128k-chat",
                    "llama-3.1-sonar-large-128k-chat",
                    "llama-3.1-8b-instruct",
                    "llama-3.1-70b-instruct"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "llama-3.1-sonar-small-128k-online",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "n": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "n",
                  "value": "",
                  "display_name": "N",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stream",
                  "value": false,
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "system_message": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_message",
                  "value": "",
                  "display_name": "System Message",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.75,
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                },
                "top_k": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_k",
                  "value": "",
                  "display_name": "Top K",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "top_p": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "top_p",
                  "value": "",
                  "display_name": "Top P",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum cumulative probability of tokens to consider when sampling.",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Generate text using Perplexity LLMs.",
              "icon": "Perplexity",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "Perplexity",
              "documentation": "https://python.langchain.com/v0.2/docs/integrations/chat/perplexity/",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "system_message",
                "stream",
                "model_name",
                "max_output_tokens",
                "api_key",
                "temperature",
                "top_p",
                "n",
                "top_k"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "PerplexityModel-VNJZu"
          },
          "selected": false,
          "width": 384,
          "height": 691,
          "positionAbsolute": {
            "x": 37.97983437284597,
            "y": 38.736238845646795
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-tPMpw",
          "type": "genericNode",
          "position": {
            "x": 1287.8653789967448,
            "y": -302.8673503368918
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "_type": "Component",
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "memory",
                  "value": "",
                  "display_name": "Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMemory"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The input value to pass to the chain.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "You're a tech investor. Ask about the app's technology and scalability.",
              "base_classes": [
                "Message"
              ],
              "display_name": "TECH SHARK",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "invoke_chain",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "llm",
                "memory"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ConversationChain-tPMpw"
          },
          "selected": false,
          "width": 384,
          "height": 426,
          "positionAbsolute": {
            "x": 1287.8653789967448,
            "y": -302.8673503368918
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-YiE8J",
          "type": "genericNode",
          "position": {
            "x": 1288.9283207486133,
            "y": 220.07227499194485
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "_type": "Component",
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "memory",
                  "value": "",
                  "display_name": "Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMemory"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The input value to pass to the chain.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "You're a marketing expert. Inquire about target audience and user acquisition.",
              "base_classes": [
                "Message"
              ],
              "display_name": "MARKETING SHARK",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "invoke_chain",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "llm",
                "memory"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ConversationChain-YiE8J"
          },
          "selected": false,
          "width": 384,
          "height": 426,
          "positionAbsolute": {
            "x": 1288.9283207486133,
            "y": 220.07227499194485
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-rW5xG",
          "type": "genericNode",
          "position": {
            "x": 617.418578352231,
            "y": 196.63243366596066
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "_type": "Component",
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "memory",
                  "value": "",
                  "display_name": "Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMemory"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The input value to pass to the chain.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "You're the Shark Tank host. Introduce the app idea and invite investors to ask questions.",
              "base_classes": [
                "Message"
              ],
              "display_name": "MODERATOR",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "invoke_chain",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "llm",
                "memory"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ConversationChain-rW5xG"
          },
          "selected": false,
          "width": 384,
          "height": 426,
          "positionAbsolute": {
            "x": 617.418578352231,
            "y": 196.63243366596066
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-Ci72Y",
          "type": "genericNode",
          "position": {
            "x": 1288.9261752052364,
            "y": 790.3949839859689
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "_type": "Component",
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "memory",
                  "value": "",
                  "display_name": "Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMemory"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The input value to pass to the chain.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "You're a financial analyst. Question the revenue model and profitability.",
              "base_classes": [
                "Message"
              ],
              "display_name": "FINANCE SHARK",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "invoke_chain",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "llm",
                "memory"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ConversationChain-Ci72Y"
          },
          "selected": false,
          "width": 384,
          "height": 426,
          "positionAbsolute": {
            "x": 1288.9261752052364,
            "y": 790.3949839859689
          },
          "dragging": false
        },
        {
          "id": "ConversationChain-rMZmk",
          "type": "genericNode",
          "position": {
            "x": 1977.0152984118686,
            "y": 108.66977958186413
          },
          "data": {
            "type": "ConversationChain",
            "node": {
              "template": {
                "_type": "Component",
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "memory": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "memory",
                  "value": "",
                  "display_name": "Memory",
                  "advanced": false,
                  "input_types": [
                    "BaseChatMemory"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from langchain.chains import ConversationChain\n\nfrom axiestudio.base.chains.model import LCChainComponent\nfrom axiestudio.field_typing import Message\nfrom axiestudio.inputs import MultilineInput, HandleInput\n\n\nclass ConversationChainComponent(LCChainComponent):\n    display_name = \"ConversationChain\"\n    description = \"Chain to have a conversation and load context from memory.\"\n    name = \"ConversationChain\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\", display_name=\"Input\", info=\"The input value to pass to the chain.\", required=True\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        HandleInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            input_types=[\"BaseChatMemory\"],\n        ),\n    ]\n\n    def invoke_chain(self) -> Message:\n        if not self.memory:\n            chain = ConversationChain(llm=self.llm)\n        else:\n            chain = ConversationChain(llm=self.llm, memory=self.memory)\n\n        result = chain.invoke({\"input\": self.input_value}, config={\"callbacks\": self.get_langchain_callbacks()})\n        if isinstance(result, dict):\n            result = result.get(chain.output_key, \"\")  # type: ignore\n\n        elif isinstance(result, str):\n            result = result\n        else:\n            result = result.get(\"response\")\n        result = str(result)\n        self.status = result\n        return Message(text=result)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The input value to pass to the chain.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "You're the final decision-maker. Review all comments and provide an investment verdict with reasoning.",
              "base_classes": [
                "Message"
              ],
              "display_name": "EVALUATOR",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "invoke_chain",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "llm",
                "memory"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ConversationChain-rMZmk"
          },
          "selected": false,
          "width": 384,
          "height": 454,
          "positionAbsolute": {
            "x": 1977.0152984118686,
            "y": 108.66977958186413
          },
          "dragging": false
        },
        {
          "id": "TextOutput-ULQAB",
          "type": "genericNode",
          "position": {
            "x": 2562.144823867685,
            "y": 191.66939275379167
          },
          "data": {
            "type": "TextOutput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.text import TextComponent\nfrom axiestudio.io import MultilineInput, Output\nfrom axiestudio.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Text to be passed as output.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "Display a text output in the Playground.",
              "icon": "type",
              "base_classes": [
                "Message"
              ],
              "display_name": "Text Output",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "TextOutput-ULQAB"
          },
          "selected": false,
          "width": 384,
          "height": 302,
          "positionAbsolute": {
            "x": 2562.144823867685,
            "y": 191.66939275379167
          },
          "dragging": false
        },
        {
          "id": "Prompt-r6Pwm",
          "type": "genericNode",
          "position": {
            "x": 1976.7405461341646,
            "y": 704.2555092028784
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "Moderator: {moderator_output}\nTech Shark: {tech_shark_output}\nMarketing Shark: {marketing_shark_output}\nFinance Shark: {finance_shark_output}\nBased on the above discussion, provide your final evaluation.",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "_input_type": "PromptInput"
                },
                "moderator_output": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "moderator_output",
                  "display_name": "moderator_output",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "tech_shark_output": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "tech_shark_output",
                  "display_name": "tech_shark_output",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "marketing_shark_output": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "marketing_shark_output",
                  "display_name": "marketing_shark_output",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "finance_shark_output": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "finance_shark_output",
                  "display_name": "finance_shark_output",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "HUMAN MESSAGE PROMPT TEMPLATE",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "moderator_output",
                  "tech_shark_output",
                  "marketing_shark_output",
                  "finance_shark_output"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false
            },
            "id": "Prompt-r6Pwm"
          },
          "selected": true,
          "width": 384,
          "height": 674,
          "positionAbsolute": {
            "x": 1976.7405461341646,
            "y": 704.2555092028784
          },
          "dragging": false
        }
      ],
      "edges": [
        {
          "source": "TextInput-Yc63h",
          "sourceHandle": "{œdataTypeœ:œTextInputœ,œidœ:œTextInput-Yc63hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-vJuNj",
          "targetHandle": "{œfieldNameœ:œtextœ,œidœ:œPrompt-vJuNjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "text",
              "id": "Prompt-vJuNj",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "TextInput",
              "id": "TextInput-Yc63h",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-TextInput-Yc63h{œdataTypeœ:œTextInputœ,œidœ:œTextInput-Yc63hœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vJuNj{œfieldNameœ:œtextœ,œidœ:œPrompt-vJuNjœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
        },
        {
          "source": "Prompt-vJuNj",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-vJuNjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "PerplexityModel-VNJZu",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œPerplexityModel-VNJZuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "PerplexityModel-VNJZu",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-vJuNj",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-vJuNj{œdataTypeœ:œPromptœ,œidœ:œPrompt-vJuNjœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-PerplexityModel-VNJZu{œfieldNameœ:œinput_valueœ,œidœ:œPerplexityModel-VNJZuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ConversationChain-rW5xG",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ConversationChain-rW5xG",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-rW5xG{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ConversationChain-rW5xG",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-rW5xG",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-rW5xG{œfieldNameœ:œllmœ,œidœ:œConversationChain-rW5xGœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ConversationChain-tPMpw",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ConversationChain-tPMpw",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-tPMpw{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ConversationChain-YiE8J",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ConversationChain-YiE8J",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-YiE8J{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ConversationChain-Ci72Y",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ConversationChain-Ci72Y",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-ConversationChain-Ci72Y{œfieldNameœ:œinput_valueœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ConversationChain-tPMpw",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-tPMpw",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-tPMpw{œfieldNameœ:œllmœ,œidœ:œConversationChain-tPMpwœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ConversationChain-YiE8J",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-YiE8J",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-YiE8J{œfieldNameœ:œllmœ,œidœ:œConversationChain-YiE8Jœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ConversationChain-Ci72Y",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-Ci72Y",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-Ci72Y{œfieldNameœ:œllmœ,œidœ:œConversationChain-Ci72Yœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
        },
        {
          "source": "PerplexityModel-VNJZu",
          "sourceHandle": "{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ConversationChain-rMZmk",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œConversationChain-rMZmkœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ConversationChain-rMZmk",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "PerplexityModel",
              "id": "PerplexityModel-VNJZu",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-PerplexityModel-VNJZu{œdataTypeœ:œPerplexityModelœ,œidœ:œPerplexityModel-VNJZuœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ConversationChain-rMZmk{œfieldNameœ:œllmœ,œidœ:œConversationChain-rMZmkœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
        },
        {
          "source": "ConversationChain-rMZmk",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rMZmkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "TextOutput-ULQAB",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ULQABœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "TextOutput-ULQAB",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rMZmk",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rMZmk{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rMZmkœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-TextOutput-ULQAB{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ULQABœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-r6Pwm",
          "targetHandle": "{œfieldNameœ:œfinance_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "finance_shark_output",
              "id": "Prompt-r6Pwm",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œfinance_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-r6Pwm",
          "targetHandle": "{œfieldNameœ:œmarketing_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "marketing_shark_output",
              "id": "Prompt-r6Pwm",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œmarketing_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-r6Pwm",
          "targetHandle": "{œfieldNameœ:œmoderator_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "moderator_output",
              "id": "Prompt-r6Pwm",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œmoderator_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ConversationChain-rW5xG",
          "sourceHandle": "{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-r6Pwm",
          "targetHandle": "{œfieldNameœ:œtech_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "tech_shark_output",
              "id": "Prompt-r6Pwm",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ConversationChain",
              "id": "ConversationChain-rW5xG",
              "name": "text",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ConversationChain-rW5xG{œdataTypeœ:œConversationChainœ,œidœ:œConversationChain-rW5xGœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-r6Pwm{œfieldNameœ:œtech_shark_outputœ,œidœ:œPrompt-r6Pwmœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"
        }
      ],
      "viewport": {
        "x": -9.486561032303939,
        "y": 261.85411858955314,
        "zoom": 0.4400666492342772
      }
    },
    "date_created": "2024-09-12T20:38:20.501Z",
    "date_updated": "2024-09-12T20:38:20.547Z",
    "status": "Public",
    "sort": null,
    "user_updated": "c027f23d-fd13-4937-bfd1-526abc02c679",
    "user_created": {
      "username": "pinch",
      "first_name": null,
      "last_name": null,
      "id": "c027f23d-fd13-4937-bfd1-526abc02c679"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:09:04.770Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 72,
    "converter_version": "1.0.0"
  }
}