{
  "id": "46fc1cb7-1eba-437a-a7fd-9c47ab7b0360",
  "name": "AIDEVS India Week 3 - Agents",
  "description": "Crafting Conversations, One Node at a Time. (Converted from Langflow Store for AxieStudio compatibility)",
  "type": "FLOW",
  "is_component": false,
  "author": {
    "username": "priyanka",
    "first_name": "Priyanka",
    "last_name": "Narayanappa",
    "id": "cadd6302-230a-4cf5-be3f-b5120287e2a6",
    "full_name": "Priyanka Narayanappa"
  },
  "store_url": "https://www.langflow.store/store/component/46fc1cb7-1eba-437a-a7fd-9c47ab7b0360",
  "stats": {
    "downloads": 0,
    "likes": 0
  },
  "dates": {
    "created": "2024-10-03T12:29:42.103Z",
    "updated": "2024-10-07T12:51:24.286Z",
    "downloaded": "2025-08-19T17:50:07.475Z"
  },
  "tags": [],
  "technical": {
    "last_tested_version": "1.0.18",
    "private": false,
    "status": "Public"
  },
  "data": {
    "nodes": [
      {
        "id": "ToolCallingAgent-wCgtq",
        "type": "genericNode",
        "position": {
          "x": 2030.1095189525147,
          "y": -781.5843537006537
        },
        "data": {
          "type": "ToolCallingAgent",
          "node": {
            "template": {
              "_type": "Component",
              "chat_history": {
                "trace_as_metadata": true,
                "list": true,
                "trace_as_input": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chat_history",
                "value": "",
                "display_name": "Chat History",
                "advanced": true,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "llm": {
                "trace_as_metadata": true,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "llm",
                "value": "",
                "display_name": "Language Model",
                "advanced": false,
                "input_types": [
                  "LanguageModel"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "tools": {
                "trace_as_metadata": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "tools",
                "value": "",
                "display_name": "Tools",
                "advanced": false,
                "input_types": [
                  "Tool",
                  "BaseTool"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "other",
                "_input_type": "HandleInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from typing import Optional, List\n\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom axiestudio.base.agents.agent import LCToolsAgentComponent\nfrom axiestudio.inputs import MultilineInput\nfrom axiestudio.inputs.inputs import HandleInput, DataInput\nfrom axiestudio.schema import Data\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant. Your responses should be based solely on the tools provided. \"\n                \"If the tool is unavailable, respond with: 'I cannot provide an answer at this time. Please try again later.' \"\n                \"Do not provide answers based on your own knowledge or speculation.\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"{input}\"\n        ),\n        DataInput(name=\"chat_history\", display_name=\"Chat History\", is_list=True, advanced=True),\n    ]\n    \n\n    def get_chat_history_data(self) -> Optional[List[Data]]:\n        return self.chat_history[-5:] if self.chat_history else None\n\n    def create_agent_runnable(self):\n        if \"input\" not in self.user_prompt:\n            raise ValueError(\"Prompt must contain 'input' key.\")\n        if not self.tools:  # Check if tools are available\n            return Message(text=\"I'm unable to provide a response right now. Please try again later or rephrase your question.\")\n        messages = [\n            (\"system\", self.system_prompt),\n            (\"placeholder\", \"{chat_history}\"),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\"input\"], template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n       \n        return create_tool_calling_agent(self.llm, self.tools, prompt)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "handle_parsing_errors": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "handle_parsing_errors",
                "value": true,
                "display_name": "Handle Parse Errors",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "max_iterations": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_iterations",
                "value": 15,
                "display_name": "Max Iterations",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "system_prompt": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_prompt",
                "value": "",
                "display_name": "System Prompt",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "System prompt for the agent.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "user_prompt": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "user_prompt",
                "value": "{input}",
                "display_name": "Prompt",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "This prompt must contain 'input' key.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "verbose": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "verbose",
                "value": true,
                "display_name": "Verbose",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Agent that uses tools",
            "icon": "LangChain",
            "base_classes": [
              "AgentExecutor",
              "Message"
            ],
            "display_name": "Tool Calling Agent",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "AgentExecutor"
                ],
                "selected": "AgentExecutor",
                "name": "agent",
                "display_name": "Agent",
                "method": "build_agent",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "response",
                "display_name": "Response",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "tools",
              "llm",
              "system_prompt",
              "user_prompt",
              "chat_history"
            ],
            "beta": true,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "ToolCallingAgent-wCgtq"
        },
        "selected": false,
        "width": 384,
        "height": 615,
        "positionAbsolute": {
          "x": 2030.1095189525147,
          "y": -781.5843537006537
        },
        "dragging": false
      },
      {
        "id": "ChatInput-2Z2B9",
        "type": "genericNode",
        "position": {
          "x": -851.005131072162,
          "y": -718.8807493567282
        },
        "data": {
          "type": "ChatInput",
          "node": {
            "template": {
              "_type": "Component",
              "files": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "files",
                "display_name": "Files",
                "advanced": true,
                "dynamic": false,
                "info": "Files to be sent with the message.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "How does the \"\"tweaks\"\" system work in Langflow? Explain how tweaks allow the dynamic modification of component parameters during flow execution. \nAdditionally, describe how to dynamically adjust the `max_tokens` parameter in a language model component, allowing control over the length of the generated response.",
                "name": "input_value",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "User",
                "name": "sender",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "User",
                "name": "sender_name",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": "",
                "name": "session_id",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "value": true,
                "name": "should_store_message",
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Get chat inputs from the Playground.",
            "icon": "ChatInput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ChatInput-2Z2B9"
        },
        "selected": false,
        "width": 384,
        "height": 302,
        "positionAbsolute": {
          "x": -851.005131072162,
          "y": -718.8807493567282
        },
        "dragging": false
      },
      {
        "id": "ChatOutput-39mWo",
        "type": "genericNode",
        "position": {
          "x": 2470.4480902661626,
          "y": -688.2466982589767
        },
        "data": {
          "type": "ChatOutput",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "data_template",
                "value": "{text}",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "sender": {
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "Machine",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "AI",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "ChatOutput",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template"
            ],
            "beta": false,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "ChatOutput-39mWo",
          "description": "Display a chat message in the Playground.",
          "display_name": "Chat Output"
        },
        "selected": true,
        "width": 384,
        "height": 302,
        "positionAbsolute": {
          "x": 2470.4480902661626,
          "y": -688.2466982589767
        },
        "dragging": false
      },
      {
        "id": "Prompt-R6fUd",
        "type": "genericNode",
        "position": {
          "x": 1259.5506634330761,
          "y": -268.48851018615903
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "dedent(f\"\"\"\nYou are Langflow Technical Support Agent, an AI expert specializing in providing accurate, concise, and user-friendly technical support for Langflow, a tool for building AI workflows. \nYour role is to offer assistance to users by answering questions related to Langflow's features, usage, and troubleshooting issues. \n\n<Important Instructions:>\n1. Only provide answers based on the information retrieved from the tools available. If no tools are available or if the tool cannot provide an answer, respond with: \"I cannot provide an answer at this time. Please try again later.\"\n2. Do not generate answers based on your own knowledge or speculation.\n3. Leverage your comprehensive knowledge base, documentation, and other Langflow resources only when responding through the provided tools.Leverage your comprehensive knowledge base, documentation, and other Langflow resources to provide relevant and actionable responses.\n\n<Instructions:>\n1. Address all parts of the question with clear, step-by-step guidance or explanations. Ensure that each sub-question is answered separately.\n2. After addressing the sub-questions, provide a final, comprehensive summary that brings all the information together in a coherent response.\n3. Ensure the response is tailored to the user’s technical level (beginner, intermediate, or advanced).\n4. Include links to relevant and verified Langflow documentation, GitHub repositories, or tutorials for further reference. Avoid speculative or irrelevant links.\n5. Avoid overly technical jargon unless absolutely necessary and explain terms where required. \n6. Provide examples or code snippets, if applicable, to clarify your response.\n\nSub-questions: \n1. {sub_question_1}\n2. {sub_question_2}\n3. {sub_question_3}\n\n<Response:>\nYour response should seamlessly integrate answers to these sub-questions into a comprehensive narrative, illustrating the connections between them without simply summarizing. Aim for clarity and depth in your responses. \n\n\"\"\")",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput",
                "load_from_db": false
              },
              "sub_question_1": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "sub_question_1",
                "display_name": "sub_question_1",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "sub_question_2": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "sub_question_2",
                "display_name": "sub_question_2",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              },
              "sub_question_3": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "sub_question_3",
                "display_name": "sub_question_3",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "sub_question_1",
                "sub_question_2",
                "sub_question_3"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "Prompt-R6fUd",
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt"
        },
        "selected": false,
        "width": 384,
        "height": 588,
        "positionAbsolute": {
          "x": 1259.5506634330761,
          "y": -268.48851018615903
        },
        "dragging": false
      },
      {
        "id": "OpenAIModel-Iiyki",
        "type": "genericNode",
        "position": {
          "x": 166.27641791192275,
          "y": -792.1463959958232
        },
        "data": {
          "type": "OpenAIModel",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "OpenAI API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import operator\r\nfrom functools import reduce\r\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming\r\nfrom langchain_openai import ChatOpenAI\r\nfrom pydantic.v1 import SecretStr\r\n\r\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\r\nfrom axiestudio.base.models.model import LCModelComponent\r\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\r\nfrom axiestudio.field_typing import LanguageModel\r\nfrom axiestudio.inputs import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    IntInput,\r\n    MessageInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\n\r\nclass OpenAIModelComponent(LCModelComponent):\r\n    display_name = \"OpenAI\"\r\n    description = \"Generates text using OpenAI LLMs.\"\r\n    icon = \"OpenAI\"\r\n    name = \"OpenAIModel\"\r\n\r\n    inputs = [\r\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\r\n        IntInput(\r\n            name=\"max_tokens\",\r\n            display_name=\"Max Tokens\",\r\n            advanced=True,\r\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\r\n        ),\r\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\r\n        BoolInput(\r\n            name=\"json_mode\",\r\n            display_name=\"JSON Mode\",\r\n            advanced=True,\r\n            info=\"If True, it will output JSON regardless of passing a schema.\",\r\n        ),\r\n        DictInput(\r\n            name=\"output_schema\",\r\n            is_list=True,\r\n            display_name=\"Schema\",\r\n            advanced=True,\r\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=[\"gpt-4o-mini\"], value=\"gpt-4o-mini\"\r\n        ),\r\n        StrInput(\r\n            name=\"openai_api_base\",\r\n            display_name=\"OpenAI API Base\",\r\n            advanced=True,\r\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\r\n            advanced=False,\r\n            value=\"OPENAI_API_KEY\",\r\n        ),\r\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\r\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\r\n        StrInput(\r\n            name=\"system_message\",\r\n            display_name=\"System Message\",\r\n            info=\"System message to pass to the model.\",\r\n            advanced=True,\r\n        ),\r\n        IntInput(\r\n            name=\"seed\",\r\n            display_name=\"Seed\",\r\n            info=\"The seed controls the reproducibility of the job.\",\r\n            advanced=True,\r\n            value=42,\r\n        ),\r\n    ]\r\n\r\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\r\n        # self.output_schema is a list of dictionaries\r\n        # let's convert it to a dictionary\r\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\r\n        openai_api_key = self.api_key\r\n        temperature = self.temperature\r\n        model_name: str = self.model_name\r\n        max_tokens = self.max_tokens\r\n        model_kwargs = self.model_kwargs or {}\r\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\r\n        json_mode = bool(output_schema_dict) or self.json_mode\r\n        seed = self.seed\r\n        stream = self.stream\r\n\r\n        if openai_api_key:\r\n            api_key = SecretStr(openai_api_key)\r\n        else:\r\n            api_key = None\r\n        output = ChatOpenAI(\r\n            max_tokens=max_tokens or None,\r\n            model_kwargs=model_kwargs,\r\n            model=model_name,\r\n            base_url=openai_api_base,\r\n            api_key=api_key,\r\n            temperature=temperature or 0.1,\r\n            seed=seed,\r\n            streaming=stream,  # Enable streaming based on the input\r\n            callbacks=[StreamingStdOutCallbackHandler()] if stream else None,  # Handle streaming tokens\r\n        )\r\n        if json_mode:\r\n            if output_schema_dict:\r\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\r\n            else:\r\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\r\n\r\n        return output  # type: ignore\r\n\r\n    def _get_exception_message(self, e: Exception):\r\n        \"\"\"\r\n        Get a message from an OpenAI exception.\r\n\r\n        Args:\r\n            exception (Exception): The exception to get the message from.\r\n\r\n        Returns:\r\n            str: The message from the exception.\r\n        \"\"\"\r\n\r\n        try:\r\n            from openai import BadRequestError\r\n        except ImportError:\r\n            return\r\n        if isinstance(e, BadRequestError):\r\n            message = e.body.get(\"message\")  # type: ignore\r\n            if message:\r\n                return message\r\n        return",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Input",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "json_mode": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "json_mode",
                "value": false,
                "display_name": "JSON Mode",
                "advanced": true,
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "max_tokens": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_tokens",
                "value": "",
                "display_name": "Max Tokens",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "model_kwargs": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_kwargs",
                "value": {},
                "display_name": "Model Kwargs",
                "advanced": true,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "dict",
                "_input_type": "DictInput"
              },
              "model_name": {
                "trace_as_metadata": true,
                "options": [
                  "gpt-4o-mini"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "model_name",
                "value": "gpt-4o-mini",
                "display_name": "Model Name",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "openai_api_base": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "openai_api_base",
                "value": "",
                "display_name": "OpenAI API Base",
                "advanced": true,
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "output_schema": {
                "trace_as_input": true,
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "output_schema",
                "value": {},
                "display_name": "Schema",
                "advanced": true,
                "dynamic": false,
                "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                "title_case": false,
                "type": "dict",
                "_input_type": "DictInput"
              },
              "seed": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "seed",
                "value": 1,
                "display_name": "Seed",
                "advanced": true,
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput",
                "load_from_db": false
              },
              "stream": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "stream",
                "value": false,
                "display_name": "Stream",
                "advanced": true,
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "system_message": {
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "system_message",
                "value": "",
                "display_name": "System Message",
                "advanced": true,
                "dynamic": false,
                "info": "System message to pass to the model.",
                "title_case": false,
                "type": "str",
                "_input_type": "StrInput"
              },
              "temperature": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "temperature",
                "value": 0.1,
                "display_name": "Temperature",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "float",
                "_input_type": "FloatInput"
              }
            },
            "description": "Generates text using OpenAI LLMs.",
            "icon": "OpenAI",
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "display_name": "OpenAI",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "text_output",
                "display_name": "Text",
                "method": "text_response",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "LanguageModel"
                ],
                "selected": "LanguageModel",
                "name": "model_output",
                "display_name": "Language Model",
                "method": "build_model",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "output_schema",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "stream",
              "system_message",
              "seed"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "OpenAIModel-Iiyki"
        },
        "selected": false,
        "width": 384,
        "height": 605,
        "positionAbsolute": {
          "x": 166.27641791192275,
          "y": -792.1463959958232
        },
        "dragging": false
      },
      {
        "id": "TavilySearch-EAGTF",
        "type": "genericNode",
        "position": {
          "x": 1500.4911188453761,
          "y": -1591.7308993109439
        },
        "data": {
          "type": "TavilySearch",
          "node": {
            "template": {
              "_type": "Component",
              "api_key": {
                "load_from_db": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "api_key",
                "value": "",
                "display_name": "Tavily API Key",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Your Tavily API Key.",
                "title_case": false,
                "password": true,
                "type": "str",
                "_input_type": "SecretStrInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import httpx\r\nimport time\r\nfrom typing import List, Optional\r\nfrom pydantic import BaseModel, Field\r\nfrom axiestudio.base.langchain_utilities.model import LCToolComponent\r\nfrom axiestudio.inputs import SecretStrInput, MessageTextInput, DropdownInput, IntInput, BoolInput\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.field_typing import Tool\r\nfrom langchain.tools import StructuredTool\r\n\r\nclass TavilySearchToolComponent(LCToolComponent):\r\n    display_name = \"Tavily Search\"\r\n    description = \"Perform web searches using the Tavily API.\"\r\n    icon = \"search\"\r\n    name = \"TavilySearch\"\r\n    documentation = \"https://tavily.com/#api\"\r\n\r\n    inputs = [\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"Tavily API Key\",\r\n            required=True,\r\n            info=\"Your Tavily API Key.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"query\",\r\n            display_name=\"Search Query\",\r\n            info=\"The search query you want to execute with Tavily.\",\r\n\r\n        ),\r\n        DropdownInput(\r\n            name=\"search_depth\",\r\n            display_name=\"Search Depth\",\r\n            info=\"The depth of the search.\",\r\n            options=[\"basic\", \"advanced\"],\r\n            value=\"basic\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"topic\",\r\n            display_name=\"Search Topic\",\r\n            info=\"The category of the search.\",\r\n            options=[\"general\", \"news\"],\r\n            value=\"general\",\r\n        ),\r\n        IntInput(\r\n            name=\"max_results\",\r\n            display_name=\"Max Results\",\r\n            info=\"The maximum number of search results to return.\",\r\n            value=5,\r\n        ),\r\n        BoolInput(\r\n            name=\"include_images\",\r\n            display_name=\"Include Images\",\r\n            info=\"Include a list of query-related images in the response.\",\r\n            value=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"include_answer\",\r\n            display_name=\"Include Answer\",\r\n            info=\"Include a short answer to original query.\",\r\n            value=False,\r\n        ),\r\n    ]\r\n\r\n    class TavilySearchSchema(BaseModel):\r\n        query: str = Field(..., description=\"The search query you want to execute with Tavily.\")\r\n        search_depth: str = Field(\"basic\", description=\"The depth of the search.\")\r\n        topic: str = Field(\"general\", description=\"The category of the search.\")\r\n        max_results: int = Field(5, description=\"The maximum number of search results to return.\")\r\n        include_images: bool = Field(False, description=\"Include a list of query-related images in the response.\")\r\n        include_answer: bool = Field(False, description=\"Include a short answer to original query.\")\r\n\r\n    def run_model(self) -> List[Data]:\r\n        return self._tavily_search(\r\n            self.query,\r\n            self.search_depth,\r\n            self.topic,\r\n            self.max_results,\r\n            self.include_images,\r\n            self.include_answer,\r\n        )\r\n\r\n    def build_tool(self) -> Tool:\r\n        return StructuredTool.from_function(\r\n            name=\"tavily_search\",\r\n            description=\"Perform a web search using the Tavily API.\",\r\n            func=self._tavily_search,\r\n            args_schema=self.TavilySearchSchema,\r\n        )\r\n\r\n    def _tavily_search(\r\n        self,\r\n        query: str,\r\n        search_depth: str = \"basic\",\r\n        topic: str = \"general\",\r\n        max_results: int = 5,\r\n        include_images: bool = False,\r\n        include_answer: bool = False,\r\n    ) -> List[Data]:\r\n        try:\r\n            url = \"https://api.tavily.com/search\"\r\n            headers = {\r\n                \"content-type\": \"application/json\",\r\n                \"accept\": \"application/json\",\r\n            }\r\n            payload = {\r\n                \"api_key\": self.api_key,\r\n                \"query\": query,\r\n                \"search_depth\": search_depth,\r\n                \"topic\": topic,\r\n                \"max_results\": max_results,\r\n                \"include_images\": include_images,\r\n                \"include_answer\": include_answer,\r\n            }\r\n\r\n            retries = 3\r\n            for i in range(retries):\r\n                try:\r\n                  with httpx.Client(timeout=30.0) as client:\r\n                      response = client.post(url, json=payload, headers=headers)\r\n                  response.raise_for_status()\r\n                \r\n                  search_results = response.json()\r\n                  data_results = [\r\n                      Data(\r\n                          data={\r\n                              \"title\": result.get(\"title\"),\r\n                              \"url\": result.get(\"url\"),\r\n                              \"content\": result.get(\"content\"),\r\n                              \"score\": result.get(\"score\"),\r\n                          }\r\n                      )\r\n                      for result in search_results.get(\"results\", [])\r\n                  ]\r\n                \r\n                  if include_answer and search_results.get(\"answer\"):\r\n                      data_results.insert(0, Data(data={\"answer\": search_results[\"answer\"]}))\r\n                \r\n                  if include_images and search_results.get(\"images\"):\r\n                      data_results.append(Data(data={\"images\": search_results[\"images\"]}))\r\n                \r\n                  self.status = data_results\r\n                  return data_results\r\n                \r\n                except httpx.TimeoutException:\r\n                  if i == retries - 1:\r\n                      error_message = \"The read operation timed out after multiple attempts.\"\r\n                      return [Data(data={\"error\": error_message})]\r\n                  else:\r\n                      time.sleep(5)\r\n                      continue\r\n        \r\n        except httpx.HTTPStatusError as e:\r\n            error_message = f\"HTTP error: {e.response.status_code} - {e.response.text}\"\r\n            self.status = error_message\r\n            return [Data(data={\"error\": error_message})]\r\n        except Exception as e:\r\n            error_message = f\"Unexpected error: {str(e)}\"\r\n            self.status = error_message\r\n            return [Data(data={\"error\": error_message})]",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "include_answer": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "include_answer",
                "value": true,
                "display_name": "Include Answer",
                "advanced": false,
                "dynamic": false,
                "info": "Include a short answer to original query.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput",
                "load_from_db": false
              },
              "include_images": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "include_images",
                "value": true,
                "display_name": "Include Images",
                "advanced": false,
                "dynamic": false,
                "info": "Include a list of query-related images in the response.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput",
                "load_from_db": false
              },
              "max_results": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "max_results",
                "value": 5,
                "display_name": "Max Results",
                "advanced": false,
                "dynamic": false,
                "info": "The maximum number of search results to return.",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "query": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "query",
                "value": "",
                "display_name": "Search Query",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The search query you want to execute with Tavily.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "search_depth": {
                "trace_as_metadata": true,
                "options": [
                  "basic",
                  "advanced"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "search_depth",
                "value": "basic",
                "display_name": "Search Depth",
                "advanced": false,
                "dynamic": false,
                "info": "The depth of the search.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "topic": {
                "trace_as_metadata": true,
                "options": [
                  "general",
                  "news"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "topic",
                "value": "general",
                "display_name": "Search Topic",
                "advanced": false,
                "dynamic": false,
                "info": "The category of the search.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              }
            },
            "description": "Perform web searches using the Tavily API.",
            "icon": "search",
            "base_classes": [
              "Data",
              "Tool"
            ],
            "display_name": "Tavily Search",
            "documentation": "https://tavily.com/#api",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "api_run_model",
                "display_name": "Data",
                "method": "run_model",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Tool"
                ],
                "selected": "Tool",
                "name": "api_build_tool",
                "display_name": "Tool",
                "method": "build_tool",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "api_key",
              "query",
              "search_depth",
              "topic",
              "max_results",
              "include_images",
              "include_answer"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "TavilySearch-EAGTF"
        },
        "selected": false,
        "width": 384,
        "height": 835,
        "positionAbsolute": {
          "x": 1500.4911188453761,
          "y": -1591.7308993109439
        },
        "dragging": false
      },
      {
        "id": "LangWatchEvaluatorComponent-dXa67",
        "type": "genericNode",
        "position": {
          "x": 2921.6917180291075,
          "y": -774.0376454008002
        },
        "data": {
          "type": "LangWatchEvaluatorComponent",
          "node": {
            "template": {
              "_type": "Component",
              "context_data": {
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "context_data",
                "value": "",
                "display_name": "RAG Search Results (optional)",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "The data to be used as context for evaluation.",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "answer": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "answer",
                "value": "",
                "display_name": "Answer",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The generated answer to be evaluated.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "import re\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.inputs import MessageTextInput, DataInput\r\nfrom axiestudio.schema.message import Message\r\nfrom axiestudio.template import Output\r\nfrom axiestudio.schema import Data\r\nimport langwatch\r\n\r\nclass LangWatchEvaluatorComponent(Component):\r\n    display_name = \"LangWatch Evaluator\"\r\n    description = \"Evaluates a question-answer pair using LangWatch and provides a trace URL.\"\r\n    icon = \"view\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"question\",\r\n            display_name=\"Question\",\r\n            info=\"The question to be evaluated.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"answer\",\r\n            display_name=\"Answer\",\r\n            info=\"The generated answer to be evaluated.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"ground_truth\",\r\n            display_name=\"Correct Answer\",\r\n            info=\"The expected correct answer.\",\r\n        ),\r\n        DataInput(\r\n            name=\"context_data\",\r\n            display_name=\"RAG Search Results (optional)\",\r\n            info=\"The data to be used as context for evaluation.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_email\",\r\n            display_name=\"User Email\",\r\n            info=\"The user email for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_name\",\r\n            display_name=\"Participant Name\",\r\n            info=\"Full name for identification in the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"question_id\",\r\n            display_name=\"Question ID\",\r\n            info=\"The question ID for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Trace URL\", name=\"trace_url\", method=\"evaluate\"),\r\n    ]\r\n\r\n    async def evaluate(self) -> Data:\r\n        question = self.question\r\n        answer = self.answer\r\n        ground_truth = self.ground_truth\r\n        context_data = self.context_data\r\n        user_email = self.user_email if self.user_email else \"\"\r\n        question_id = self.question_id if self.question_id else \"\"\r\n        user_name = self.user_name if self.user_name else \"\"\r\n\r\n        # Validate email if provided\r\n        if user_email and not self.validate_email(user_email):\r\n            raise ValueError(f\"Invalid email address: {user_email}\")\r\n\r\n        langwatch.api_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0aW1lc3RhbXAiOjE3MjY2Njk3NTkyMjcsInJhbmQiOjAuMjg5OTc4Mjk4NzU2MzIzOTcsImlhdCI6MTcyNjY2OTc1OX0._ow7WQ5RSTlYE-HjdcouHUCoXf9nRnWHk4u9nfq4LIw'\r\n\r\n        trace = langwatch.trace(\r\n            metadata={\r\n                \"user_email\": user_email,\r\n                \"question_id\": question_id,\r\n                \"user_name\": user_name,\r\n            },\r\n            expected_output=ground_truth\r\n        )\r\n        \r\n        contexts = [item.text.replace(\"\\t\", \" \").replace(\"\\n\", \" \") for item in context_data[:5]] if context_data else []\r\n        rag_span = trace.span(type=\"rag\", name=\"LangWatch Evaluator\", input=question, contexts=contexts, output=answer)\r\n        rag_span.end()\r\n\r\n        trace.send_spans()\r\n\r\n        public_url = trace.share()\r\n        self.log(f\"See the trace at: {public_url}\")\r\n\r\n        self.status = Data(data={\"trace_url\": public_url})\r\n        return Data(data={\"trace_url\": public_url})\r\n        \r\n    def validate_email(self, email):\r\n        pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\r\n        return re.match(pattern, email) is not None",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "ground_truth": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "ground_truth",
                "value": "",
                "display_name": "Correct Answer",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The expected correct answer.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "question": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "question",
                "value": "",
                "display_name": "Question",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The question to be evaluated.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "question_id": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "question_id",
                "value": "",
                "display_name": "Question ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The question ID for the trace metadata.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "user_email": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "user_email",
                "value": "",
                "display_name": "User Email",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The user email for the trace metadata.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "user_name": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "user_name",
                "value": "",
                "display_name": "Participant Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Full name for identification in the trace metadata.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Evaluates a question-answer pair using LangWatch and provides a trace URL.",
            "icon": "view",
            "base_classes": [
              "Data"
            ],
            "display_name": "Langwatch Evaluator",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "trace_url",
                "display_name": "Trace URL",
                "method": "evaluate",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "question",
              "answer",
              "ground_truth",
              "context_data",
              "user_email",
              "user_name",
              "question_id"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "LangWatchEvaluatorComponent-dXa67",
          "description": "Evaluates a question-answer pair using LangWatch and provides a trace URL.",
          "display_name": "Langwatch Evaluator"
        },
        "selected": false,
        "width": 384,
        "height": 550,
        "positionAbsolute": {
          "x": 2921.6917180291075,
          "y": -774.0376454008002
        },
        "dragging": false
      },
      {
        "id": "Prompt-aWHmV",
        "type": "genericNode",
        "position": {
          "x": -316.670636728983,
          "y": -857.8164557082557
        },
        "data": {
          "type": "Prompt",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "template": {
                "trace_as_input": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "template",
                "value": "You are tasked with decomposing a complex question into three distinct sub-questions. Each sub-question should address a specific aspect of the main question, facilitating a comprehensive understanding of the topic.\n\nQuestion: {question}\n\nSub-question1: \n\nSub-question2: \n\nSub-question3: ",
                "display_name": "Template",
                "advanced": false,
                "dynamic": false,
                "info": "",
                "title_case": false,
                "type": "prompt",
                "_input_type": "PromptInput"
              },
              "question": {
                "field_type": "str",
                "required": false,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "",
                "fileTypes": [],
                "file_path": "",
                "name": "question",
                "display_name": "question",
                "advanced": false,
                "input_types": [
                  "Message",
                  "Text"
                ],
                "dynamic": false,
                "info": "",
                "load_from_db": false,
                "title_case": false,
                "type": "str"
              }
            },
            "description": "Create a prompt template with dynamic variables.",
            "icon": "prompts",
            "is_input": null,
            "is_output": null,
            "is_composition": null,
            "base_classes": [
              "Message"
            ],
            "name": "",
            "display_name": "Prompt",
            "documentation": "",
            "custom_fields": {
              "template": [
                "question"
              ]
            },
            "output_types": [],
            "full_path": null,
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "prompt",
                "hidden": null,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "template"
            ],
            "beta": false,
            "error": null,
            "edited": false,
            "lf_version": "1.0.18"
          },
          "id": "Prompt-aWHmV"
        },
        "selected": false,
        "width": 384,
        "height": 416,
        "positionAbsolute": {
          "x": -316.670636728983,
          "y": -857.8164557082557
        },
        "dragging": false
      },
      {
        "id": "CustomComponent-jJ1gH",
        "type": "genericNode",
        "position": {
          "x": 680.1020288254518,
          "y": -730.9925257915561
        },
        "data": {
          "type": "SubQuestionExtractor",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\r\nfrom axiestudio.io import MultilineInput, Output\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass SubQuestionExtractorComponent(Component):\r\n    display_name: str = \"Sub-question Extractor\"\r\n    description: str = \"Extract sub-questions from a multiline string and output each one separately.\"\r\n    icon = \"questions\"\r\n    name = \"SubQuestionExtractor\"\r\n\r\n    inputs = [\r\n        MultilineInput(\r\n            name=\"questions\",\r\n            display_name=\"Questions\",\r\n            info=\"A multiline string containing sub-questions.\",\r\n        ),\r\n    ]\r\n\r\n    # Define separate outputs for each sub-question\r\n    outputs = [\r\n        Output(display_name=\"Sub-question 1\", name=\"sub_question_1\", method=\"get_sub_question_1\"),\r\n        Output(display_name=\"Sub-question 2\", name=\"sub_question_2\", method=\"get_sub_question_2\"),\r\n        Output(display_name=\"Sub-question 3\", name=\"sub_question_3\", method=\"get_sub_question_3\"),\r\n    ]\r\n\r\n    def extract_sub_questions(self):\r\n        # Strip input and split by new lines, filtering out empty lines\r\n        questions = [q.strip() for q in self.questions.strip().split('\\n') if q.strip()]\r\n        \r\n        extracted = {}\r\n        \r\n        # Dynamically extract sub-questions\r\n        for i, question in enumerate(questions, start=1):\r\n            extracted[f\"Sub-question {i}\"] = question\r\n\r\n        return extracted\r\n\r\n    # Define separate methods to return each sub-question\r\n    def get_sub_question_1(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 1\", \"No Sub-question 1 found\"))\r\n\r\n    def get_sub_question_2(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 2\", \"No Sub-question 2 found\"))\r\n\r\n    def get_sub_question_3(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 3\", \"No Sub-question 3 found\"))\r\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "questions": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "questions",
                "value": "",
                "display_name": "Questions",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "A multiline string containing sub-questions.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Extract sub-questions from a multiline string and output each one separately.",
            "icon": "questions",
            "base_classes": [
              "Message"
            ],
            "display_name": "Custom Component",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "sub_question_1",
                "display_name": "Sub-question 1",
                "method": "get_sub_question_1",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "sub_question_2",
                "display_name": "Sub-question 2",
                "method": "get_sub_question_2",
                "value": "__UNDEFINED__",
                "cache": true
              },
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "sub_question_3",
                "display_name": "Sub-question 3",
                "method": "get_sub_question_3",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "questions"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "CustomComponent-jJ1gH"
        },
        "selected": false,
        "width": 384,
        "height": 420,
        "dragging": false,
        "positionAbsolute": {
          "x": 680.1020288254518,
          "y": -730.9925257915561
        }
      },
      {
        "id": "CustomComponent-ZBC0T",
        "type": "genericNode",
        "position": {
          "x": 1251.5416960631237,
          "y": -819.1018997155185
        },
        "data": {
          "type": "CombineQuestions",
          "node": {
            "template": {
              "_type": "Component",
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import StrInput, Output\r\nfrom axiestudio.schema.message import Message\r\n\r\nclass CombineQuestionsComponent(Component):\r\n    display_name: str = \"Combine Questions\"\r\n    description: str = \"Combine multiple sub-questions into a single string for processing.\"\r\n    icon = \"merge\"\r\n    name = \"CombineQuestions\"\r\n\r\n    inputs = [\r\n        MultilineInput(name=\"sub_question_1\", display_name=\"Sub-question 1\", info=\"First sub-question\"),\r\n        MultilineInput(name=\"sub_question_2\", display_name=\"Sub-question 2\", info=\"Second sub-question\"),\r\n        MultilineInput(name=\"sub_question_3\", display_name=\"Sub-question 3\", info=\"Third sub-question\"),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Combined Questions\", name=\"combined_questions\", method=\"combine_questions\"),\r\n    ]\r\n\r\n    def combine_questions(self) -> Message:\r\n        combined_input = (\r\n            f\"Sub-question 1: {self.sub_question_1}\\n\"\r\n            f\"Sub-question 2: {self.sub_question_2}\\n\"\r\n            f\"Sub-question 3: {self.sub_question_3}\\n\"\r\n        )\r\n        return Message(text=combined_input)\r\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "sub_question_1": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sub_question_1",
                "value": "",
                "display_name": "Sub-question 1",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "First sub-question",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sub_question_2": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sub_question_2",
                "value": "",
                "display_name": "Sub-question 2",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Second sub-question",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sub_question_3": {
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sub_question_3",
                "value": "",
                "display_name": "Sub-question 3",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Third sub-question",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              }
            },
            "description": "Combine multiple sub-questions into a single string for processing.",
            "icon": "merge",
            "base_classes": [
              "Message"
            ],
            "display_name": "Custom Component",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "combined_questions",
                "display_name": "Combined Questions",
                "method": "combine_questions",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "sub_question_1",
              "sub_question_2",
              "sub_question_3"
            ],
            "beta": false,
            "edited": true,
            "lf_version": "1.0.18"
          },
          "id": "CustomComponent-ZBC0T"
        },
        "selected": false,
        "width": 384,
        "height": 502,
        "dragging": false,
        "positionAbsolute": {
          "x": 1251.5416960631237,
          "y": -819.1018997155185
        }
      }
    ],
    "edges": [
      {
        "source": "Prompt-R6fUd",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-R6fUdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ToolCallingAgent-wCgtq",
        "targetHandle": "{œfieldNameœ:œsystem_promptœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "system_prompt",
            "id": "ToolCallingAgent-wCgtq",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-R6fUd",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-R6fUd{œdataTypeœ:œPromptœ,œidœ:œPrompt-R6fUdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œsystem_promptœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false,
        "selected": false
      },
      {
        "source": "ToolCallingAgent-wCgtq",
        "sourceHandle": "{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-wCgtqœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-39mWo",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-39mWoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-39mWo",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ToolCallingAgent",
            "id": "ToolCallingAgent-wCgtq",
            "name": "response",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ToolCallingAgent-wCgtq{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-wCgtqœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-39mWo{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-39mWoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "OpenAIModel-Iiyki",
        "sourceHandle": "{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "ToolCallingAgent-wCgtq",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "llm",
            "id": "ToolCallingAgent-wCgtq",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-Iiyki",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          }
        },
        "id": "reactflow__edge-OpenAIModel-Iiyki{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "className": "",
        "animated": false
      },
      {
        "source": "TavilySearch-EAGTF",
        "sourceHandle": "{œdataTypeœ:œTavilySearchœ,œidœ:œTavilySearch-EAGTFœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}",
        "target": "ToolCallingAgent-wCgtq",
        "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œToolœ,œBaseToolœ],œtypeœ:œotherœ}",
        "data": {
          "targetHandle": {
            "fieldName": "tools",
            "id": "ToolCallingAgent-wCgtq",
            "inputTypes": [
              "Tool",
              "BaseTool"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "TavilySearch",
            "id": "TavilySearch-EAGTF",
            "name": "api_build_tool",
            "output_types": [
              "Tool"
            ]
          }
        },
        "id": "reactflow__edge-TavilySearch-EAGTF{œdataTypeœ:œTavilySearchœ,œidœ:œTavilySearch-EAGTFœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œToolœ,œBaseToolœ],œtypeœ:œotherœ}",
        "animated": false,
        "className": ""
      },
      {
        "source": "ChatOutput-39mWo",
        "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-39mWoœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LangWatchEvaluatorComponent-dXa67",
        "targetHandle": "{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "answer",
            "id": "LangWatchEvaluatorComponent-dXa67",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatOutput",
            "id": "ChatOutput-39mWo",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "className": "",
        "animated": false,
        "id": "reactflow__edge-ChatOutput-39mWo{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-39mWoœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-dXa67{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "source": "ChatInput-2Z2B9",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "LangWatchEvaluatorComponent-dXa67",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "question",
            "id": "LangWatchEvaluatorComponent-dXa67",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-2Z2B9",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "className": "",
        "animated": false,
        "id": "reactflow__edge-ChatInput-2Z2B9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-dXa67{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false
      },
      {
        "source": "ChatInput-2Z2B9",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-aWHmV",
        "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-aWHmVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "question",
            "id": "Prompt-aWHmV",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-2Z2B9",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-2Z2B9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-aWHmV{œfieldNameœ:œquestionœ,œidœ:œPrompt-aWHmVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "Prompt-aWHmV",
        "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-aWHmVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "OpenAIModel-Iiyki",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Iiykiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-Iiyki",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-aWHmV",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-Prompt-aWHmV{œdataTypeœ:œPromptœ,œidœ:œPrompt-aWHmVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-Iiyki{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Iiykiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "OpenAIModel-Iiyki",
        "sourceHandle": "{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-jJ1gH",
        "targetHandle": "{œfieldNameœ:œquestionsœ,œidœ:œCustomComponent-jJ1gHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "questions",
            "id": "CustomComponent-jJ1gH",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-Iiyki",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OpenAIModel-Iiyki{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-jJ1gH{œfieldNameœ:œquestionsœ,œidœ:œCustomComponent-jJ1gHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-R6fUd",
        "targetHandle": "{œfieldNameœ:œsub_question_1œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_1",
            "id": "Prompt-R6fUd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_1",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_1œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-R6fUd",
        "targetHandle": "{œfieldNameœ:œsub_question_2œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_2",
            "id": "Prompt-R6fUd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_2",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_2œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}",
        "target": "Prompt-R6fUd",
        "targetHandle": "{œfieldNameœ:œsub_question_3œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_3",
            "id": "Prompt-R6fUd",
            "inputTypes": [
              "Message",
              "Text"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_3",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_3œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-ZBC0T",
        "targetHandle": "{œfieldNameœ:œsub_question_1œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_1",
            "id": "CustomComponent-ZBC0T",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_1",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_1œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-ZBC0T",
        "sourceHandle": "{œdataTypeœ:œCombineQuestionsœ,œidœ:œCustomComponent-ZBC0Tœ,œnameœ:œcombined_questionsœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ToolCallingAgent-wCgtq",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ToolCallingAgent-wCgtq",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "CombineQuestions",
            "id": "CustomComponent-ZBC0T",
            "name": "combined_questions",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-ZBC0T{œdataTypeœ:œCombineQuestionsœ,œidœ:œCustomComponent-ZBC0Tœ,œnameœ:œcombined_questionsœ,œoutput_typesœ:[œMessageœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œinput_valueœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-ZBC0T",
        "targetHandle": "{œfieldNameœ:œsub_question_2œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_2",
            "id": "CustomComponent-ZBC0T",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_2",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_2œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      },
      {
        "source": "CustomComponent-jJ1gH",
        "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-ZBC0T",
        "targetHandle": "{œfieldNameœ:œsub_question_3œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "data": {
          "targetHandle": {
            "fieldName": "sub_question_3",
            "id": "CustomComponent-ZBC0T",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "SubQuestionExtractor",
            "id": "CustomComponent-jJ1gH",
            "name": "sub_question_3",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_3œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "className": ""
      }
    ],
    "viewport": {
      "x": -2273.581560403503,
      "y": 760.0503760528654,
      "zoom": 0.9920464525884166
    }
  },
  "metadata": {
    "ToolCallingAgent": {
      "count": 1
    },
    "ChatInput": {
      "count": 1
    },
    "ChatOutput": {
      "count": 1
    },
    "Prompt": {
      "count": 2
    },
    "OpenAIModel": {
      "count": 1
    },
    "TavilySearch": {
      "count": 1
    },
    "LangWatchEvaluatorComponent": {
      "count": 1
    },
    "CustomComponent": {
      "count": 2
    },
    "total": 10
  },
  "original": {
    "id": "46fc1cb7-1eba-437a-a7fd-9c47ab7b0360",
    "name": "AIDEVS India Week 3 - Agents",
    "description": "Crafting Conversations, One Node at a Time.",
    "is_component": false,
    "liked_by_count": "0",
    "downloads_count": "4",
    "metadata": {
      "ToolCallingAgent": {
        "count": 1
      },
      "ChatInput": {
        "count": 1
      },
      "ChatOutput": {
        "count": 1
      },
      "Prompt": {
        "count": 2
      },
      "OpenAIModel": {
        "count": 1
      },
      "TavilySearch": {
        "count": 1
      },
      "LangWatchEvaluatorComponent": {
        "count": 1
      },
      "CustomComponent": {
        "count": 2
      },
      "total": 10
    },
    "last_tested_version": "1.0.18",
    "private": false,
    "data": {
      "nodes": [
        {
          "id": "ToolCallingAgent-wCgtq",
          "type": "genericNode",
          "position": {
            "x": 2030.1095189525147,
            "y": -781.5843537006537
          },
          "data": {
            "type": "ToolCallingAgent",
            "node": {
              "template": {
                "_type": "Component",
                "chat_history": {
                  "trace_as_metadata": true,
                  "list": true,
                  "trace_as_input": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "chat_history",
                  "value": "",
                  "display_name": "Chat History",
                  "advanced": true,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "DataInput"
                },
                "llm": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "llm",
                  "value": "",
                  "display_name": "Language Model",
                  "advanced": false,
                  "input_types": [
                    "LanguageModel"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "tools": {
                  "trace_as_metadata": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "tools",
                  "value": "",
                  "display_name": "Tools",
                  "advanced": false,
                  "input_types": [
                    "Tool",
                    "BaseTool"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "HandleInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from typing import Optional, List\n\nfrom langchain.agents import create_tool_calling_agent\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate\nfrom axiestudio.base.agents.agent import LCToolsAgentComponent\nfrom axiestudio.inputs import MultilineInput\nfrom axiestudio.inputs.inputs import HandleInput, DataInput\nfrom axiestudio.schema import Data\n\nclass ToolCallingAgentComponent(LCToolsAgentComponent):\n    display_name: str = \"Tool Calling Agent\"\n    description: str = \"Agent that uses tools\"\n    icon = \"LangChain\"\n    beta = True\n    name = \"ToolCallingAgent\"\n\n    inputs = LCToolsAgentComponent._base_inputs + [\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"System Prompt\",\n            info=\"System prompt for the agent.\",\n            value=\"You are a helpful assistant. Your responses should be based solely on the tools provided. \"\n                \"If the tool is unavailable, respond with: 'I cannot provide an answer at this time. Please try again later.' \"\n                \"Do not provide answers based on your own knowledge or speculation.\",\n        ),\n        MultilineInput(\n            name=\"user_prompt\", display_name=\"Prompt\", info=\"This prompt must contain 'input' key.\", value=\"{input}\"\n        ),\n        DataInput(name=\"chat_history\", display_name=\"Chat History\", is_list=True, advanced=True),\n    ]\n    \n\n    def get_chat_history_data(self) -> Optional[List[Data]]:\n        return self.chat_history[-5:] if self.chat_history else None\n\n    def create_agent_runnable(self):\n        if \"input\" not in self.user_prompt:\n            raise ValueError(\"Prompt must contain 'input' key.\")\n        if not self.tools:  # Check if tools are available\n            return Message(text=\"I'm unable to provide a response right now. Please try again later or rephrase your question.\")\n        messages = [\n            (\"system\", self.system_prompt),\n            (\"placeholder\", \"{chat_history}\"),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[\"input\"], template=self.user_prompt)),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n        prompt = ChatPromptTemplate.from_messages(messages)\n       \n        return create_tool_calling_agent(self.llm, self.tools, prompt)",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "handle_parsing_errors": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "handle_parsing_errors",
                  "value": true,
                  "display_name": "Handle Parse Errors",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "max_iterations": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "max_iterations",
                  "value": 15,
                  "display_name": "Max Iterations",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "system_prompt": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_prompt",
                  "value": "",
                  "display_name": "System Prompt",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "System prompt for the agent.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "user_prompt": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "user_prompt",
                  "value": "{input}",
                  "display_name": "Prompt",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "This prompt must contain 'input' key.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "verbose": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "verbose",
                  "value": true,
                  "display_name": "Verbose",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Agent that uses tools",
              "icon": "LangChain",
              "base_classes": [
                "AgentExecutor",
                "Message"
              ],
              "display_name": "Tool Calling Agent",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "AgentExecutor"
                  ],
                  "selected": "AgentExecutor",
                  "name": "agent",
                  "display_name": "Agent",
                  "method": "build_agent",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "response",
                  "display_name": "Response",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "handle_parsing_errors",
                "verbose",
                "max_iterations",
                "tools",
                "llm",
                "system_prompt",
                "user_prompt",
                "chat_history"
              ],
              "beta": true,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "ToolCallingAgent-wCgtq"
          },
          "selected": false,
          "width": 384,
          "height": 615,
          "positionAbsolute": {
            "x": 2030.1095189525147,
            "y": -781.5843537006537
          },
          "dragging": false
        },
        {
          "id": "ChatInput-2Z2B9",
          "type": "genericNode",
          "position": {
            "x": -851.005131072162,
            "y": -718.8807493567282
          },
          "data": {
            "type": "ChatInput",
            "node": {
              "template": {
                "_type": "Component",
                "files": {
                  "trace_as_metadata": true,
                  "file_path": "",
                  "fileTypes": [
                    "txt",
                    "md",
                    "mdx",
                    "csv",
                    "json",
                    "yaml",
                    "yml",
                    "xml",
                    "html",
                    "htm",
                    "pdf",
                    "docx",
                    "py",
                    "sh",
                    "sql",
                    "js",
                    "ts",
                    "tsx",
                    "jpg",
                    "jpeg",
                    "png",
                    "bmp",
                    "image"
                  ],
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "files",
                  "display_name": "Files",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Files to be sent with the message.",
                  "title_case": false,
                  "type": "file",
                  "_input_type": "FileInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "How does the \"\"tweaks\"\" system work in Langflow? Explain how tweaks allow the dynamic modification of component parameters during flow execution. \nAdditionally, describe how to dynamically adjust the `max_tokens` parameter in a language model component, allowing control over the length of the generated response.",
                  "name": "input_value",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as input.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "User",
                  "name": "sender",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "User",
                  "name": "sender_name",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": "",
                  "name": "session_id",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "should_store_message": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "value": true,
                  "name": "should_store_message",
                  "display_name": "Store Messages",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Get chat inputs from the Playground.",
              "icon": "ChatInput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Input",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "files"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ChatInput-2Z2B9"
          },
          "selected": false,
          "width": 384,
          "height": 302,
          "positionAbsolute": {
            "x": -851.005131072162,
            "y": -718.8807493567282
          },
          "dragging": false
        },
        {
          "id": "ChatOutput-39mWo",
          "type": "genericNode",
          "position": {
            "x": 2470.4480902661626,
            "y": -688.2466982589767
          },
          "data": {
            "type": "ChatOutput",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.io.chat import ChatComponent\nfrom axiestudio.inputs import BoolInput\nfrom axiestudio.io import DropdownInput, MessageTextInput, Output\nfrom axiestudio.memory import store_message\nfrom axiestudio.schema.message import Message\nfrom axiestudio.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "data_template": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "data_template",
                  "value": "{text}",
                  "display_name": "Data Template",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Text",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Message to be passed as output.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "sender": {
                  "trace_as_metadata": true,
                  "options": [
                    "Machine",
                    "User"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender",
                  "value": "Machine",
                  "display_name": "Sender Type",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Type of sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "sender_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sender_name",
                  "value": "AI",
                  "display_name": "Sender Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Name of the sender.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "session_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "session_id",
                  "value": "",
                  "display_name": "Session ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "should_store_message": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "should_store_message",
                  "value": true,
                  "display_name": "Store Messages",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Store the message in the history.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                }
              },
              "description": "Display a chat message in the Playground.",
              "icon": "ChatOutput",
              "base_classes": [
                "Message"
              ],
              "display_name": "Chat Output",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "message",
                  "display_name": "Message",
                  "method": "message_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "should_store_message",
                "sender",
                "sender_name",
                "session_id",
                "data_template"
              ],
              "beta": false,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "ChatOutput-39mWo",
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output"
          },
          "selected": true,
          "width": 384,
          "height": 302,
          "positionAbsolute": {
            "x": 2470.4480902661626,
            "y": -688.2466982589767
          },
          "dragging": false
        },
        {
          "id": "Prompt-R6fUd",
          "type": "genericNode",
          "position": {
            "x": 1259.5506634330761,
            "y": -268.48851018615903
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "dedent(f\"\"\"\nYou are Langflow Technical Support Agent, an AI expert specializing in providing accurate, concise, and user-friendly technical support for Langflow, a tool for building AI workflows. \nYour role is to offer assistance to users by answering questions related to Langflow's features, usage, and troubleshooting issues. \n\n<Important Instructions:>\n1. Only provide answers based on the information retrieved from the tools available. If no tools are available or if the tool cannot provide an answer, respond with: \"I cannot provide an answer at this time. Please try again later.\"\n2. Do not generate answers based on your own knowledge or speculation.\n3. Leverage your comprehensive knowledge base, documentation, and other Langflow resources only when responding through the provided tools.Leverage your comprehensive knowledge base, documentation, and other Langflow resources to provide relevant and actionable responses.\n\n<Instructions:>\n1. Address all parts of the question with clear, step-by-step guidance or explanations. Ensure that each sub-question is answered separately.\n2. After addressing the sub-questions, provide a final, comprehensive summary that brings all the information together in a coherent response.\n3. Ensure the response is tailored to the user’s technical level (beginner, intermediate, or advanced).\n4. Include links to relevant and verified Langflow documentation, GitHub repositories, or tutorials for further reference. Avoid speculative or irrelevant links.\n5. Avoid overly technical jargon unless absolutely necessary and explain terms where required. \n6. Provide examples or code snippets, if applicable, to clarify your response.\n\nSub-questions: \n1. {sub_question_1}\n2. {sub_question_2}\n3. {sub_question_3}\n\n<Response:>\nYour response should seamlessly integrate answers to these sub-questions into a comprehensive narrative, illustrating the connections between them without simply summarizing. Aim for clarity and depth in your responses. \n\n\"\"\")",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "_input_type": "PromptInput",
                  "load_from_db": false
                },
                "sub_question_1": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "sub_question_1",
                  "display_name": "sub_question_1",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "sub_question_2": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "sub_question_2",
                  "display_name": "sub_question_2",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                },
                "sub_question_3": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "sub_question_3",
                  "display_name": "sub_question_3",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "sub_question_1",
                  "sub_question_2",
                  "sub_question_3"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "Prompt-R6fUd",
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt"
          },
          "selected": false,
          "width": 384,
          "height": 588,
          "positionAbsolute": {
            "x": 1259.5506634330761,
            "y": -268.48851018615903
          },
          "dragging": false
        },
        {
          "id": "OpenAIModel-Iiyki",
          "type": "genericNode",
          "position": {
            "x": 166.27641791192275,
            "y": -792.1463959958232
          },
          "data": {
            "type": "OpenAIModel",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "OpenAI API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The OpenAI API Key to use for the OpenAI model.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import operator\r\nfrom functools import reduce\r\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming\r\nfrom langchain_openai import ChatOpenAI\r\nfrom pydantic.v1 import SecretStr\r\n\r\nfrom axiestudio.base.constants import STREAM_INFO_TEXT\r\nfrom axiestudio.base.models.model import LCModelComponent\r\nfrom axiestudio.base.models.openai_constants import MODEL_NAMES\r\nfrom axiestudio.field_typing import LanguageModel\r\nfrom axiestudio.inputs import (\r\n    BoolInput,\r\n    DictInput,\r\n    DropdownInput,\r\n    FloatInput,\r\n    IntInput,\r\n    MessageInput,\r\n    SecretStrInput,\r\n    StrInput,\r\n)\r\n\r\nclass OpenAIModelComponent(LCModelComponent):\r\n    display_name = \"OpenAI\"\r\n    description = \"Generates text using OpenAI LLMs.\"\r\n    icon = \"OpenAI\"\r\n    name = \"OpenAIModel\"\r\n\r\n    inputs = [\r\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\r\n        IntInput(\r\n            name=\"max_tokens\",\r\n            display_name=\"Max Tokens\",\r\n            advanced=True,\r\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\r\n        ),\r\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\r\n        BoolInput(\r\n            name=\"json_mode\",\r\n            display_name=\"JSON Mode\",\r\n            advanced=True,\r\n            info=\"If True, it will output JSON regardless of passing a schema.\",\r\n        ),\r\n        DictInput(\r\n            name=\"output_schema\",\r\n            is_list=True,\r\n            display_name=\"Schema\",\r\n            advanced=True,\r\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=[\"gpt-4o-mini\"], value=\"gpt-4o-mini\"\r\n        ),\r\n        StrInput(\r\n            name=\"openai_api_base\",\r\n            display_name=\"OpenAI API Base\",\r\n            advanced=True,\r\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\r\n        ),\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\r\n            advanced=False,\r\n            value=\"OPENAI_API_KEY\",\r\n        ),\r\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\r\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\r\n        StrInput(\r\n            name=\"system_message\",\r\n            display_name=\"System Message\",\r\n            info=\"System message to pass to the model.\",\r\n            advanced=True,\r\n        ),\r\n        IntInput(\r\n            name=\"seed\",\r\n            display_name=\"Seed\",\r\n            info=\"The seed controls the reproducibility of the job.\",\r\n            advanced=True,\r\n            value=42,\r\n        ),\r\n    ]\r\n\r\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\r\n        # self.output_schema is a list of dictionaries\r\n        # let's convert it to a dictionary\r\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\r\n        openai_api_key = self.api_key\r\n        temperature = self.temperature\r\n        model_name: str = self.model_name\r\n        max_tokens = self.max_tokens\r\n        model_kwargs = self.model_kwargs or {}\r\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\r\n        json_mode = bool(output_schema_dict) or self.json_mode\r\n        seed = self.seed\r\n        stream = self.stream\r\n\r\n        if openai_api_key:\r\n            api_key = SecretStr(openai_api_key)\r\n        else:\r\n            api_key = None\r\n        output = ChatOpenAI(\r\n            max_tokens=max_tokens or None,\r\n            model_kwargs=model_kwargs,\r\n            model=model_name,\r\n            base_url=openai_api_base,\r\n            api_key=api_key,\r\n            temperature=temperature or 0.1,\r\n            seed=seed,\r\n            streaming=stream,  # Enable streaming based on the input\r\n            callbacks=[StreamingStdOutCallbackHandler()] if stream else None,  # Handle streaming tokens\r\n        )\r\n        if json_mode:\r\n            if output_schema_dict:\r\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\r\n            else:\r\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\r\n\r\n        return output  # type: ignore\r\n\r\n    def _get_exception_message(self, e: Exception):\r\n        \"\"\"\r\n        Get a message from an OpenAI exception.\r\n\r\n        Args:\r\n            exception (Exception): The exception to get the message from.\r\n\r\n        Returns:\r\n            str: The message from the exception.\r\n        \"\"\"\r\n\r\n        try:\r\n            from openai import BadRequestError\r\n        except ImportError:\r\n            return\r\n        if isinstance(e, BadRequestError):\r\n            message = e.body.get(\"message\")  # type: ignore\r\n            if message:\r\n                return message\r\n        return",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "input_value": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "input_value",
                  "value": "",
                  "display_name": "Input",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageInput"
                },
                "json_mode": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "json_mode",
                  "value": false,
                  "display_name": "JSON Mode",
                  "advanced": true,
                  "dynamic": false,
                  "info": "If True, it will output JSON regardless of passing a schema.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "max_tokens": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "max_tokens",
                  "value": "",
                  "display_name": "Max Tokens",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "model_kwargs": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_kwargs",
                  "value": {},
                  "display_name": "Model Kwargs",
                  "advanced": true,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "dict",
                  "_input_type": "DictInput"
                },
                "model_name": {
                  "trace_as_metadata": true,
                  "options": [
                    "gpt-4o-mini"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "model_name",
                  "value": "gpt-4o-mini",
                  "display_name": "Model Name",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "openai_api_base": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "openai_api_base",
                  "value": "",
                  "display_name": "OpenAI API Base",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "output_schema": {
                  "trace_as_input": true,
                  "list": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "output_schema",
                  "value": {},
                  "display_name": "Schema",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.",
                  "title_case": false,
                  "type": "dict",
                  "_input_type": "DictInput"
                },
                "seed": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "seed",
                  "value": 1,
                  "display_name": "Seed",
                  "advanced": true,
                  "dynamic": false,
                  "info": "The seed controls the reproducibility of the job.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput",
                  "load_from_db": false
                },
                "stream": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "stream",
                  "value": false,
                  "display_name": "Stream",
                  "advanced": true,
                  "dynamic": false,
                  "info": "Stream the response from the model. Streaming works only in Chat.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput"
                },
                "system_message": {
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "system_message",
                  "value": "",
                  "display_name": "System Message",
                  "advanced": true,
                  "dynamic": false,
                  "info": "System message to pass to the model.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "StrInput"
                },
                "temperature": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "temperature",
                  "value": 0.1,
                  "display_name": "Temperature",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "float",
                  "_input_type": "FloatInput"
                }
              },
              "description": "Generates text using OpenAI LLMs.",
              "icon": "OpenAI",
              "base_classes": [
                "LanguageModel",
                "Message"
              ],
              "display_name": "OpenAI",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "text_output",
                  "display_name": "Text",
                  "method": "text_response",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "LanguageModel"
                  ],
                  "selected": "LanguageModel",
                  "name": "model_output",
                  "display_name": "Language Model",
                  "method": "build_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "input_value",
                "max_tokens",
                "model_kwargs",
                "json_mode",
                "output_schema",
                "model_name",
                "openai_api_base",
                "api_key",
                "temperature",
                "stream",
                "system_message",
                "seed"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "OpenAIModel-Iiyki"
          },
          "selected": false,
          "width": 384,
          "height": 605,
          "positionAbsolute": {
            "x": 166.27641791192275,
            "y": -792.1463959958232
          },
          "dragging": false
        },
        {
          "id": "TavilySearch-EAGTF",
          "type": "genericNode",
          "position": {
            "x": 1500.4911188453761,
            "y": -1591.7308993109439
          },
          "data": {
            "type": "TavilySearch",
            "node": {
              "template": {
                "_type": "Component",
                "api_key": {
                  "load_from_db": true,
                  "required": true,
                  "placeholder": "",
                  "show": true,
                  "name": "api_key",
                  "value": "",
                  "display_name": "Tavily API Key",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Your Tavily API Key.",
                  "title_case": false,
                  "password": true,
                  "type": "str",
                  "_input_type": "SecretStrInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import httpx\r\nimport time\r\nfrom typing import List, Optional\r\nfrom pydantic import BaseModel, Field\r\nfrom axiestudio.base.langchain_utilities.model import LCToolComponent\r\nfrom axiestudio.inputs import SecretStrInput, MessageTextInput, DropdownInput, IntInput, BoolInput\r\nfrom axiestudio.schema import Data\r\nfrom axiestudio.field_typing import Tool\r\nfrom langchain.tools import StructuredTool\r\n\r\nclass TavilySearchToolComponent(LCToolComponent):\r\n    display_name = \"Tavily Search\"\r\n    description = \"Perform web searches using the Tavily API.\"\r\n    icon = \"search\"\r\n    name = \"TavilySearch\"\r\n    documentation = \"https://tavily.com/#api\"\r\n\r\n    inputs = [\r\n        SecretStrInput(\r\n            name=\"api_key\",\r\n            display_name=\"Tavily API Key\",\r\n            required=True,\r\n            info=\"Your Tavily API Key.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"query\",\r\n            display_name=\"Search Query\",\r\n            info=\"The search query you want to execute with Tavily.\",\r\n\r\n        ),\r\n        DropdownInput(\r\n            name=\"search_depth\",\r\n            display_name=\"Search Depth\",\r\n            info=\"The depth of the search.\",\r\n            options=[\"basic\", \"advanced\"],\r\n            value=\"basic\",\r\n        ),\r\n        DropdownInput(\r\n            name=\"topic\",\r\n            display_name=\"Search Topic\",\r\n            info=\"The category of the search.\",\r\n            options=[\"general\", \"news\"],\r\n            value=\"general\",\r\n        ),\r\n        IntInput(\r\n            name=\"max_results\",\r\n            display_name=\"Max Results\",\r\n            info=\"The maximum number of search results to return.\",\r\n            value=5,\r\n        ),\r\n        BoolInput(\r\n            name=\"include_images\",\r\n            display_name=\"Include Images\",\r\n            info=\"Include a list of query-related images in the response.\",\r\n            value=False,\r\n        ),\r\n        BoolInput(\r\n            name=\"include_answer\",\r\n            display_name=\"Include Answer\",\r\n            info=\"Include a short answer to original query.\",\r\n            value=False,\r\n        ),\r\n    ]\r\n\r\n    class TavilySearchSchema(BaseModel):\r\n        query: str = Field(..., description=\"The search query you want to execute with Tavily.\")\r\n        search_depth: str = Field(\"basic\", description=\"The depth of the search.\")\r\n        topic: str = Field(\"general\", description=\"The category of the search.\")\r\n        max_results: int = Field(5, description=\"The maximum number of search results to return.\")\r\n        include_images: bool = Field(False, description=\"Include a list of query-related images in the response.\")\r\n        include_answer: bool = Field(False, description=\"Include a short answer to original query.\")\r\n\r\n    def run_model(self) -> List[Data]:\r\n        return self._tavily_search(\r\n            self.query,\r\n            self.search_depth,\r\n            self.topic,\r\n            self.max_results,\r\n            self.include_images,\r\n            self.include_answer,\r\n        )\r\n\r\n    def build_tool(self) -> Tool:\r\n        return StructuredTool.from_function(\r\n            name=\"tavily_search\",\r\n            description=\"Perform a web search using the Tavily API.\",\r\n            func=self._tavily_search,\r\n            args_schema=self.TavilySearchSchema,\r\n        )\r\n\r\n    def _tavily_search(\r\n        self,\r\n        query: str,\r\n        search_depth: str = \"basic\",\r\n        topic: str = \"general\",\r\n        max_results: int = 5,\r\n        include_images: bool = False,\r\n        include_answer: bool = False,\r\n    ) -> List[Data]:\r\n        try:\r\n            url = \"https://api.tavily.com/search\"\r\n            headers = {\r\n                \"content-type\": \"application/json\",\r\n                \"accept\": \"application/json\",\r\n            }\r\n            payload = {\r\n                \"api_key\": self.api_key,\r\n                \"query\": query,\r\n                \"search_depth\": search_depth,\r\n                \"topic\": topic,\r\n                \"max_results\": max_results,\r\n                \"include_images\": include_images,\r\n                \"include_answer\": include_answer,\r\n            }\r\n\r\n            retries = 3\r\n            for i in range(retries):\r\n                try:\r\n                  with httpx.Client(timeout=30.0) as client:\r\n                      response = client.post(url, json=payload, headers=headers)\r\n                  response.raise_for_status()\r\n                \r\n                  search_results = response.json()\r\n                  data_results = [\r\n                      Data(\r\n                          data={\r\n                              \"title\": result.get(\"title\"),\r\n                              \"url\": result.get(\"url\"),\r\n                              \"content\": result.get(\"content\"),\r\n                              \"score\": result.get(\"score\"),\r\n                          }\r\n                      )\r\n                      for result in search_results.get(\"results\", [])\r\n                  ]\r\n                \r\n                  if include_answer and search_results.get(\"answer\"):\r\n                      data_results.insert(0, Data(data={\"answer\": search_results[\"answer\"]}))\r\n                \r\n                  if include_images and search_results.get(\"images\"):\r\n                      data_results.append(Data(data={\"images\": search_results[\"images\"]}))\r\n                \r\n                  self.status = data_results\r\n                  return data_results\r\n                \r\n                except httpx.TimeoutException:\r\n                  if i == retries - 1:\r\n                      error_message = \"The read operation timed out after multiple attempts.\"\r\n                      return [Data(data={\"error\": error_message})]\r\n                  else:\r\n                      time.sleep(5)\r\n                      continue\r\n        \r\n        except httpx.HTTPStatusError as e:\r\n            error_message = f\"HTTP error: {e.response.status_code} - {e.response.text}\"\r\n            self.status = error_message\r\n            return [Data(data={\"error\": error_message})]\r\n        except Exception as e:\r\n            error_message = f\"Unexpected error: {str(e)}\"\r\n            self.status = error_message\r\n            return [Data(data={\"error\": error_message})]",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "include_answer": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "include_answer",
                  "value": true,
                  "display_name": "Include Answer",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Include a short answer to original query.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput",
                  "load_from_db": false
                },
                "include_images": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "include_images",
                  "value": true,
                  "display_name": "Include Images",
                  "advanced": false,
                  "dynamic": false,
                  "info": "Include a list of query-related images in the response.",
                  "title_case": false,
                  "type": "bool",
                  "_input_type": "BoolInput",
                  "load_from_db": false
                },
                "max_results": {
                  "trace_as_metadata": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "max_results",
                  "value": 5,
                  "display_name": "Max Results",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The maximum number of search results to return.",
                  "title_case": false,
                  "type": "int",
                  "_input_type": "IntInput"
                },
                "query": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "query",
                  "value": "",
                  "display_name": "Search Query",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The search query you want to execute with Tavily.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "search_depth": {
                  "trace_as_metadata": true,
                  "options": [
                    "basic",
                    "advanced"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "search_depth",
                  "value": "basic",
                  "display_name": "Search Depth",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The depth of the search.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                },
                "topic": {
                  "trace_as_metadata": true,
                  "options": [
                    "general",
                    "news"
                  ],
                  "combobox": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "topic",
                  "value": "general",
                  "display_name": "Search Topic",
                  "advanced": false,
                  "dynamic": false,
                  "info": "The category of the search.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "DropdownInput"
                }
              },
              "description": "Perform web searches using the Tavily API.",
              "icon": "search",
              "base_classes": [
                "Data",
                "Tool"
              ],
              "display_name": "Tavily Search",
              "documentation": "https://tavily.com/#api",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "api_run_model",
                  "display_name": "Data",
                  "method": "run_model",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Tool"
                  ],
                  "selected": "Tool",
                  "name": "api_build_tool",
                  "display_name": "Tool",
                  "method": "build_tool",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "api_key",
                "query",
                "search_depth",
                "topic",
                "max_results",
                "include_images",
                "include_answer"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "TavilySearch-EAGTF"
          },
          "selected": false,
          "width": 384,
          "height": 835,
          "positionAbsolute": {
            "x": 1500.4911188453761,
            "y": -1591.7308993109439
          },
          "dragging": false
        },
        {
          "id": "LangWatchEvaluatorComponent-dXa67",
          "type": "genericNode",
          "position": {
            "x": 2921.6917180291075,
            "y": -774.0376454008002
          },
          "data": {
            "type": "LangWatchEvaluatorComponent",
            "node": {
              "template": {
                "_type": "Component",
                "context_data": {
                  "trace_as_metadata": true,
                  "list": false,
                  "trace_as_input": true,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "context_data",
                  "value": "",
                  "display_name": "RAG Search Results (optional)",
                  "advanced": false,
                  "input_types": [
                    "Data"
                  ],
                  "dynamic": false,
                  "info": "The data to be used as context for evaluation.",
                  "title_case": false,
                  "type": "other",
                  "_input_type": "DataInput"
                },
                "answer": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "answer",
                  "value": "",
                  "display_name": "Answer",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The generated answer to be evaluated.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "import re\r\nfrom axiestudio.custom import Component\r\nfrom axiestudio.inputs import MessageTextInput, DataInput\r\nfrom axiestudio.schema.message import Message\r\nfrom axiestudio.template import Output\r\nfrom axiestudio.schema import Data\r\nimport langwatch\r\n\r\nclass LangWatchEvaluatorComponent(Component):\r\n    display_name = \"LangWatch Evaluator\"\r\n    description = \"Evaluates a question-answer pair using LangWatch and provides a trace URL.\"\r\n    icon = \"view\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"question\",\r\n            display_name=\"Question\",\r\n            info=\"The question to be evaluated.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"answer\",\r\n            display_name=\"Answer\",\r\n            info=\"The generated answer to be evaluated.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"ground_truth\",\r\n            display_name=\"Correct Answer\",\r\n            info=\"The expected correct answer.\",\r\n        ),\r\n        DataInput(\r\n            name=\"context_data\",\r\n            display_name=\"RAG Search Results (optional)\",\r\n            info=\"The data to be used as context for evaluation.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_email\",\r\n            display_name=\"User Email\",\r\n            info=\"The user email for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"user_name\",\r\n            display_name=\"Participant Name\",\r\n            info=\"Full name for identification in the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"question_id\",\r\n            display_name=\"Question ID\",\r\n            info=\"The question ID for the trace metadata.\",\r\n            advanced=True,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Trace URL\", name=\"trace_url\", method=\"evaluate\"),\r\n    ]\r\n\r\n    async def evaluate(self) -> Data:\r\n        question = self.question\r\n        answer = self.answer\r\n        ground_truth = self.ground_truth\r\n        context_data = self.context_data\r\n        user_email = self.user_email if self.user_email else \"\"\r\n        question_id = self.question_id if self.question_id else \"\"\r\n        user_name = self.user_name if self.user_name else \"\"\r\n\r\n        # Validate email if provided\r\n        if user_email and not self.validate_email(user_email):\r\n            raise ValueError(f\"Invalid email address: {user_email}\")\r\n\r\n        langwatch.api_key = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0aW1lc3RhbXAiOjE3MjY2Njk3NTkyMjcsInJhbmQiOjAuMjg5OTc4Mjk4NzU2MzIzOTcsImlhdCI6MTcyNjY2OTc1OX0._ow7WQ5RSTlYE-HjdcouHUCoXf9nRnWHk4u9nfq4LIw'\r\n\r\n        trace = langwatch.trace(\r\n            metadata={\r\n                \"user_email\": user_email,\r\n                \"question_id\": question_id,\r\n                \"user_name\": user_name,\r\n            },\r\n            expected_output=ground_truth\r\n        )\r\n        \r\n        contexts = [item.text.replace(\"\\t\", \" \").replace(\"\\n\", \" \") for item in context_data[:5]] if context_data else []\r\n        rag_span = trace.span(type=\"rag\", name=\"LangWatch Evaluator\", input=question, contexts=contexts, output=answer)\r\n        rag_span.end()\r\n\r\n        trace.send_spans()\r\n\r\n        public_url = trace.share()\r\n        self.log(f\"See the trace at: {public_url}\")\r\n\r\n        self.status = Data(data={\"trace_url\": public_url})\r\n        return Data(data={\"trace_url\": public_url})\r\n        \r\n    def validate_email(self, email):\r\n        pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\r\n        return re.match(pattern, email) is not None",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "ground_truth": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "ground_truth",
                  "value": "",
                  "display_name": "Correct Answer",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The expected correct answer.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "question": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "question",
                  "value": "",
                  "display_name": "Question",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The question to be evaluated.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "question_id": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "question_id",
                  "value": "",
                  "display_name": "Question ID",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The question ID for the trace metadata.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "user_email": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "user_email",
                  "value": "",
                  "display_name": "User Email",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "The user email for the trace metadata.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                },
                "user_name": {
                  "trace_as_input": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "user_name",
                  "value": "",
                  "display_name": "Participant Name",
                  "advanced": true,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Full name for identification in the trace metadata.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MessageTextInput"
                }
              },
              "description": "Evaluates a question-answer pair using LangWatch and provides a trace URL.",
              "icon": "view",
              "base_classes": [
                "Data"
              ],
              "display_name": "Langwatch Evaluator",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Data"
                  ],
                  "selected": "Data",
                  "name": "trace_url",
                  "display_name": "Trace URL",
                  "method": "evaluate",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "question",
                "answer",
                "ground_truth",
                "context_data",
                "user_email",
                "user_name",
                "question_id"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "LangWatchEvaluatorComponent-dXa67",
            "description": "Evaluates a question-answer pair using LangWatch and provides a trace URL.",
            "display_name": "Langwatch Evaluator"
          },
          "selected": false,
          "width": 384,
          "height": 550,
          "positionAbsolute": {
            "x": 2921.6917180291075,
            "y": -774.0376454008002
          },
          "dragging": false
        },
        {
          "id": "Prompt-aWHmV",
          "type": "genericNode",
          "position": {
            "x": -316.670636728983,
            "y": -857.8164557082557
          },
          "data": {
            "type": "Prompt",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.base.prompts.api_utils import process_prompt_template\nfrom axiestudio.custom import Component\nfrom axiestudio.inputs.inputs import DefaultPromptField\nfrom axiestudio.io import Output, PromptInput\nfrom axiestudio.schema.message import Message\nfrom axiestudio.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "template": {
                  "trace_as_input": true,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "template",
                  "value": "You are tasked with decomposing a complex question into three distinct sub-questions. Each sub-question should address a specific aspect of the main question, facilitating a comprehensive understanding of the topic.\n\nQuestion: {question}\n\nSub-question1: \n\nSub-question2: \n\nSub-question3: ",
                  "display_name": "Template",
                  "advanced": false,
                  "dynamic": false,
                  "info": "",
                  "title_case": false,
                  "type": "prompt",
                  "_input_type": "PromptInput"
                },
                "question": {
                  "field_type": "str",
                  "required": false,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "",
                  "fileTypes": [],
                  "file_path": "",
                  "name": "question",
                  "display_name": "question",
                  "advanced": false,
                  "input_types": [
                    "Message",
                    "Text"
                  ],
                  "dynamic": false,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false,
                  "type": "str"
                }
              },
              "description": "Create a prompt template with dynamic variables.",
              "icon": "prompts",
              "is_input": null,
              "is_output": null,
              "is_composition": null,
              "base_classes": [
                "Message"
              ],
              "name": "",
              "display_name": "Prompt",
              "documentation": "",
              "custom_fields": {
                "template": [
                  "question"
                ]
              },
              "output_types": [],
              "full_path": null,
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "prompt",
                  "hidden": null,
                  "display_name": "Prompt Message",
                  "method": "build_prompt",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "template"
              ],
              "beta": false,
              "error": null,
              "edited": false,
              "lf_version": "1.0.18"
            },
            "id": "Prompt-aWHmV"
          },
          "selected": false,
          "width": 384,
          "height": 416,
          "positionAbsolute": {
            "x": -316.670636728983,
            "y": -857.8164557082557
          },
          "dragging": false
        },
        {
          "id": "CustomComponent-jJ1gH",
          "type": "genericNode",
          "position": {
            "x": 680.1020288254518,
            "y": -730.9925257915561
          },
          "data": {
            "type": "SubQuestionExtractor",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\r\nfrom axiestudio.io import MultilineInput, Output\r\nfrom axiestudio.schema.message import Message\r\n\r\n\r\nclass SubQuestionExtractorComponent(Component):\r\n    display_name: str = \"Sub-question Extractor\"\r\n    description: str = \"Extract sub-questions from a multiline string and output each one separately.\"\r\n    icon = \"questions\"\r\n    name = \"SubQuestionExtractor\"\r\n\r\n    inputs = [\r\n        MultilineInput(\r\n            name=\"questions\",\r\n            display_name=\"Questions\",\r\n            info=\"A multiline string containing sub-questions.\",\r\n        ),\r\n    ]\r\n\r\n    # Define separate outputs for each sub-question\r\n    outputs = [\r\n        Output(display_name=\"Sub-question 1\", name=\"sub_question_1\", method=\"get_sub_question_1\"),\r\n        Output(display_name=\"Sub-question 2\", name=\"sub_question_2\", method=\"get_sub_question_2\"),\r\n        Output(display_name=\"Sub-question 3\", name=\"sub_question_3\", method=\"get_sub_question_3\"),\r\n    ]\r\n\r\n    def extract_sub_questions(self):\r\n        # Strip input and split by new lines, filtering out empty lines\r\n        questions = [q.strip() for q in self.questions.strip().split('\\n') if q.strip()]\r\n        \r\n        extracted = {}\r\n        \r\n        # Dynamically extract sub-questions\r\n        for i, question in enumerate(questions, start=1):\r\n            extracted[f\"Sub-question {i}\"] = question\r\n\r\n        return extracted\r\n\r\n    # Define separate methods to return each sub-question\r\n    def get_sub_question_1(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 1\", \"No Sub-question 1 found\"))\r\n\r\n    def get_sub_question_2(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 2\", \"No Sub-question 2 found\"))\r\n\r\n    def get_sub_question_3(self) -> Message:\r\n        extracted_questions = self.extract_sub_questions()\r\n        return Message(text=extracted_questions.get(\"Sub-question 3\", \"No Sub-question 3 found\"))\r\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "questions": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "questions",
                  "value": "",
                  "display_name": "Questions",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "A multiline string containing sub-questions.",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "Extract sub-questions from a multiline string and output each one separately.",
              "icon": "questions",
              "base_classes": [
                "Message"
              ],
              "display_name": "Custom Component",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "sub_question_1",
                  "display_name": "Sub-question 1",
                  "method": "get_sub_question_1",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "sub_question_2",
                  "display_name": "Sub-question 2",
                  "method": "get_sub_question_2",
                  "value": "__UNDEFINED__",
                  "cache": true
                },
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "sub_question_3",
                  "display_name": "Sub-question 3",
                  "method": "get_sub_question_3",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "questions"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "CustomComponent-jJ1gH"
          },
          "selected": false,
          "width": 384,
          "height": 420,
          "dragging": false,
          "positionAbsolute": {
            "x": 680.1020288254518,
            "y": -730.9925257915561
          }
        },
        {
          "id": "CustomComponent-ZBC0T",
          "type": "genericNode",
          "position": {
            "x": 1251.5416960631237,
            "y": -819.1018997155185
          },
          "data": {
            "type": "CombineQuestions",
            "node": {
              "template": {
                "_type": "Component",
                "code": {
                  "type": "code",
                  "required": true,
                  "placeholder": "",
                  "list": false,
                  "show": true,
                  "multiline": true,
                  "value": "from axiestudio.custom import Component\r\nfrom axiestudio.helpers.data import data_to_text\r\nfrom axiestudio.io import StrInput, Output\r\nfrom axiestudio.schema.message import Message\r\n\r\nclass CombineQuestionsComponent(Component):\r\n    display_name: str = \"Combine Questions\"\r\n    description: str = \"Combine multiple sub-questions into a single string for processing.\"\r\n    icon = \"merge\"\r\n    name = \"CombineQuestions\"\r\n\r\n    inputs = [\r\n        MultilineInput(name=\"sub_question_1\", display_name=\"Sub-question 1\", info=\"First sub-question\"),\r\n        MultilineInput(name=\"sub_question_2\", display_name=\"Sub-question 2\", info=\"Second sub-question\"),\r\n        MultilineInput(name=\"sub_question_3\", display_name=\"Sub-question 3\", info=\"Third sub-question\"),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Combined Questions\", name=\"combined_questions\", method=\"combine_questions\"),\r\n    ]\r\n\r\n    def combine_questions(self) -> Message:\r\n        combined_input = (\r\n            f\"Sub-question 1: {self.sub_question_1}\\n\"\r\n            f\"Sub-question 2: {self.sub_question_2}\\n\"\r\n            f\"Sub-question 3: {self.sub_question_3}\\n\"\r\n        )\r\n        return Message(text=combined_input)\r\n",
                  "fileTypes": [],
                  "file_path": "",
                  "password": false,
                  "name": "code",
                  "advanced": true,
                  "dynamic": true,
                  "info": "",
                  "load_from_db": false,
                  "title_case": false
                },
                "sub_question_1": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sub_question_1",
                  "value": "",
                  "display_name": "Sub-question 1",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "First sub-question",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "sub_question_2": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sub_question_2",
                  "value": "",
                  "display_name": "Sub-question 2",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Second sub-question",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                },
                "sub_question_3": {
                  "trace_as_input": true,
                  "multiline": true,
                  "trace_as_metadata": true,
                  "load_from_db": false,
                  "list": false,
                  "required": false,
                  "placeholder": "",
                  "show": true,
                  "name": "sub_question_3",
                  "value": "",
                  "display_name": "Sub-question 3",
                  "advanced": false,
                  "input_types": [
                    "Message"
                  ],
                  "dynamic": false,
                  "info": "Third sub-question",
                  "title_case": false,
                  "type": "str",
                  "_input_type": "MultilineInput"
                }
              },
              "description": "Combine multiple sub-questions into a single string for processing.",
              "icon": "merge",
              "base_classes": [
                "Message"
              ],
              "display_name": "Custom Component",
              "documentation": "",
              "custom_fields": {},
              "output_types": [],
              "pinned": false,
              "conditional_paths": [],
              "frozen": false,
              "outputs": [
                {
                  "types": [
                    "Message"
                  ],
                  "selected": "Message",
                  "name": "combined_questions",
                  "display_name": "Combined Questions",
                  "method": "combine_questions",
                  "value": "__UNDEFINED__",
                  "cache": true
                }
              ],
              "field_order": [
                "sub_question_1",
                "sub_question_2",
                "sub_question_3"
              ],
              "beta": false,
              "edited": true,
              "lf_version": "1.0.18"
            },
            "id": "CustomComponent-ZBC0T"
          },
          "selected": false,
          "width": 384,
          "height": 502,
          "dragging": false,
          "positionAbsolute": {
            "x": 1251.5416960631237,
            "y": -819.1018997155185
          }
        }
      ],
      "edges": [
        {
          "source": "Prompt-R6fUd",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-R6fUdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ToolCallingAgent-wCgtq",
          "targetHandle": "{œfieldNameœ:œsystem_promptœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "system_prompt",
              "id": "ToolCallingAgent-wCgtq",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-R6fUd",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-R6fUd{œdataTypeœ:œPromptœ,œidœ:œPrompt-R6fUdœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œsystem_promptœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": "",
          "animated": false,
          "selected": false
        },
        {
          "source": "ToolCallingAgent-wCgtq",
          "sourceHandle": "{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-wCgtqœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ChatOutput-39mWo",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-39mWoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ChatOutput-39mWo",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ToolCallingAgent",
              "id": "ToolCallingAgent-wCgtq",
              "name": "response",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ToolCallingAgent-wCgtq{œdataTypeœ:œToolCallingAgentœ,œidœ:œToolCallingAgent-wCgtqœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-39mWo{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-39mWoœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": "",
          "animated": false
        },
        {
          "source": "OpenAIModel-Iiyki",
          "sourceHandle": "{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
          "target": "ToolCallingAgent-wCgtq",
          "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "llm",
              "id": "ToolCallingAgent-wCgtq",
              "inputTypes": [
                "LanguageModel"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "OpenAIModel",
              "id": "OpenAIModel-Iiyki",
              "name": "model_output",
              "output_types": [
                "LanguageModel"
              ]
            }
          },
          "id": "reactflow__edge-OpenAIModel-Iiyki{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œllmœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
          "className": "",
          "animated": false
        },
        {
          "source": "TavilySearch-EAGTF",
          "sourceHandle": "{œdataTypeœ:œTavilySearchœ,œidœ:œTavilySearch-EAGTFœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}",
          "target": "ToolCallingAgent-wCgtq",
          "targetHandle": "{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œToolœ,œBaseToolœ],œtypeœ:œotherœ}",
          "data": {
            "targetHandle": {
              "fieldName": "tools",
              "id": "ToolCallingAgent-wCgtq",
              "inputTypes": [
                "Tool",
                "BaseTool"
              ],
              "type": "other"
            },
            "sourceHandle": {
              "dataType": "TavilySearch",
              "id": "TavilySearch-EAGTF",
              "name": "api_build_tool",
              "output_types": [
                "Tool"
              ]
            }
          },
          "id": "reactflow__edge-TavilySearch-EAGTF{œdataTypeœ:œTavilySearchœ,œidœ:œTavilySearch-EAGTFœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œtoolsœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œToolœ,œBaseToolœ],œtypeœ:œotherœ}",
          "animated": false,
          "className": ""
        },
        {
          "source": "ChatOutput-39mWo",
          "sourceHandle": "{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-39mWoœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "LangWatchEvaluatorComponent-dXa67",
          "targetHandle": "{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "answer",
              "id": "LangWatchEvaluatorComponent-dXa67",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatOutput",
              "id": "ChatOutput-39mWo",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "className": "",
          "animated": false,
          "id": "reactflow__edge-ChatOutput-39mWo{œdataTypeœ:œChatOutputœ,œidœ:œChatOutput-39mWoœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-dXa67{œfieldNameœ:œanswerœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
        },
        {
          "source": "ChatInput-2Z2B9",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "LangWatchEvaluatorComponent-dXa67",
          "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "question",
              "id": "LangWatchEvaluatorComponent-dXa67",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-2Z2B9",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "className": "",
          "animated": false,
          "id": "reactflow__edge-ChatInput-2Z2B9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-LangWatchEvaluatorComponent-dXa67{œfieldNameœ:œquestionœ,œidœ:œLangWatchEvaluatorComponent-dXa67œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "selected": false
        },
        {
          "source": "ChatInput-2Z2B9",
          "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-aWHmV",
          "targetHandle": "{œfieldNameœ:œquestionœ,œidœ:œPrompt-aWHmVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "question",
              "id": "Prompt-aWHmV",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "ChatInput",
              "id": "ChatInput-2Z2B9",
              "name": "message",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-ChatInput-2Z2B9{œdataTypeœ:œChatInputœ,œidœ:œChatInput-2Z2B9œ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-aWHmV{œfieldNameœ:œquestionœ,œidœ:œPrompt-aWHmVœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "Prompt-aWHmV",
          "sourceHandle": "{œdataTypeœ:œPromptœ,œidœ:œPrompt-aWHmVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
          "target": "OpenAIModel-Iiyki",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Iiykiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "OpenAIModel-Iiyki",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "Prompt",
              "id": "Prompt-aWHmV",
              "name": "prompt",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-Prompt-aWHmV{œdataTypeœ:œPromptœ,œidœ:œPrompt-aWHmVœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-Iiyki{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Iiykiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "OpenAIModel-Iiyki",
          "sourceHandle": "{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}",
          "target": "CustomComponent-jJ1gH",
          "targetHandle": "{œfieldNameœ:œquestionsœ,œidœ:œCustomComponent-jJ1gHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "questions",
              "id": "CustomComponent-jJ1gH",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "OpenAIModel",
              "id": "OpenAIModel-Iiyki",
              "name": "text_output",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-OpenAIModel-Iiyki{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Iiykiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-jJ1gH{œfieldNameœ:œquestionsœ,œidœ:œCustomComponent-jJ1gHœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-R6fUd",
          "targetHandle": "{œfieldNameœ:œsub_question_1œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_1",
              "id": "Prompt-R6fUd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_1",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_1œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-R6fUd",
          "targetHandle": "{œfieldNameœ:œsub_question_2œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_2",
              "id": "Prompt-R6fUd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_2",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_2œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}",
          "target": "Prompt-R6fUd",
          "targetHandle": "{œfieldNameœ:œsub_question_3œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_3",
              "id": "Prompt-R6fUd",
              "inputTypes": [
                "Message",
                "Text"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_3",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}-Prompt-R6fUd{œfieldNameœ:œsub_question_3œ,œidœ:œPrompt-R6fUdœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}",
          "target": "CustomComponent-ZBC0T",
          "targetHandle": "{œfieldNameœ:œsub_question_1œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_1",
              "id": "CustomComponent-ZBC0T",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_1",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_1œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_1œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-ZBC0T",
          "sourceHandle": "{œdataTypeœ:œCombineQuestionsœ,œidœ:œCustomComponent-ZBC0Tœ,œnameœ:œcombined_questionsœ,œoutput_typesœ:[œMessageœ]}",
          "target": "ToolCallingAgent-wCgtq",
          "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "input_value",
              "id": "ToolCallingAgent-wCgtq",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "CombineQuestions",
              "id": "CustomComponent-ZBC0T",
              "name": "combined_questions",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-ZBC0T{œdataTypeœ:œCombineQuestionsœ,œidœ:œCustomComponent-ZBC0Tœ,œnameœ:œcombined_questionsœ,œoutput_typesœ:[œMessageœ]}-ToolCallingAgent-wCgtq{œfieldNameœ:œinput_valueœ,œidœ:œToolCallingAgent-wCgtqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}",
          "target": "CustomComponent-ZBC0T",
          "targetHandle": "{œfieldNameœ:œsub_question_2œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_2",
              "id": "CustomComponent-ZBC0T",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_2",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_2œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_2œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        },
        {
          "source": "CustomComponent-jJ1gH",
          "sourceHandle": "{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}",
          "target": "CustomComponent-ZBC0T",
          "targetHandle": "{œfieldNameœ:œsub_question_3œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "data": {
            "targetHandle": {
              "fieldName": "sub_question_3",
              "id": "CustomComponent-ZBC0T",
              "inputTypes": [
                "Message"
              ],
              "type": "str"
            },
            "sourceHandle": {
              "dataType": "SubQuestionExtractor",
              "id": "CustomComponent-jJ1gH",
              "name": "sub_question_3",
              "output_types": [
                "Message"
              ]
            }
          },
          "id": "reactflow__edge-CustomComponent-jJ1gH{œdataTypeœ:œSubQuestionExtractorœ,œidœ:œCustomComponent-jJ1gHœ,œnameœ:œsub_question_3œ,œoutput_typesœ:[œMessageœ]}-CustomComponent-ZBC0T{œfieldNameœ:œsub_question_3œ,œidœ:œCustomComponent-ZBC0Tœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
          "className": ""
        }
      ],
      "viewport": {
        "x": -2273.581560403503,
        "y": 760.0503760528654,
        "zoom": 0.9920464525884166
      }
    },
    "date_created": "2024-10-03T12:29:42.103Z",
    "date_updated": "2024-10-07T12:51:24.286Z",
    "status": "Public",
    "sort": null,
    "user_updated": "cadd6302-230a-4cf5-be3f-b5120287e2a6",
    "user_created": {
      "username": "priyanka",
      "first_name": "Priyanka",
      "last_name": "Narayanappa",
      "id": "cadd6302-230a-4cf5-be3f-b5120287e2a6"
    },
    "tags": []
  },
  "conversion": {
    "converted_at": "2025-08-19T18:08:57.241Z",
    "converted_from": "langflow",
    "converted_to": "axiestudio",
    "conversions_made": 100,
    "converter_version": "1.0.0"
  }
}